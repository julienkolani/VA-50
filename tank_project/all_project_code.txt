################################################################################
PATH: ./config/arena.yaml
################################################################################
arena:
  height_m: 0.80
  width_m: 0.80
display:
  display_index: 0
  fullscreen: false
grid:
  resolution_m: 0.02 # 2 cm par cellule
robot:
  # Rayon physique du robot (Turtlebot Burger ~9cm)
  radius_m: 0.09
  # Marge de sécurité supplémentaire pour ne pas frotter les murs
  inflation_margin_m: 0.05
obstacles: []
projector:
  height: 960
  margin: 50
  margin_px: 50
  width: 1280
transform:
  H_C2W:
    - - -0.0010267945196526461
      - -3.730098981565071e-05
      - 1.5189658207743262
    - - -3.418757045764291e-05
      - 0.0015329706593207066
      - -0.5017419121719356
    - - -9.992942953194014e-07
      - 2.247742515662756e-05
      - 1.0
  scale: 0.5720342439860417
  scale_m_per_av: 0.5720342439860417



################################################################################
PATH: ./config/camera.yaml
################################################################################
# Camera Configuration

realsense:
  # Résolution réduite pour détection ArUco rapide (~15ms au lieu de 50-80ms)
  # L'homographie gère le changement d'échelle automatiquement
  width: 848
  height: 480
  fps: 60 # Plus fluide pour le contrôle robot
  enable_depth: false

aruco:
  dictionary: DICT_4X4_50 # ArUco dictionary type

  # Marker IDs and their meanings
  markers:
    projected_corners: [0, 1, 2, 3] # Virtual corners for calibration
    robot_ai: 4 # AI robot marker
    robot_human: 5 # Human robot marker

  # Physical marker size for scale estimation
  marker_size_m: 0.10 # 10cm markers

kalman:
  # Process noise covariance
  process_noise:
    position: 0.01
    velocity: 0.1
    orientation: 0.01
    angular_velocity: 0.1

  # Measurement noise covariance
  measurement_noise:
    position: 0.05
    orientation: 0.1

  # Time step (matches game tick rate)
  dt: 0.0333 # 30 FPS



################################################################################
PATH: ./config/game.yaml
################################################################################
# Game Rules Configuration

match:
  duration_seconds: 180 # 3 minutes
  tick_rate_fps: 30 # Game loop frequency

cooldowns:
  human_shot_seconds: 5.0 # Human can shoot every 5 seconds
  ai_shot_seconds: 3.0 # AI can shoot every 3 seconds

win_conditions:
  max_hits: 10 # First to 10 hits wins
  time_limit: true # Game ends when time expires
  sudden_death: false # Continue after time expires if tied?

shot_mechanics:
  range_m: 5.0 # Maximum shot range
  speed_mps: 10.0 # Shot travel speed (for animation)
  instant_hit: true # Hitscan vs projectile



################################################################################
PATH: ./config/ia.yaml
################################################################################
# AI Configuration

behavior:
  # Threat assessment thresholds
  danger_distance_m: 0.8 # Enemy too close threshold
  optimal_range_min_m: 1.2 # Min optimal firing range
  optimal_range_max_m: 3.5 # Max optimal firing range

  # Behavior priorities
  survival_priority: true # Retreat overrides attack

strategy:
  # Path planning
  heuristic: "euclidean" # 'euclidean', 'manhattan', 'diagonal'
  path_simplify: true # Apply Douglas-Peucker simplification
  simplify_epsilon_m: 0.05 # Simplification tolerance

  # Path smoothing
  smooth_path: true
  smooth_weight_data: 0.5
  smooth_weight_smooth: 0.3

  # Flanking behavior
  flank_angle_deg: 90 # Preferred flanking angle
  cover_preference: 0.8 # Preference for cover (0-1)

decision_rate:
  # How often to replan (in ticks)
  replan_interval: 10 # Replan every 10 ticks (~0.33s at 30fps)



################################################################################
PATH: ./config/projector.yaml
################################################################################
# =============================================================================
# Configuration Projecteur - Tank Arena
# =============================================================================
# Modifier ces valeurs selon votre setup matériel

projector:
  # Résolution du projecteur VGA
  width: 1024
  height: 768

  # Marge de sécurité (pixels depuis le bord)
  margin_px: 50

# Configuration multi-écran
display:
  # Offset X = largeur de l'écran principal (ex: 1920 pour Full HD)
  monitor_offset_x: 1920
  monitor_offset_y: 0

  # Mode sans bordure: supprime la barre de titre et les bordures
  # true = fenêtre propre pour projection (recommandé prod)
  # false = fenêtre normale avec barre titre (utile pour debug)
  borderless: true

  # Cacher le curseur souris dans la fenêtre projetée
  # true = pas de flèche visible sur l'arène (recommandé prod)
  # false = curseur visible (utile pour debug)
  hide_cursor: true

# Calibration métrique
calibration:
  # Taille réelle du marqueur ArUco en mètres
  marker_size_m: 0.10

  # IDs des marqueurs
  corner_marker_ids: [0, 1, 2, 3] # Coins projetés
  robot_marker_ids: [4, 5] # Marqueurs robot

  # Dictionnaire ArUco utilisé
  aruco_dictionary: "DICT_4X4_100"



################################################################################
PATH: ./config/robot.yaml
################################################################################
# Robot Configuration (Turtlebot Burger)

kinematics:
  wheel_base_m: 0.16 # Distance between wheels
  wheel_radius_m: 0.033 # Wheel radius

physical:
  robot_radius_m: 0.09 # Approximation as circle

velocity_limits:
  max_linear_mps: 0.22 # Max linear velocity (m/s)
  max_angular_radps: 2.84 # Max angular velocity (rad/s)

acceleration_limits:
  max_linear_accel: 0.5 # m/s²
  max_angular_accel: 5.0 # rad/s²

control:
  # Trajectory following parameters
  lookahead_distance_m: 0.3
  k_velocity: 0.5 # Linear velocity gain
  k_theta: 0.5 # Angular velocity gain
  waypoint_threshold_m: 0.1 # Distance to consider waypoint reached

ros_bridge:
  host: "localhost"
  port: 8765 # Must match safety_bridge WebSocket port
  timeout_s: 1.0



################################################################################
PATH: ./core/control/__init__.py
################################################################################



################################################################################
PATH: ./core/control/kinematics.py
################################################################################
"""
Kinematics - Robot Motion Model

Differential drive robot kinematics for Turtlebot Burger:
- Forward kinematics: (v, ω) → (dx, dy, dθ)
- Inverse kinematics: velocity constraints
- Dynamics model (simplified)

Used for simulation and control validation.
"""

import numpy as np
from typing import Tuple


class DifferentialDriveKinematics:
    """
    Kinematics for differential drive robot (Turtlebot Burger).
    
    Robot parameters:
    - Wheel base: distance between wheels
    - Wheel radius
    """
    
    def __init__(self, wheel_base: float = 0.16, wheel_radius: float = 0.033):
        """
        Initialize kinematics model.
        
        Args:
            wheel_base: Distance between wheels in meters (Burger: 0.16m)
            wheel_radius: Wheel radius in meters (Burger: 0.033m)
        """
        self.wheel_base = wheel_base
        self.wheel_radius = wheel_radius
        
    def forward_kinematics(self, 
                          v: float, 
                          omega: float, 
                          current_pose: Tuple[float, float, float],
                          dt: float) -> Tuple[float, float, float]:
        """
        Compute new pose from velocity commands.
        
        Args:
            v: Linear velocity in m/s
            omega: Angular velocity in rad/s
            current_pose: (x, y, theta) current pose
            dt: Time step in seconds
            
        Returns:
            (x_new, y_new, theta_new) updated pose
            
        Equations:
            dx = v * cos(θ) * dt
            dy = v * sin(θ) * dt
            dθ = ω * dt
        """
        x, y, theta = current_pose
        
        # Update orientation first
        theta_new = theta + omega * dt
        
        # Average theta for more accurate integration
        theta_avg = theta + 0.5 * omega * dt
        
        # Update position
        x_new = x + v * np.cos(theta_avg) * dt
        y_new = y + v * np.sin(theta_avg) * dt
        
        # Normalize theta to [-pi, pi]
        theta_new = np.arctan2(np.sin(theta_new), np.cos(theta_new))
        
        return (x_new, y_new, theta_new)
    
    def wheel_velocities_to_body(self, 
                                 v_left: float, 
                                 v_right: float) -> Tuple[float, float]:
        """
        Convert wheel velocities to body velocities.
        
        Args:
            v_left: Left wheel velocity in m/s
            v_right: Right wheel velocity in m/s
            
        Returns:
            (v, omega): body linear and angular velocities
        """
        v = (v_right + v_left) / 2.0
        omega = (v_right - v_left) / self.wheel_base
        
        return (v, omega)
    
    def body_to_wheel_velocities(self, 
                                 v: float, 
                                 omega: float) -> Tuple[float, float]:
        """
        Convert body velocities to wheel velocities.
        
        Args:
            v: Linear velocity in m/s
            omega: Angular velocity in rad/s
            
        Returns:
            (v_left, v_right): wheel velocities in m/s
        """
        v_left = v - (omega * self.wheel_base) / 2.0
        v_right = v + (omega * self.wheel_base) / 2.0
        
        return (v_left, v_right)
    
    def validate_velocities(self, 
                           v: float, 
                           omega: float,
                           max_v: float = 0.22,
                           max_omega: float = 2.84) -> Tuple[float, float]:
        """
        Validate and clamp velocities to robot limits.
        
        Args:
            v, omega: Desired velocities
            max_v: Maximum linear velocity (Burger: 0.22 m/s)
            max_omega: Maximum angular velocity (Burger: 2.84 rad/s)
            
        Returns:
            (v_clamped, omega_clamped)
        """
        v_clamped = np.clip(v, -max_v, max_v)
        omega_clamped = np.clip(omega, -max_omega, max_omega)
        
        return (v_clamped, omega_clamped)



################################################################################
PATH: ./core/control/motion_constraints.py
################################################################################
"""
Motion Constraints - Velocity & Acceleration Limits

Enforces robot physical constraints:
- Maximum velocities (linear, angular)
- Maximum accelerations
- Smooth velocity ramping
- Emergency stop logic

Prevents damaging commands and ensures smooth motion.
"""

import numpy as np
from typing import Tuple


class MotionConstraints:
    """
    Enforces physical motion constraints for safe robot control.
    
    Prevents:
    - Exceeding velocity limits
    - Excessive acceleration (sudden changes)
    - Unsafe commands
    """
    
    def __init__(self, config):
        """
        Initialize motion constraints.
        
        Args:
            config: Robot configuration:
                - max_linear_vel: m/s
                - max_angular_vel: rad/s
                - max_linear_accel: m/s²
                - max_angular_accel: rad/s²
        """
        self.max_v = config.get('max_linear_vel', 0.22)
        self.max_omega = config.get('max_angular_vel', 2.84)
        self.max_accel_v = config.get('max_linear_accel', 0.5)
        self.max_accel_omega = config.get('max_angular_accel', 5.0)
        
        # Previous commands for acceleration limiting
        self.prev_v = 0.0
        self.prev_omega = 0.0
        
    def apply_constraints(self, 
                         v_desired: float, 
                         omega_desired: float,
                         dt: float) -> Tuple[float, float]:
        """
        Apply all motion constraints.
        
        Args:
            v_desired: Desired linear velocity
            omega_desired: Desired angular velocity
            dt: Time since last command (seconds)
            
        Returns:
            (v_safe, omega_safe): constrained velocities
            
        Steps:
            1. Clamp to velocity limits
            2. Limit acceleration
            3. Update previous commands
        """
        # Velocity limits
        v = np.clip(v_desired, -self.max_v, self.max_v)
        omega = np.clip(omega_desired, -self.max_omega, self.max_omega)
        
        # Acceleration limits
        if dt > 0:
            v = self._limit_acceleration(v, self.prev_v, self.max_accel_v, dt)
            omega = self._limit_acceleration(omega, self.prev_omega, 
                                            self.max_accel_omega, dt)
        
        # Store for next iteration
        self.prev_v = v
        self.prev_omega = omega
        
        return (v, omega)
    
    def _limit_acceleration(self, 
                           desired: float, 
                           previous: float,
                           max_accel: float, 
                           dt: float) -> float:
        """
        Limit acceleration of a single velocity component.
        
        Args:
            desired: Desired velocity
            previous: Previous velocity
            max_accel: Maximum allowed acceleration
            dt: Time step
            
        Returns:
            Acceleration-limited velocity
        """
        delta = desired - previous
        max_delta = max_accel * dt
        
        if abs(delta) > max_delta:
            delta = np.sign(delta) * max_delta
        
        return previous + delta
    
    def emergency_stop(self):
        """
        Reset to zero velocity immediately.
        
        Used for safety stop.
        """
        self.prev_v = 0.0
        self.prev_omega = 0.0
        
        return (0.0, 0.0)
    
    def soft_stop(self, dt: float) -> Tuple[float, float]:
        """
        Gradually decelerate to zero.
        
        Args:
            dt: Time step
            
        Returns:
            Decelerating velocities
        """
        return self.apply_constraints(0.0, 0.0, dt)



################################################################################
PATH: ./core/control/ros_bridge_client.py
################################################################################
"""
ROS Bridge Client - Communication with ROS System

WebSocket client for sending commands to ROS bridge:
- Connects to ROS bridge server via WebSocket
- Sends velocity commands to /cmd_vel
- Receives acknowledgments
- Maintains connection health

Logs: [ROS] Command sent: v=X, ω=Y
"""

import json
import time
import asyncio
import threading
from typing import Optional

try:
    import websockets
    from websockets.sync.client import connect as ws_connect
    HAS_WEBSOCKETS = True
except ImportError:
    HAS_WEBSOCKETS = False
    print("[ROS] Warning: websockets not installed, using fallback")


class ROSBridgeClient:
    """
    Client for communicating with ROS bridge via WebSocket.
    
    Sends velocity commands to physical robot.
    """
    
    def __init__(self, host: str = 'localhost', port: int = 8765):
        """
        Initialize ROS bridge client.
        
        Args:
            host: ROS bridge server host
            port: ROS bridge server port (default 8765 for WebSocket)
        """
        self.host = host
        self.port = port
        self.ws = None
        self.connected = False
        self.uri = f"ws://{host}:{port}"
        
    def connect(self, max_retries: int = 3, retry_interval: float = 2.0):
        """
        Establish WebSocket connection to ROS bridge.
        
        Args:
            max_retries: Max retry attempts
            retry_interval: Seconds between retries
        
        Logs:
            [ROS] Connected to bridge at host:port
            [ROS] Connection failed: error
        """
        if not HAS_WEBSOCKETS:
            print("[ROS] WebSocket library not available")
            return False
            
        attempt = 0
        while attempt < max_retries:
            attempt += 1
            try:
                self.ws = ws_connect(self.uri, open_timeout=5)
                self.connected = True
                
                # Read welcome message
                try:
                    welcome = self.ws.recv(timeout=2)
                    print(f"[ROS] Connected to bridge at {self.host}:{self.port}")
                except:
                    print(f"[ROS] Connected to bridge at {self.host}:{self.port}")
                    
                return True
                
            except Exception as e:
                self.connected = False
                print(f"[ROS] Connection attempt {attempt} failed: {e}")
                
                if attempt >= max_retries:
                    print("[ROS] Max retries reached, running without ROS connection")
                    return False
                
                print(f"[ROS] Retrying in {retry_interval} seconds...")
                time.sleep(retry_interval)
        
        return False
    
    def disconnect(self):
        """Close WebSocket connection to ROS bridge."""
        if self.ws:
            try:
                self.ws.close()
            except:
                pass
            self.connected = False
            print("[ROS] Disconnected from bridge")
    
    def send_velocity_command(self, 
                             robot_id: int, 
                             v: float, 
                             omega: float) -> bool:
        """
        Send velocity command to robot.
        
        Args:
            robot_id: 4 (AI) or 5 (Human)
            v: Linear velocity in m/s
            omega: Angular velocity in rad/s
            
        Returns:
            True if sent successfully
            
        Message format (JSON):
            {
                "type": "cmd_vel",
                "linear_x": v,
                "angular_z": omega,
                "timestamp": unix_time
            }
            
        Logs:
            [ROS] Robot4 cmd: v=0.15 m/s, ω=-0.30 rad/s
        """
        if not self.connected or not self.ws:
            # Try to reconnect silently
            if not self.connect(max_retries=1, retry_interval=0.5):
                return False
        
        # Message format matching safety_bridge.py expectations
        message = {
            "type": "cmd_vel",
            "linear_x": round(v, 4),
            "angular_z": round(omega, 4),
            "timestamp": time.time()
        }
        
        try:
            msg_json = json.dumps(message)
            self.ws.send(msg_json)
            
            # Log (less verbose - only log every 30th command or significant ones)
            if abs(v) > 0.01 or abs(omega) > 0.1:
                print(f"[ROS] Robot{robot_id} cmd: v={v:.2f} m/s, w={omega:.2f} rad/s")
            
            # Try to receive acknowledgment (non-blocking)
            try:
                self.ws.recv(timeout=0.01)
            except:
                pass  # Ignore if no response
            
            return True
            
        except Exception as e:
            print(f"[ROS] Send failed: {e}")
            self.connected = False
            return False
    
    def receive_feedback(self, timeout: float = 0.01) -> Optional[dict]:
        """
        Receive feedback from ROS bridge (non-blocking).
        
        Args:
            timeout: Socket timeout in seconds
            
        Returns:
            dict with data, or None
        """
        if not self.connected or not self.ws:
            return None
        
        try:
            data = self.ws.recv(timeout=timeout)
            if data:
                return json.loads(data)
        except:
            pass  # No data available
            
        return None
    
    def send_stop_command(self, robot_id: int):
        """
        Send emergency stop to robot.
        
        Args:
            robot_id: Robot to stop
        """
        self.send_velocity_command(robot_id, 0.0, 0.0)



################################################################################
PATH: ./core/control/trajectory_follower.py
################################################################################
"""
Trajectory Follower - Path Following Controller

Implements trajectory following for waypoint navigation:
- Pure pursuit controller
- PID-based heading control
- Dynamic waypoint advancement
- Obstacle avoidance reactions

Takes a path (list of waypoints) and current pose, outputs (v, ω).

Logs: [CTRL] Following waypoint (x,y), distance: Dm
"""

import numpy as np
from typing import Tuple, List, Optional


class TrajectoryFollower:
    """
    Trajectory following controller for waypoint navigation.
    
    Uses pure pursuit algorithm for smooth path following.
    """
    
    def __init__(self, config):
        """
        Initialize trajectory follower.
        
        Args:
            config: Controller parameters from config/robot.yaml:
                - lookahead_distance: Pure pursuit lookahead
                - k_v: Linear velocity gain
                - k_theta: Angular velocity gain
                - max_linear_vel: Max v in m/s
                - max_angular_vel: Max ω in rad/s
        """
        self.lookahead_distance = config.get('lookahead_distance', 0.3)
        self.k_v = config.get('k_v', 1.0)
        self.k_theta = config.get('k_theta', 2.0)
        self.max_linear_vel = config.get('max_linear_vel', 0.22)
        self.max_angular_vel = config.get('max_angular_vel', 2.84)
        
        self.waypoint_reached_threshold = config.get('waypoint_threshold', 0.1)
        
    def compute_control(self, 
                       current_pose: Tuple[float, float, float],
                       waypoints: List[Tuple[float, float]]) -> Tuple[float, float]:
        """
        Compute control commands to follow waypoints.
        
        Args:
            current_pose: (x, y, theta) current robot pose
            waypoints: List of (x, y) waypoints in meters
            
        Returns:
            (v, omega): linear and angular velocities
            
        Algorithm (Pure Pursuit):
            1. Find lookahead point on path
            2. Calculate curvature to reach lookahead point
            3. Compute v and ω from curvature
            4. Clamp to velocity limits
            
        Logs:
            [CTRL] Target waypoint (x,y), distance: Dm, heading error: θ rad
        """
        if not waypoints:
            return (0.0, 0.0)
        
        x, y, theta = current_pose
        
        # Find target waypoint (lookahead)
        target_wp = self._get_lookahead_point(current_pose, waypoints)
        
        if target_wp is None:
            return (0.0, 0.0)
        
        # Calculate control
        v, omega = self._pure_pursuit(current_pose, target_wp)
        
        # Clamp velocities
        v = np.clip(v, -self.max_linear_vel, self.max_linear_vel)
        omega = np.clip(omega, -self.max_angular_vel, self.max_angular_vel)
        
        return (v, omega)
    
    def _get_lookahead_point(self, pose, waypoints):
        """
        Find the waypoint at lookahead distance.
        
        Args:
            pose: Current robot pose
            waypoints: List of waypoints
            
        Returns:
            (x, y) target waypoint
        """
        x, y, _ = pose
        
        # Find first waypoint beyond lookahead distance
        for wp in waypoints:
            dist = np.sqrt((wp[0] - x)**2 + (wp[1] - y)**2)
            if dist >= self.lookahead_distance:
                return wp
        
        # If all waypoints are closer, return last one
        return waypoints[-1] if waypoints else None
    
    def _pure_pursuit(self, pose, target):
        """
        Pure pursuit control law.
        
        Args:
            pose: (x, y, theta)
            target: (x_t, y_t)
            
        Returns:
            (v, omega)
        """
        x, y, theta = pose
        x_t, y_t = target
        
        # Calculate distance and angle to target
        dx = x_t - x
        dy = y_t - y
        distance = np.sqrt(dx**2 + dy**2)
        
        # Target angle
        target_theta = np.arctan2(dy, dx)
        
        # Heading error
        theta_error = self._normalize_angle(target_theta - theta)
        
        # Linear velocity proportional to distance
        v = self.k_v * distance
        
        # Angular velocity proportional to heading error
        omega = self.k_theta * theta_error
        
        # Reduce linear velocity when turning
        v *= np.cos(theta_error)
        
        return (v, omega)
    
    def _normalize_angle(self, angle):
        """Normalize angle to [-pi, pi]."""
        while angle > np.pi:
            angle -= 2 * np.pi
        while angle < -np.pi:
            angle += 2 * np.pi
        return angle
    
    def is_waypoint_reached(self, pose, waypoint):
        """
        Check if current waypoint is reached.
        
        Args:
            pose: (x, y, theta)
            waypoint: (x, y)
            
        Returns:
            True if within threshold
        """
        x, y, _ = pose
        dist = np.sqrt((waypoint[0] - x)**2 + (waypoint[1] - y)**2)
        return dist < self.waypoint_reached_threshold



################################################################################
PATH: ./core/game/game_engine.py
################################################################################
"""
Game Engine - Central Arbitrator

This module orchestrates the game loop and coordinates all subsystems.
It acts as the referee, managing:
- Game state transitions
- Timer management  
- Shot validation and resolution
- Win/loss conditions
- Tick-based game cycle (30 FPS)

The game engine receives:
- World state (robot poses, obstacles from perception)
- IA decisions (target, fire_request)
- Human input (triggers)

The game engine produces:
- Updated game state (scores, cooldowns, status)
- Events (shots fired, hits registered, game over)

Logs: [GAME] prefix for all game-related events
"""

class GameEngine:
    """
    Central game arbitrator managing the game loop and rules enforcement.
    
    Responsibilities:
    - Orchestrate 30 FPS game tick
    - Validate and execute shots (human + AI)
    - Update timers and cooldowns
    - Check win conditions
    - Emit game events for visualization
    
    Does NOT:
    - Make AI decisions (that's core/ia/)
    - Control motors (that's core/control/)
    - Draw anything (that's visualization/)
    """
    
    
    def __init__(self, config):
        """
        Initialize game engine with configuration.
        
        Args:
            config: Game configuration from config/game.yaml
                   (match duration, cooldowns, win conditions)
        """
        self.config = config
        
        # Load rules
        self.rules = GameRules.from_config(config['match']) if 'match' in config else GameRules()
        
        # Helper subsystems
        # Note: Raycast and Hits need WorldModel, which is passed in tick() or initialized later
        # For now we create placeholders or require world in tick
        self.raycast = None 
        self.hit_manager = None
        
        # Timers
        self.cooldowns = CooldownManager(
            self.rules.ai_shot_cooldown, 
            self.rules.human_shot_cooldown
        )
        
        # State
        self.start_time = 0.0
        self.state = GameStatus.READY
        
        print("[GAME] Engine initialized")
    
    def _ensure_subsystems(self, world_model):
        """Lazy initialization of subsystems that need world model."""
        if self.raycast is None:
            from .raycast import Raycast
            from .hits import HitManager
            
            self.raycast = Raycast(world_model.grid)
            self.hit_manager = HitManager(self.raycast)
            print("[GAME] Subsystems linked to WorldModel")

    def tick(self, world_state, ia_request, human_input):
        """
        Execute one game tick (called at 30 FPS).
        
        Args:
            world_state: WorldModel instance (NOT just a dict)
            ia_request: AI decision (target, fire_request)
            human_input: Human controls/triggers
            
        Returns:
            dict: Updated game state for visualization
        """
        import time
        current_time = time.time()
        
        # Ensure subsystems are ready
        self._ensure_subsystems(world_state)
        
        # 0. Handle Game State Transitions
        if self.state == GameStatus.READY:
            # Check for start condition (e.g. human input)
            if human_input.get('start_game', False):
                self.start_time = current_time
                self.state = GameStatus.PLAYING
                self.hit_manager.clear_history()
                print("[GAME] Match STARTED")
                
        elif self.state == GameStatus.PLAYING:
            # Check time expiration
            elapsed = current_time - self.start_time
            if elapsed >= self.rules.match_duration_seconds:
                self.state = GameStatus.FINISHED
                self._check_win_condition()
                print("[GAME] Match TIME OVER")
                
            # 1. Process Shooting
            self._handle_shooting(world_state, ia_request, human_input, current_time)
            
            # 2. Check Win Condition (Score limit)
            game_over, winner = self._check_win_condition()
            if game_over and self.state != GameStatus.FINISHED:
                self.state = GameStatus.FINISHED
                print("[GAME] Match FINISHED. Winner: {}".format(winner))

        # 3. Build State Dictionary for View
        scores = self.hit_manager.get_score_summary()
        
        state_dict = {
            'status': self.state.value,
            'time_remaining_s': max(0, self.rules.match_duration_seconds - (current_time - self.start_time)) if self.state == GameStatus.PLAYING else 0,
            
            # Scores
            'robot_4_hits_inflicted': scores['robot_4_hits_inflicted'],
            'robot_5_hits_inflicted': scores['robot_5_hits_inflicted'],
            'robot_4_hits_received': scores['robot_4_hits_received'],
            'robot_5_hits_received': scores['robot_5_hits_received'],
            
            # Cooldowns
            'can_shoot_ai': self.cooldowns.can_shoot_ai(),
            'can_shoot_human': self.cooldowns.can_shoot_human(),
            
            # Debug info passed through
            'ai_has_los': ia_request.get('has_los', False),
            'ai_fire_request': ia_request.get('fire_request', False),
            'ai_state': ia_request.get('state', 'UNKNOWN')
        }
        
        return state_dict
    
    def _handle_shooting(self, world_state, ia_request, human_input, current_time):
        """Handle fire requests from AI and Human."""
        
        # Robot poses from WorldModel
        # Assuming world_state has methods or attributes for poses
        # We need to access the latest poses. 
        # Since world_state passed here is WorldModel object, we can ask it.
        # But wait, main.py passes `world` which is WorldModel. 
        # Let's assume we can get poses from it.
        
        r4_pose = world_state.get_robot_pose(4)
        r5_pose = world_state.get_robot_pose(5)
        
        if r4_pose is None or r5_pose is None:
            return # Can't shoot if robots not tracked
            
        # --- AI SHOOTING ---
        if ia_request.get('fire_request', False):
            if self.cooldowns.can_shoot_ai():
                self.cooldowns.register_shot_ai()
                print("[GAME] AI Firing!")
                self.hit_manager.process_shot(4, r4_pose, r5_pose, current_time)
                
        # --- HUMAN SHOOTING ---
        if human_input.get('fire_request', False):
            if self.cooldowns.can_shoot_human():
                self.cooldowns.register_shot_human()
                print("[GAME] Human Firing!")
                self.hit_manager.process_shot(5, r5_pose, r4_pose, current_time)

    def _check_win_condition(self):
        """
        Check if match should end based on hits.
        
        Returns:
            (game_over: bool, winner: str or None)
        """
        scores = self.hit_manager.get_score_summary()
        ai_hits = scores['robot_4_hits_inflicted']
        human_hits = scores['robot_5_hits_inflicted']
        
        max_hits = self.rules.max_hits_to_win
        
        if ai_hits >= max_hits:
            return True, "AI"
        
        if human_hits >= max_hits:
            return True, "HUMAN"
            
        return False, None

# Imports at bottom to avoid circular deps if needed, 
# or top if safe. 
from .rules import GameRules
from .timers import CooldownManager
from .state import GameStatus



################################################################################
PATH: ./core/game/hits.py
################################################################################
"""
Hits - Scoring & Impact Management

Manages hit detection and scoring:
- Validate hits (is target in range, not obstructed)
- Record hits for both robots
- Calculate score deltas
- Emit hit events for visualization

Works with raycast.py for collision detection.
"""

from dataclasses import dataclass
from typing import Optional


@dataclass
class HitEvent:
    """
    Represents a single hit event.
    
    Used for visualization (flash effect, sound, score update).
    """
    shooter_id: int  # 4 (AI) or 5 (Human)
    target_id: int   # 4 or 5
    impact_point: tuple  # (x, y) in meters
    timestamp: float
    damage: int = 1  # Future: variable damage
    

class HitManager:
    """
    Manages hit validation and scoring.
    
    Collaborates with Raycast to determine valid hits.
    """
    
    def __init__(self, raycast):
        """
        Initialize hit manager.
        
        Args:
            raycast: Raycast instance for collision detection
        """
        self.raycast = raycast
        self.hit_history = []  # List of HitEvent
        
    
    def process_shot(self, shooter_id, shooter_pose, target_pose, current_time):
        """
        Process a shot attempt and determine if it hits.
        
        Args:
            shooter_id: 4 (AI) or 5 (Human)
            shooter_pose: (x, y, theta) of shooter
            target_pose: (x, y, theta) of target
            current_time: Current game time
            
        Returns:
            HitEvent if hit, None if miss
        """
        x, y, theta = shooter_pose
        target_id = 5 if shooter_id == 4 else 4
        
        target_x, target_y, _ = target_pose
        
        # Max range hardcoded for now (should come from rules)
        MAX_RANGE = 5.0 
        
        # 1. Check for obstacles first
        # We only care about obstacles closer than the target
        dist_to_target = ((target_x - x)**2 + (target_y - y)**2)**0.5
        
        obstacle_hit = self.raycast.cast_shot((x, y), theta, dist_to_target)
        if obstacle_hit['hit'] and obstacle_hit['target'] == 'obstacle':
            # Hit obstacle before target
            print("[HIT] Robot {} shot blocked by obstacle at {:.2f}m".format(shooter_id, obstacle_hit['distance']))
            return None
            
        # 2. Check for robot hit
        is_hit = self.raycast.check_robot_collision(
            (x, y), theta, MAX_RANGE, (target_x, target_y)
        )
        
        if is_hit:
            print("[HIT] Robot {} HIT Robot {}!".format(shooter_id, target_id))
            
            # Calculate impact point (approximate)
            impact_point = (target_x, target_y) 
            
            event = HitEvent(
                shooter_id=shooter_id,
                target_id=target_id,
                impact_point=impact_point,
                timestamp=current_time
            )
            self.hit_history.append(event)
            return event
            
        return None
    
    def get_score_summary(self):
        """
        Calculate current score from hit history.
        
        Returns:
            dict: {
                'robot_4_hits': int,  # Hits scored by AI
                'robot_5_hits': int,  # Hits scored by Human
            }
        """
        r4_hits = sum(1 for h in self.hit_history if h.shooter_id == 4)
        r5_hits = sum(1 for h in self.hit_history if h.shooter_id == 5)
        
        return {
            'robot_4_hits_inflicted': r4_hits,
            'robot_5_hits_inflicted': r5_hits,
            'robot_4_hits_received': r5_hits,
            'robot_5_hits_received': r4_hits
        }
    
    def clear_history(self):
        """Clear hit history (used when starting new match)."""
        self.hit_history = []



################################################################################
PATH: ./core/game/__init__.py
################################################################################



################################################################################
PATH: ./core/game/raycast.py
################################################################################
"""
Raycast - Shot Collision Detection

Implements ray-based collision detection for laser shots:
- Cast ray from shooter position in direction theta
- Check intersections with:
  * Static obstacles (walls, blocks)
  * Dynamic obstacles (robots)
- Return first hit or None

Uses the occupancy grid from core/world for obstacle detection.
Implements DDA (Digital Differential Analyzer) for efficient grid traversal.

Logs: [RAYCAST] Hit detected / Miss
"""

import numpy as np
from typing import Optional, Tuple


class Raycast:
    """
    Efficient ray-based collision detection for shots.
    
    Uses DDA algorithm to traverse occupancy grid and detect hits.
    """
    
    def __init__(self, occupancy_grid):
        """
        Initialize raycast with world occupancy grid.
        
        Args:
            occupancy_grid: OccupancyGrid instance from core/world
        """
        self.grid = occupancy_grid
    
    
    def cast_shot(self, start_pos, theta, max_range_m):
        """
        Cast a shot ray and detect collisions.
        
        Args:
            start_pos: (x, y) shooter position in meters
            theta: Shot direction in radians
            max_range_m: Maximum shot range
            
        Returns:
            dict: {
                'hit': bool,
                'target': 'robot4' | 'robot5' | 'obstacle' | None,
                'impact_point': (x, y) in meters or None,
                'distance': float in meters
            }
        """
        start_x, start_y = start_pos
        
        # Ray direction vector
        dx = np.cos(theta)
        dy = np.sin(theta)
        
        # Walk the grid
        current_dist = 0.0
        step_size = self.grid.resolution
        
        # Check every point along the ray
        while current_dist <= max_range_m:
            curr_x = start_x + dx * current_dist
            curr_y = start_y + dy * current_dist
            
            # 1. Check map boundaries
            if not (0 <= curr_x <= self.grid.width_m and 0 <= curr_y <= self.grid.height_m):
                break
                
            # 2. Check static obstacles using occupancy grid
            grid_val = self.grid.get_value(curr_x, curr_y)
            if grid_val > 50:  # Threshold for occupied
                return {
                    'hit': True,
                    'target': 'obstacle',
                    'impact_point': (curr_x, curr_y),
                    'distance': current_dist
                }
            
            current_dist += step_size
            
        return {
            'hit': False,
            'target': None,
            'impact_point': None,
            'distance': max_range_m
        }

    def check_robot_collision(self, start_pos, theta, max_range_m, target_pos, target_radius_m=0.15):
        """
        Check if ray hits a specific robot.
        
        Args:
            start_pos: (x,y) shooter
            theta: angle
            max_range_m: max range
            target_pos: (x,y) target center
            target_radius_m: target hit radius
        """
        # Vector from shooter to target
        sx, sy = start_pos
        tx, ty = target_pos
        
        val_x = tx - sx
        val_y = ty - sy
        
        # Project target center onto ray
        # Ray vector: (cos, sin)
        ray_x, ray_y = np.cos(theta), np.sin(theta)
        
        # Dot product
        t = val_x * ray_x + val_y * ray_y
        
        # Closest point on ray to target center
        closest_x = sx + t * ray_x
        closest_y = sy + t * ray_y
        
        # Distance checks
        if t < 0: return False # Target behind shooter
        if t > max_range_m: return False # Target out of range
        
        # Distance from closest point to target center
        dist_sq = (closest_x - tx)**2 + (closest_y - ty)**2
        
        return dist_sq <= (target_radius_m**2)

    def _check_line_of_sight(self, pos1, pos2):
        """Check if LOS is clear between two points (simple version)."""
        x1, y1 = pos1
        x2, y2 = pos2
        
        dist = np.sqrt((x2-x1)**2 + (y2-y1)**2)
        if dist == 0: return True
        
        dx = (x2 - x1) / dist
        dy = (y2 - y1) / dist
        
        # Step through grid
        curr_dist = 0
        step = self.grid.resolution
        
        while curr_dist < dist:
            cx = x1 + dx * curr_dist
            cy = y1 + dy * curr_dist
            
            if self.grid.get_value(cx, cy) > 50:
                return False
                
            curr_dist += step
            
        return True




################################################################################
PATH: ./core/game/rules.py
################################################################################
"""
Game Rules - Configuration & Constants

Defines all game parameters and rules:
- Shot cooldowns (AI vs Human)
- Match duration
- Win conditions
- Scoring rules

All values are loaded from config/game.yaml.
This module provides validation and defaults.
"""

from dataclasses import dataclass


@dataclass
class GameRules:
    """
    Game rules and parameters.
    
    Loaded from config/game.yaml but provides sensible defaults.
    """
    
    # Match timing
    match_duration_seconds: float = 180.0  # 3 minutes default
    
    # Shot cooldowns
    human_shot_cooldown: float = 5.0  # Human can shoot every 5 seconds
    ai_shot_cooldown: float = 3.0     # AI can shoot every 3 seconds
    
    # Win conditions
    max_hits_to_win: int = 10         # First to 10 hits wins
    sudden_death: bool = False        # Continue after time expires?
    
    # Shot mechanics
    shot_range_meters: float = 5.0    # Maximum effective range
    shot_speed_mps: float = 10.0      # Shot travel speed (for animation)
    
    @classmethod
    def from_config(cls, config_dict):
        """
        Create rules from config dictionary.
        
        Args:
            config_dict: Parsed YAML from config/game.yaml['match']
            
        Returns:
            GameRules instance with validated values
        """
        # Map config keys to class attributes
        mapped = {}
        
        if 'duration_seconds' in config_dict:
            mapped['match_duration_seconds'] = config_dict['duration_seconds']
        if 'human_shot_seconds' in config_dict:
            mapped['human_shot_cooldown'] = config_dict['human_shot_seconds']
        if 'ai_shot_seconds' in config_dict:
            mapped['ai_shot_cooldown'] = config_dict['ai_shot_seconds']
        if 'max_hits' in config_dict:
            mapped['max_hits_to_win'] = config_dict['max_hits']
        if 'sudden_death' in config_dict:
            mapped['sudden_death'] = config_dict['sudden_death']
        if 'range_m' in config_dict:
            mapped['shot_range_meters'] = config_dict['range_m']
        if 'speed_mps' in config_dict:
            mapped['shot_speed_mps'] = config_dict['speed_mps']
        
        return cls(**mapped)
    
    def validate(self):
        """
        Validate rule consistency.
        
        Raises:
            ValueError: If rules are inconsistent or invalid
        """
        if self.match_duration_seconds <= 0:
            raise ValueError("Match duration must be positive")
        if self.human_shot_cooldown <= 0 or self.ai_shot_cooldown <= 0:
            raise ValueError("Cooldowns must be positive")
        if self.max_hits_to_win < 1:
            raise ValueError("Max hits must be >= 1")



################################################################################
PATH: ./core/game/state.py
################################################################################
"""
Game State - Complete Game Status

Maintains the complete state of the ongoing match:
- Robot scores (hits given/received)
- Match timer (elapsed, remaining)
- Cooldown timers (next allowed shot for each robot)
- Game status (calibration, playing, paused, finished)
- Winner information

This is a pure data structure with no logic.
All state modifications are done by game_engine.py.
"""

from dataclasses import dataclass
from enum import Enum

class GameStatus(Enum):
    """Game lifecycle states"""
    CALIBRATION = "calibration"
    READY = "ready"
    PLAYING = "playing"
    PAUSED = "paused"
    FINISHED = "finished"


@dataclass
class RobotScore:
    """Per-robot scoring information"""
    robot_id: int
    hits_inflicted: int = 0  # Hits this robot scored on enemy
    hits_received: int = 0   # Hits this robot took
    shots_fired: int = 0     # Total shots attempted
    

@dataclass
class GameState:
    """
    Complete game state snapshot.
    
    This is the single source of truth for game status.
    Immutable between ticks - game_engine creates new state each tick.
    """
    status: GameStatus
    
    # Time tracking
    match_start_time: float  # Unix timestamp
    current_time: float      # Unix timestamp
    match_duration: float    # Total match length in seconds
    
    # Scores
    robot_4_score: RobotScore  # AI robot
    robot_5_score: RobotScore  # Human robot
    
    # Cooldowns (Unix timestamps)
    next_shot_robot_4: float
    next_shot_robot_5: float
    
    # Winner info (None if ongoing)
    winner: str = None  # "AI", "HUMAN", or "DRAW"
    
    @property
    def time_remaining(self):
        """Calculate remaining match time in seconds."""
        elapsed = self.current_time - self.match_start_time
        return max(0, self.match_duration - elapsed)
    
    @property
    def can_shoot_ai(self):
        """Check if AI cooldown allows shooting."""
        return self.current_time >= self.next_shot_robot_4
    
    @property
    def can_shoot_human(self):
        """Check if human cooldown allows shooting."""
        return self.current_time >= self.next_shot_robot_5



################################################################################
PATH: ./core/game/timers.py
################################################################################
"""
Timers - Cooldown & Match Time Management

Manages all time-based game mechanics:
- Match timer (total elapsed, remaining)
- Shot cooldowns per robot
- Temporary buffs/debuffs (future)

All times are in seconds (float).
Uses system time (time.time()) for absolute timing.
"""

import time


class Timer:
    """Simple countdown timer."""
    
    def __init__(self, duration_seconds):
        self.duration = duration_seconds
        self.start_time = None
        
    def start(self):
        """Start the timer."""
        self.start_time = time.time()
        
    def elapsed(self):
        """Get elapsed time in seconds."""
        if self.start_time is None:
            return 0.0
        return time.time() - self.start_time
    
    def remaining(self):
        """Get remaining time in seconds."""
        return max(0.0, self.duration - self.elapsed())
    
    def is_expired(self):
        """Check if timer has expired."""
        return self.elapsed() >= self.duration
    
    def reset(self):
        """Reset timer to start."""
        self.start_time = None


class CooldownManager:
    """
    Manages shot cooldowns for both robots.
    
    Tracks when each robot last fired and when they can fire again.
    """
    
    def __init__(self, ai_cooldown_sec, human_cooldown_sec):
        """
        Initialize cooldown manager.
        
        Args:
            ai_cooldown_sec: AI robot cooldown duration
            human_cooldown_sec: Human robot cooldown duration
        """
        self.ai_cooldown = ai_cooldown_sec
        self.human_cooldown = human_cooldown_sec
        
        self.last_shot_ai = 0.0
        self.last_shot_human = 0.0
        
    def can_shoot_ai(self):
        """Check if AI can shoot now."""
        return time.time() >= self.last_shot_ai + self.ai_cooldown
    
    def can_shoot_human(self):
        """Check if human can shoot now."""
        return time.time() >= self.last_shot_human + self.human_cooldown
    
    def register_shot_ai(self):
        """
        Register that AI fired.
        
        Logs:
            [TIMER] AI cooldown started (Xs remaining)
        """
        self.last_shot_ai = time.time()
        
    def register_shot_human(self):
        """
        Register that human fired.
        
        Logs:
            [TIMER] Human cooldown started (Xs remaining)
        """
        self.last_shot_human = time.time()
    
    def time_until_next_shot_ai(self):
        """Get seconds until AI can shoot again."""
        return max(0.0, self.last_shot_ai + self.ai_cooldown - time.time())
    
    def time_until_next_shot_human(self):
        """Get seconds until human can shoot again."""
        return max(0.0, self.last_shot_human + self.human_cooldown - time.time())



################################################################################
PATH: ./core/ia/behavior_tree.py
################################################################################
"""
Behavior Tree - AI Decision Making Framework

Implements a composable behavior tree for AI decision making:
- Selector nodes (try children until one succeeds)
- Sequence nodes (execute all children in order)
- Condition nodes (check world state)
- Action nodes (output decisions)

The AI does NOT modify game state directly.
It only returns INTENTIONS: (target_position, fire_request).

Logs: [BT] Node X succeeded/failed
"""

from enum import Enum
from abc import ABC, abstractmethod
from .decisions import (
    is_enemy_too_close,
    has_line_of_sight,
    is_optimal_firing_range,
    should_retreat,
    find_nearest_cover,
    calculate_flank_position
)


class NodeStatus(Enum):
    """Behavior tree node execution status."""
    SUCCESS = "success"
    FAILURE = "failure"
    RUNNING = "running"


class BTNode(ABC):
    """
    Base class for all behavior tree nodes.
    
    All nodes implement tick() which returns a NodeStatus.
    """
    
    def __init__(self, name):
        self.name = name
    
    @abstractmethod
    def tick(self, context):
        """
        Execute this node with given context.
        
        Args:
            context: dict with world state, robot poses, etc.
            
        Returns:
            NodeStatus
        """
        pass


class Selector(BTNode):
    """
    Selector node: tries children until one succeeds.
    
    Returns SUCCESS if any child succeeds.
    Returns FAILURE if all children fail.
    """
    
    def __init__(self, name, children):
        super().__init__(name)
        self.children = children
    
    def tick(self, context):
        for child in self.children:
            status = child.tick(context)
            if status == NodeStatus.SUCCESS:
                return NodeStatus.SUCCESS
            elif status == NodeStatus.RUNNING:
                return NodeStatus.RUNNING
        return NodeStatus.FAILURE


class Sequence(BTNode):
    """
    Sequence node: executes children in order.
    
    Returns SUCCESS if all children succeed.
    Returns FAILURE if any child fails.
    """
    
    def __init__(self, name, children):
        super().__init__(name)
        self.children = children
    
    def tick(self, context):
        for child in self.children:
            status = child.tick(context)
            if status == NodeStatus.FAILURE:
                return NodeStatus.FAILURE
            elif status == NodeStatus.RUNNING:
                return NodeStatus.RUNNING
        return NodeStatus.SUCCESS


class Condition(BTNode):
    """
    Condition node: checks a predicate function.
    
    Returns SUCCESS if predicate is True, FAILURE otherwise.
    """
    
    def __init__(self, name, predicate_fn):
        super().__init__(name)
        self.predicate_fn = predicate_fn
    
    def tick(self, context):
        if self.predicate_fn(context):
            return NodeStatus.SUCCESS
        return NodeStatus.FAILURE


class Action(BTNode):
    """
    Action node: executes an action function.
    
    The action function modifies context['ai_output'] with decisions.
    """
    
    def __init__(self, name, action_fn):
        super().__init__(name)
        self.action_fn = action_fn
    
    def tick(self, context):
        return self.action_fn(context)


# --- Action Functions ---

def action_retreat_to_cover(context):
    """
    Action: Find cover and set as target.
    
    Modifies context['ai_output'] with retreat target.
    """
    cover_pos = find_nearest_cover(context)
    
    if cover_pos is not None:
        context['ai_output']['target_position'] = cover_pos
        context['ai_output']['state'] = 'RETREAT'
        context['ai_output']['fire_request'] = False
        print("[BT] Action: RETREAT to cover at ({:.2f}, {:.2f})".format(
            cover_pos[0], cover_pos[1]))
        return NodeStatus.SUCCESS
    else:
        print("[BT] Action: RETREAT failed - no cover found")
        return NodeStatus.FAILURE


def action_aim_and_fire(context):
    """
    Action: Aim at enemy and request fire.
    
    Modifies context['ai_output'] with fire request.
    """
    human_pose = context.get('human_pose')
    
    if human_pose is None:
        return NodeStatus.FAILURE
    
    context['ai_output']['target_position'] = None  # Stay in place
    context['ai_output']['target_orientation'] = human_pose[:2]  # Aim at enemy
    context['ai_output']['fire_request'] = True
    context['ai_output']['state'] = 'ATTACK'
    
    print("[BT] Action: AIM AND FIRE at enemy")
    return NodeStatus.SUCCESS


def action_find_flank_position(context):
    """
    Action: Calculate flanking position.
    """
    flank_pos = calculate_flank_position(context)
    
    if flank_pos is not None:
        context['ai_output']['target_position'] = flank_pos
        context['ai_output']['state'] = 'FLANK'
        context['ai_output']['fire_request'] = False
        print("[BT] Action: FLANK to ({:.2f}, {:.2f})".format(
            flank_pos[0], flank_pos[1]))
        return NodeStatus.SUCCESS
    else:
        return NodeStatus.FAILURE


def action_move_to_flank(context):
    """
    Action: Move towards flanking position.
    """
    # This is handled by trajectory follower, just confirm we have a target
    target = context['ai_output'].get('target_position')
    if target is not None:
        print("[BT] Action: Moving to flank position")
        return NodeStatus.RUNNING
    return NodeStatus.FAILURE


def action_hunt_enemy(context):
    """
    Action: Move towards enemy's last known position.
    """
    human_pose = context.get('human_pose')
    
    if human_pose is not None:
        context['ai_output']['target_position'] = human_pose[:2]
        context['ai_output']['state'] = 'HUNT'
        context['ai_output']['fire_request'] = False
        print("[BT] Action: HUNT - moving to enemy position")
        return NodeStatus.SUCCESS
    
    return NodeStatus.FAILURE


def build_ai_behavior_tree():
    """
    Construct the main AI behavior tree.
    
    Structure:
    
    Selector (Root)
      +-- Sequence (SURVIVAL)
      |   +-- Condition: "enemy too close?"
      |   +-- Action: "retreat to cover"
      |
      +-- Sequence (ATTACK)
      |   +-- Condition: "have line of sight?"
      |   +-- Condition: "in optimal range?"
      |   +-- Action: "aim and request fire"
      |
      +-- Sequence (FLANK)
          +-- Action: "find flanking position"
          +-- Action: "move to flank"
    
    Returns:
        BTNode: root of the behavior tree
    """
    # SURVIVAL branch: retreat if enemy too close
    survival_sequence = Sequence("SURVIVAL", [
        Condition("enemy_too_close", is_enemy_too_close),
        Action("retreat_to_cover", action_retreat_to_cover)
    ])
    
    # ATTACK branch: fire if we have clear shot in optimal range
    attack_sequence = Sequence("ATTACK", [
        Condition("has_line_of_sight", has_line_of_sight),
        Condition("in_optimal_range", is_optimal_firing_range),
        Action("aim_and_fire", action_aim_and_fire)
    ])
    
    # FLANK branch: try to get into better position
    flank_sequence = Sequence("FLANK", [
        Action("find_flank_position", action_find_flank_position),
        Action("move_to_flank", action_move_to_flank)
    ])
    
    # HUNT branch: fallback - just chase enemy
    hunt_action = Action("hunt_enemy", action_hunt_enemy)
    
    # Root selector: try survival first, then attack, then flank, then hunt
    root = Selector("AI_ROOT", [
        survival_sequence,
        attack_sequence,
        flank_sequence,
        hunt_action
    ])
    
    print("[BT] Behavior tree constructed")
    return root


class BehaviorTreeExecutor:
    """
    Executor that runs the behavior tree each tick.
    """
    
    def __init__(self):
        self.tree = build_ai_behavior_tree()
    
    def execute(self, context):
        """
        Execute behavior tree with given context.
        
        Args:
            context: World state dict. Must contain:
                - ai_pose: (x, y, theta)
                - human_pose: (x, y, theta)
                - occupancy_grid: grid object
                
        Returns:
            dict: AI output decisions
        """
        # Initialize output structure
        context['ai_output'] = {
            'target_position': None,
            'target_orientation': None,
            'fire_request': False,
            'state': 'IDLE',
            'has_los': False
        }
        
        # Check LOS for output
        context['ai_output']['has_los'] = has_line_of_sight(context)
        
        # Execute tree
        status = self.tree.tick(context)
        
        print("[BT] Tree execution: {}".format(status.value))
        
        return context['ai_output']



################################################################################
PATH: ./core/ia/decisions.py
################################################################################
"""
Decisions - Tactical Decision Functions

Provides tactical assessment functions used by behavior tree conditions:
- Is enemy too close? (threat assessment)
- Do we have line of sight? (visibility check)
- Are we in optimal firing range?
- Is cover available nearby?
- Should we retreat?

All functions take context dict and return bool or tactical value.

Logs: [DECISION] Assessment X: value Y
"""

import numpy as np
from typing import Dict, Tuple, Optional, List


def is_enemy_too_close(context: Dict, threshold_m: float = 0.8) -> bool:
    """
    Check if enemy is dangerously close.
    
    Args:
        context: World state with robot poses
        threshold_m: Danger threshold in meters
        
    Returns:
        True if enemy within threshold
    """
    ai_pos = context['ai_pose'][:2]
    human_pos = context['human_pose'][:2]
    distance = np.linalg.norm(np.array(ai_pos) - np.array(human_pos))
    
    return distance < threshold_m


def has_line_of_sight(context: Dict) -> bool:
    """
    Check if AI has clear line of sight to enemy.
    
    Uses raycast from core.world to check for obstacles.
    
    Args:
        context: World state with robot poses and raycast
        
    Returns:
        True if clear line of sight exists
        
    Logs:
        [DECISION] LOS check: CLEAR/BLOCKED
    """
    ai_pose = context.get('ai_pose')
    human_pose = context.get('human_pose')
    raycast = context.get('raycast_sys')
    
    if ai_pose is None or human_pose is None:
        return False
    
    if raycast is None:
        # No raycast system available, assume clear LOS
        print("[DECISION] LOS check: NO RAYCAST SYSTEM")
        return True
    
    # Use raycast's internal LOS check
    ai_pos = ai_pose[:2]
    human_pos = human_pose[:2]
    
    los_clear = raycast._check_line_of_sight(ai_pos, human_pos)
    
    status = "CLEAR" if los_clear else "BLOCKED"
    print("[DECISION] LOS check: {}".format(status))
    
    return los_clear


def is_optimal_firing_range(context: Dict, 
                            min_range: float = 1.2, 
                            max_range: float = 3.5) -> bool:
    """
    Check if enemy is in optimal firing range.
    
    Too close: risk of being hit back
    Too far: accuracy decreases
    
    Args:
        context: World state
        min_range: Minimum safe distance
        max_range: Maximum effective distance
        
    Returns:
        True if in optimal range
    """
    ai_pose = context.get('ai_pose')
    human_pose = context.get('human_pose')
    
    if ai_pose is None or human_pose is None:
        return False
    
    ai_pos = np.array(ai_pose[:2])
    human_pos = np.array(human_pose[:2])
    
    distance = np.linalg.norm(ai_pos - human_pos)
    
    in_range = min_range <= distance <= max_range
    
    print("[DECISION] Firing range check: distance={:.2f}m, optimal={}".format(
        distance, in_range))
    
    return in_range


def find_nearest_cover(context: Dict) -> Optional[Tuple[float, float]]:
    """
    Find nearest cover position relative to enemy.
    
    Cover = obstacle that blocks line of sight to enemy.
    
    Args:
        context: World state with occupancy grid
        
    Returns:
        (x, y) position of best cover, or None
        
    Algorithm:
        1. Get all obstacle cells from grid
        2. For each obstacle, check if it blocks LOS to enemy
        3. Rank by:
           - Distance to AI (closer is better)
           - Cover effectiveness (blocks LOS well)
        4. Return best position
    """
    ai_pose = context.get('ai_pose')
    human_pose = context.get('human_pose')
    grid = context.get('occupancy_grid')
    
    if ai_pose is None or human_pose is None or grid is None:
        return None
    
    ai_pos = np.array(ai_pose[:2])
    human_pos = np.array(human_pose[:2])
    
    # Direction from enemy to AI
    direction = ai_pos - human_pos
    dist = np.linalg.norm(direction)
    if dist < 0.1:
        return None
    direction = direction / dist
    
    # Look for cover positions: move perpendicular to enemy direction
    perpendicular = np.array([-direction[1], direction[0]])
    
    # Check positions to the left and right of current position
    cover_distance = 0.5  # meters from current position
    
    candidates = [
        ai_pos + perpendicular * cover_distance,
        ai_pos - perpendicular * cover_distance,
        ai_pos + direction * cover_distance,  # Move away from enemy
    ]
    
    # Find valid cover position (within arena bounds)
    for candidate in candidates:
        x, y = candidate
        if 0 < x < grid.width_m and 0 < y < grid.height_m:
            if not grid.is_occupied(x, y):
                print("[DECISION] Cover found at ({:.2f}, {:.2f})".format(x, y))
                return (x, y)
    
    print("[DECISION] No cover found")
    return None


def should_retreat(context: Dict) -> bool:
    """
    Comprehensive retreat decision.
    
    Retreat if:
    - Enemy too close AND has LOS
    - Low health (future feature)
    - Surrounded
    
    Args:
        context: World state
        
    Returns:
        True if should retreat
    """
    too_close = is_enemy_too_close(context)
    los = has_line_of_sight(context)
    
    should_run = too_close and los
    
    if should_run:
        print("[DECISION] RETREAT triggered: enemy too close with LOS")
    
    return should_run


def calculate_flank_position(context: Dict) -> Optional[Tuple[float, float]]:
    """
    Calculate optimal flanking position.
    
    Goal: position that:
    - Gives AI line of sight to enemy
    - Is NOT in enemy's current line of sight
    - Uses cover for approach
    
    Args:
        context: World state
        
    Returns:
        (x, y) target flanking position
        
    Algorithm:
        1. Get enemy position and orientation
        2. Find positions 90 deg left/right of enemy facing
        3. Filter by cover availability during approach
        4. Choose closest valid position
    """
    ai_pose = context.get('ai_pose')
    human_pose = context.get('human_pose')
    grid = context.get('occupancy_grid')
    
    if ai_pose is None or human_pose is None:
        return None
    
    ai_pos = np.array(ai_pose[:2])
    human_pos = np.array(human_pose[:2])
    human_theta = human_pose[2] if len(human_pose) > 2 else 0.0
    
    # Enemy facing direction
    enemy_facing = np.array([np.cos(human_theta), np.sin(human_theta)])
    
    # Flanking positions: 90 degrees to enemy facing
    flank_distance = 1.5  # meters from enemy
    
    # Left flank (perpendicular)
    left_perp = np.array([-enemy_facing[1], enemy_facing[0]])
    left_flank = human_pos + left_perp * flank_distance
    
    # Right flank
    right_perp = np.array([enemy_facing[1], -enemy_facing[0]])
    right_flank = human_pos + right_perp * flank_distance
    
    # Choose flank closest to AI
    dist_left = np.linalg.norm(ai_pos - left_flank)
    dist_right = np.linalg.norm(ai_pos - right_flank)
    
    if dist_left < dist_right:
        chosen_flank = left_flank
    else:
        chosen_flank = right_flank
    
    x, y = chosen_flank
    
    # Validate position is within arena
    if grid is not None:
        if not (0 < x < grid.width_m and 0 < y < grid.height_m):
            print("[DECISION] Flank position out of bounds")
            return None
        if grid.is_occupied(x, y):
            print("[DECISION] Flank position occupied")
            return None
    
    print("[DECISION] Flank position: ({:.2f}, {:.2f})".format(x, y))
    return (x, y)


def get_distance_to_enemy(context: Dict) -> float:
    """
    Get distance between AI and enemy.
    
    Args:
        context: World state
        
    Returns:
        Distance in meters, or infinity if poses unknown
    """
    ai_pose = context.get('ai_pose')
    human_pose = context.get('human_pose')
    
    if ai_pose is None or human_pose is None:
        return float('inf')
    
    ai_pos = np.array(ai_pose[:2])
    human_pos = np.array(human_pose[:2])
    
    return float(np.linalg.norm(ai_pos - human_pos))


def get_angle_to_enemy(context: Dict) -> float:
    """
    Get angle from AI to enemy.
    
    Args:
        context: World state
        
    Returns:
        Angle in radians, or 0 if poses unknown
    """
    ai_pose = context.get('ai_pose')
    human_pose = context.get('human_pose')
    
    if ai_pose is None or human_pose is None:
        return 0.0
    
    dx = human_pose[0] - ai_pose[0]
    dy = human_pose[1] - ai_pose[1]
    
    return float(np.arctan2(dy, dx))



################################################################################
PATH: ./core/ia/__init__.py
################################################################################



################################################################################
PATH: ./core/ia/planners/a_star.py
################################################################################
"""
A* Path Planning Algorithm

Implements A* pathfinding on the occupancy grid:
- Finds shortest collision-free path
- Uses configurable heuristics
- Handles dynamic obstacles (robots)
- Returns waypoint list in meters

The planner works on the inflated costmap from core/world.

Logs: [ASTAR] Path found: N waypoints, length: M meters
"""

import numpy as np
import heapq
from typing import List, Tuple, Optional
from .heuristics import euclidean_distance, manhattan_distance, diagonal_distance


class AStarPlanner:
    """
    A* pathfinding on 2D occupancy grid.
    
    Finds optimal path from start to goal avoiding obstacles.
    """
    
    def __init__(self, occupancy_grid, heuristic='euclidean'):
        """
        Initialize A* planner.
        
        Args:
            occupancy_grid: OccupancyGrid from core/world
            heuristic: 'euclidean', 'manhattan', or 'diagonal'
        """
        self.grid = occupancy_grid
        self.heuristic_name = heuristic
        
    def _heuristic(self, cell1, cell2):
        """Calculate heuristic cost."""
        if self.heuristic_name == 'manhattan':
            return manhattan_distance(cell1, cell2)
        elif self.heuristic_name == 'diagonal':
            return diagonal_distance(cell1, cell2)
        else:
            return euclidean_distance(cell1, cell2)

    
    def plan(self, start_m: Tuple[float, float], 
             goal_m: Tuple[float, float]) -> Optional[List[Tuple[float, float]]]:
        """
        Find path from start to goal.
        """
        start_cell = self.grid.world_to_grid(*start_m)
        goal_cell = self.grid.world_to_grid(*goal_m)
        
        print(f"[ASTAR] Planning from {start_m} to {goal_m}")
        print(f"[ASTAR]   Start cell: {start_cell}, Goal cell: {goal_cell}")
        print(f"[ASTAR]   Grid size: {self.grid.grid.shape}")
        
        # Check bounds
        if not self.grid._is_valid_cell(*start_cell):
            print(f"[ASTAR]   ERROR: Start cell {start_cell} is OUT OF BOUNDS")
            return None
        if not self.grid._is_valid_cell(*goal_cell):
            print(f"[ASTAR]   ERROR: Goal cell {goal_cell} is OUT OF BOUNDS")
            return None
        
        # Note: We don't check if start or goal is occupied
        # Start: robot is already there
        # Goal: enemy robot is there - we WANT to go there!
            
        # Initialize
        open_set = []
        heapq.heappush(open_set, (0, start_cell))
        
        came_from = {}
        g_score = {start_cell: 0}
        f_score = {start_cell: self._heuristic(start_cell, goal_cell)}
        
        closed_set = set()
        
        while open_set:
            current = heapq.heappop(open_set)[1]
            
            if current == goal_cell:
                path_cells = self._reconstruct_path(came_from, current)
                simplified = self._simplify_path(path_cells)
                result = [self.grid.grid_to_world(r, c) for r, c in simplified]
                print(f"[ASTAR]   SUCCESS: Path found with {len(result)} waypoints")
                return result
            
            closed_set.add(current)
            
            for neighbor, cost in self._get_neighbors(current, start_cell):
                if neighbor in closed_set:
                    continue
                    
                tentative_g = g_score[current] + cost
                
                if neighbor not in g_score or tentative_g < g_score[neighbor]:
                    came_from[neighbor] = current
                    g_score[neighbor] = tentative_g
                    f = tentative_g + self._heuristic(neighbor, goal_cell)
                    f_score[neighbor] = f
                    heapq.heappush(open_set, (f, neighbor))
        
        print(f"[ASTAR]   FAILED: No path found (explored {len(closed_set)} cells)")
        return None
        
    def _get_neighbors(self, cell, start_cell=None, ignore_radius=5):
        """Get valid neighbor cells (8-connected).
        
        Uses COSTMAP (inflated) for collision checking, not raw grid.
        If start_cell is provided and we're within ignore_radius of it,
        we ignore obstacles (to escape from robot's own footprint).
        """
        row, col = cell
        neighbors = []
        
        # Use costmap if available (inflated obstacles), fallback to grid
        collision_grid = getattr(self.grid, 'costmap', self.grid.grid)
        
        for dr in [-1, 0, 1]:
            for dc in [-1, 0, 1]:
                if dr == 0 and dc == 0:
                    continue
                    
                r, c = row + dr, col + dc
                
                # Check validity
                if self.grid._is_valid_cell(r, c):
                    # If near start, ignore obstacles (robot's own footprint)
                    if start_cell is not None:
                        dist_to_start = abs(r - start_cell[0]) + abs(c - start_cell[1])
                        if dist_to_start <= ignore_radius:
                            cost = 1.414 if (dr != 0 and dc != 0) else 1.0
                            neighbors.append(((r, c), cost))
                            continue
                    
                    # Check occupancy on COSTMAP (inflated, safety margins)
                    if collision_grid[r, c] < 0.5:
                        cost = 1.414 if (dr != 0 and dc != 0) else 1.0
                        neighbors.append(((r, c), cost))
                        
        return neighbors
    
    def _reconstruct_path(self, came_from, current):
        """Reconstruct path from goal to start."""
        total_path = [current]
        while current in came_from:
            current = came_from[current]
            total_path.append(current)
        return total_path[::-1] # Reverse
    
    def _simplify_path(self, path_cells):
        """Simple path smoothing (skip minimal steps)."""
        if len(path_cells) <= 2:
            return path_cells
            
        simplified = [path_cells[0]]
        for i in range(1, len(path_cells)-1):
            # Keep every Nth point or check Line of Sight (expensive)
            # For now, just return all points to be safe for trajectory follower
            simplified.append(path_cells[i])
            
        simplified.append(path_cells[-1])
        return simplified



################################################################################
PATH: ./core/ia/planners/heuristics.py
################################################################################
"""
Heuristics - Cost Functions for Path Planning

Provides heuristic functions for A* and other planners:
- Euclidean distance (standard, admissible)
- Manhattan distance (grid-based)
- Diagonal distance (Chebyshev + diagonal cost)
- Custom costmap-aware heuristics

All heuristics must be admissible (never overestimate).
"""

import numpy as np
from typing import Tuple


def euclidean_distance(cell1: Tuple[int, int], cell2: Tuple[int, int]) -> float:
    """
    Euclidean distance heuristic.
    
    Most accurate for free space planning.
    
    Args:
        cell1: (row, col)
        cell2: (row, col)
        
    Returns:
        Euclidean distance
    """
    return np.sqrt((cell1[0] - cell2[0])**2 + (cell1[1] - cell2[1])**2)


def manhattan_distance(cell1: Tuple[int, int], cell2: Tuple[int, int]) -> float:
    """
    Manhattan (L1) distance.
    
    Good for 4-connected grids.
    
    Args:
        cell1: (row, col)
        cell2: (row, col)
        
    Returns:
        Manhattan distance
    """
    return abs(cell1[0] - cell2[0]) + abs(cell1[1] - cell2[1])


def diagonal_distance(cell1: Tuple[int, int], cell2: Tuple[int, int]) -> float:
    """
    Diagonal distance (Chebyshev + diagonal cost).
    
    Good for 8-connected grids with diagonal cost √2.
    
    Args:
        cell1: (row, col)
        cell2: (row, col)
        
    Returns:
        Diagonal-aware distance
    """
    dx = abs(cell1[0] - cell2[0])
    dy = abs(cell1[1] - cell2[1])
    
    # Cost: move diagonal (√2 ≈ 1.414) then straight (1.0)
    D = 1.0  # Straight cost
    D2 = 1.414  # Diagonal cost
    
    return D * (dx + dy) + (D2 - 2*D) * min(dx, dy)


def costmap_aware_heuristic(cell1: Tuple[int, int], 
                            cell2: Tuple[int, int],
                            costmap) -> float:
    """
    Costmap-aware heuristic.
    
    Incorporates obstacle proximity into heuristic.
    Still admissible if base is euclidean.
    
    Args:
        cell1: (row, col)
        cell2: (row, col)
        costmap: OccupancyGrid with inflation
        
    Returns:
        Modified heuristic favoring safer paths
    """
    base_h = euclidean_distance(cell1, cell2)
    
    # Optional: add small cost penalty based on average costmap value
    # Must remain admissible!
    
    return base_h



################################################################################
PATH: ./core/ia/planners/__init__.py
################################################################################



################################################################################
PATH: ./core/ia/planners/path_utils.py
################################################################################
"""
Path Utilities - Path Processing and Optimization

Utilities for working with planned paths:
- Path smoothing
- Waypoint simplification (Douglas-Peucker)
- Path validation
- Distance calculation
- Interpolation

Takes raw A* output and makes it execution-ready.
"""

import numpy as np
from typing import List, Tuple, Optional


def smooth_path(waypoints: List[Tuple[float, float]], 
                weight_data: float = 0.5,
                weight_smooth: float = 0.3,
                tolerance: float = 0.01,
                max_iterations: int = 100) -> List[Tuple[float, float]]:
    """
    Smooth a path using gradient descent.
    
    Balances staying close to original path vs smoothness.
    
    Args:
        waypoints: Original path [(x1,y1), ...]
        weight_data: How much to stay close to original
        weight_smooth: How much to smooth
        tolerance: Convergence threshold
        max_iterations: Maximum iterations
        
    Returns:
        Smoothed path
    """
    if len(waypoints) <= 2:
        return waypoints
    
    # Convert to numpy array for easier manipulation
    path = np.array(waypoints, dtype=np.float64)
    smoothed = path.copy()
    
    n_points = len(path)
    
    for iteration in range(max_iterations):
        change = 0.0
        
        # Don't modify first and last points
        for i in range(1, n_points - 1):
            for j in range(2):  # x and y
                old_val = smoothed[i, j]
                
                # Data term: stay close to original
                data_term = weight_data * (path[i, j] - smoothed[i, j])
                
                # Smooth term: average of neighbors
                smooth_term = weight_smooth * (
                    smoothed[i-1, j] + smoothed[i+1, j] - 2 * smoothed[i, j]
                )
                
                smoothed[i, j] += data_term + smooth_term
                change += abs(smoothed[i, j] - old_val)
        
        # Check convergence
        if change < tolerance:
            break
    
    return [(float(p[0]), float(p[1])) for p in smoothed]


def simplify_path_douglas_peucker(waypoints: List[Tuple[float, float]], 
                                  epsilon: float = 0.05) -> List[Tuple[float, float]]:
    """
    Simplify path using Douglas-Peucker algorithm.
    
    Removes waypoints that are nearly collinear.
    
    Args:
        waypoints: Original path
        epsilon: Maximum deviation tolerance in meters
        
    Returns:
        Simplified path with fewer waypoints
        
    Algorithm:
        1. Find point farthest from line between start and end
        2. If distance < epsilon, remove all intermediate points
        3. Otherwise, recursively apply to [start, farthest] and [farthest, end]
    """
    if len(waypoints) <= 2:
        return waypoints
    
    # Convert to numpy for calculations
    points = np.array(waypoints)
    
    # Find point with maximum distance from line (start -> end)
    start = points[0]
    end = points[-1]
    
    # Line vector
    line_vec = end - start
    line_len = np.linalg.norm(line_vec)
    
    if line_len < 1e-10:
        # Start and end are same point
        return [waypoints[0], waypoints[-1]]
    
    line_unit = line_vec / line_len
    
    # Find perpendicular distances
    max_dist = 0.0
    max_idx = 0
    
    for i in range(1, len(points) - 1):
        # Vector from start to point
        vec_to_point = points[i] - start
        
        # Project onto line
        proj_length = np.dot(vec_to_point, line_unit)
        proj_point = start + proj_length * line_unit
        
        # Perpendicular distance
        dist = np.linalg.norm(points[i] - proj_point)
        
        if dist > max_dist:
            max_dist = dist
            max_idx = i
    
    # If max distance is less than epsilon, simplify to just endpoints
    if max_dist < epsilon:
        return [waypoints[0], waypoints[-1]]
    
    # Otherwise, recursively simplify
    left_simplified = simplify_path_douglas_peucker(waypoints[:max_idx + 1], epsilon)
    right_simplified = simplify_path_douglas_peucker(waypoints[max_idx:], epsilon)
    
    # Combine (avoid duplicating the split point)
    return left_simplified[:-1] + right_simplified


def validate_path(waypoints: List[Tuple[float, float]], 
                 occupancy_grid) -> bool:
    """
    Check if path is collision-free.
    
    Args:
        waypoints: Path to validate
        occupancy_grid: Current occupancy grid
        
    Returns:
        True if path is valid (no collisions)
    """
    if len(waypoints) < 2:
        return True
    
    # Check each waypoint
    for x, y in waypoints:
        if not (0 <= x <= occupancy_grid.width_m and 0 <= y <= occupancy_grid.height_m):
            return False
        if occupancy_grid.is_occupied(x, y):
            return False
    
    # Check line segments between waypoints
    resolution = occupancy_grid.resolution
    
    for i in range(len(waypoints) - 1):
        x1, y1 = waypoints[i]
        x2, y2 = waypoints[i + 1]
        
        # Sample points along segment
        dist = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)
        n_samples = max(2, int(dist / resolution) + 1)
        
        for t in np.linspace(0, 1, n_samples):
            x = x1 + t * (x2 - x1)
            y = y1 + t * (y2 - y1)
            
            if occupancy_grid.is_occupied(x, y):
                return False
    
    return True


def calculate_path_length(waypoints: List[Tuple[float, float]]) -> float:
    """
    Calculate total path length in meters.
    
    Args:
        waypoints: Path [(x1,y1), ...]
        
    Returns:
        Total length in meters
    """
    if len(waypoints) < 2:
        return 0.0
    
    length = 0.0
    for i in range(len(waypoints) - 1):
        dx = waypoints[i+1][0] - waypoints[i][0]
        dy = waypoints[i+1][1] - waypoints[i][1]
        length += np.sqrt(dx**2 + dy**2)
    
    return length


def interpolate_path(waypoints: List[Tuple[float, float]], 
                    resolution: float = 0.05) -> List[Tuple[float, float]]:
    """
    Densify path by interpolating between waypoints.
    
    Useful for smooth visualization or fine-grained control.
    
    Args:
        waypoints: Sparse path
        resolution: Desired spacing in meters
        
    Returns:
        Dense path with points every ~resolution meters
    """
    if len(waypoints) < 2:
        return waypoints
    
    dense_path = [waypoints[0]]
    
    for i in range(len(waypoints) - 1):
        x1, y1 = waypoints[i]
        x2, y2 = waypoints[i + 1]
        
        # Distance between consecutive waypoints
        dist = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)
        
        if dist < resolution:
            # No interpolation needed
            dense_path.append((x2, y2))
            continue
        
        # Number of intermediate points
        n_points = int(dist / resolution)
        
        for j in range(1, n_points + 1):
            t = j / (n_points + 1)
            x = x1 + t * (x2 - x1)
            y = y1 + t * (y2 - y1)
            dense_path.append((x, y))
        
        dense_path.append((x2, y2))
    
    return dense_path


def get_path_curvature(waypoints: List[Tuple[float, float]]) -> List[float]:
    """
    Calculate curvature at each waypoint.
    
    Useful for speed adaptation (slow down at sharp turns).
    
    Args:
        waypoints: Path [(x1,y1), ...]
        
    Returns:
        List of curvature values (1/radius, 0 for straight)
    """
    if len(waypoints) < 3:
        return [0.0] * len(waypoints)
    
    curvatures = [0.0]  # First point has no curvature
    
    for i in range(1, len(waypoints) - 1):
        p0 = np.array(waypoints[i - 1])
        p1 = np.array(waypoints[i])
        p2 = np.array(waypoints[i + 1])
        
        # Vectors
        v1 = p1 - p0
        v2 = p2 - p1
        
        # Cross product magnitude (2D)
        cross = abs(v1[0] * v2[1] - v1[1] * v2[0])
        
        # Segment lengths
        len1 = np.linalg.norm(v1)
        len2 = np.linalg.norm(v2)
        
        if len1 < 1e-10 or len2 < 1e-10:
            curvatures.append(0.0)
            continue
        
        # Curvature approximation: 2 * sin(angle) / chord_length
        # Simplified: cross / (len1 * len2)
        curvature = cross / (len1 * len2)
        curvatures.append(curvature)
    
    curvatures.append(0.0)  # Last point has no curvature
    
    return curvatures



################################################################################
PATH: ./core/ia/strategy.py
################################################################################
"""
Strategy - High-Level AI Controller

Orchestrates the AI system:
1. Read world state
2. Execute behavior tree
3. Trigger path planning if needed
4. Return AI decisions (target, fire_request)

This is the main entry point for the AI subsystem.

Logs: [AI] State: ATTACK/FLANK/RETREAT, target=(x,y), fire=True/False
"""

import numpy as np
from typing import Dict, Tuple, Optional
from .behavior_tree import BehaviorTreeExecutor, NodeStatus
from .decisions import has_line_of_sight, is_enemy_too_close, is_optimal_firing_range
from .planners.a_star import AStarPlanner


class AIStrategy:
    """
    Main AI controller.
    
    Combines behavior tree + path planning to produce AI actions.
    """
    
    def __init__(self, config):
        """
        Initialize AI strategy.
        
        Args:
            config: AI configuration from config/ia.yaml
        """
        self.config = config
        self.behavior_tree = BehaviorTreeExecutor()
        self.planner = None  # Set when world is available
        
        # AI state
        self.current_path = []
        self.current_waypoint_idx = 0
        self.state = "IDLE"  # ATTACK, FLANK, RETREAT, HUNT
        
        # Decision rate control
        self.decision_interval = config.get('decision_rate', {}).get('replan_interval', 10)
        self.tick_count = 0
        
        print("[AI] Strategy initialized")
        
    def set_planner(self, occupancy_grid):
        """
        Initialize path planner with occupancy grid.
        
        Args:
            occupancy_grid: OccupancyGrid from core/world
        """
        heuristic = self.config.get('strategy', {}).get('heuristic', 'euclidean')
        self.planner = AStarPlanner(occupancy_grid, heuristic)
        print("[AI] Path planner initialized with {} heuristic".format(heuristic))
        
    def decide(self, world_state: Dict) -> Dict:
        """
        Main decision function called each game tick.
        
        Args:
            world_state: {
                'ai_pose': (x, y, theta),
                'human_pose': (x, y, theta),
                'occupancy_grid': grid object,
                'raycast_sys': raycast object,
                'game_time': float,
            }
            
        Returns:
            {
                'target_position': (x, y) or None,
                'target_orientation': theta or None,
                'fire_request': bool,
                'state': str (ATTACK/FLANK/RETREAT),
                'has_los': bool,
            }
        """
        self.tick_count += 1
        
        ai_pose = world_state.get('ai_pose')
        enemy_pose = world_state.get('human_pose')
        
        # Default Output
        decision = {
            'target_position': None,
            'target_orientation': None,
            'fire_request': False,
            'state': self.state,
            'has_los': False
        }
        
        if ai_pose is None or enemy_pose is None:
            return decision

        # Prepare context for behavior tree
        context = {
            'ai_pose': ai_pose,
            'human_pose': enemy_pose,
            'occupancy_grid': world_state.get('occupancy_grid'),
            'raycast_sys': world_state.get('raycast_sys'),
        }
        
        # Execute behavior tree
        bt_output = self.behavior_tree.execute(context)
        
        # Copy behavior tree decisions
        decision['fire_request'] = bt_output.get('fire_request', False)
        decision['has_los'] = bt_output.get('has_los', False)
        decision['state'] = bt_output.get('state', 'IDLE')
        self.state = decision['state']
        
        target_pos = bt_output.get('target_position')
        
        # Path Planning - only replan periodically or when target changes significantly
        if target_pos is not None and self.planner:
            should_replan = False
            
            # Replan if no path or on interval
            if not self.current_path:
                should_replan = True
            elif self.tick_count % self.decision_interval == 0:
                should_replan = True
            elif len(self.current_path) > 0:
                # Replan if target moved significantly
                last_goal = self.current_path[-1]
                dist_to_new = np.linalg.norm(
                    np.array(last_goal) - np.array(target_pos)
                )
                if dist_to_new > 0.5:
                    should_replan = True
            
            if should_replan:
                path = self.planner.plan(ai_pose[:2], target_pos)
                if path:
                    self.current_path = path
                    self.current_waypoint_idx = 0
                    print("[AI] New path planned: {} waypoints".format(len(path)))
        
        decision['target_position'] = target_pos
        
        # Log decision
        if self.tick_count % 30 == 0:  # Log every second at 30 FPS
            self._log_decision(decision)
        
        return decision
    
    def _log_decision(self, decision):
        """Log current AI decision."""
        target = decision.get('target_position')
        target_str = "({:.2f}, {:.2f})".format(target[0], target[1]) if target else "None"
        
        print("[AI] State: {}, target={}, fire={}, LOS={}".format(
            decision['state'],
            target_str,
            decision['fire_request'],
            decision['has_los']
        ))
    
    def get_next_waypoint(self) -> Optional[Tuple[float, float]]:
        """
        Get next waypoint from current path.
        
        Used by control module for trajectory following.
        
        Returns:
            (x, y) of next waypoint, or None if no path
        """
        if self.current_waypoint_idx >= len(self.current_path):
            return None
        return self.current_path[self.current_waypoint_idx]
    
    def advance_waypoint(self):
        """
        Mark current waypoint as reached, move to next.
        
        Called by control module when waypoint is reached.
        """
        self.current_waypoint_idx += 1
        
    def get_full_path(self) -> list:
        """
        Get complete current path for visualization.
        
        Returns:
            List of (x, y) waypoints
        """
        return self.current_path



################################################################################
PATH: ./core/__init__.py
################################################################################
# Package initialization files for Python modules

__all__ = []



################################################################################
PATH: ./core/world/coordinate_frames.py
################################################################################
"""
Coordinate Frames - Transformation Management

Manages all coordinate transformations:
- Camera → Arena Virtual (H_C2AV) - homography from projected corners
- Arena Virtual → World (scaling) - metric calibration
- Camera → World (H_C2W) - combined transform
- World → Pygame (projection display)
- World → Projector (overlay display)

All transformations are 2D homographies or affine transforms.

Logs: [TRANSFORM] prefix for all transform operations
"""

import numpy as np
import cv2
from typing import Tuple, List


class TransformManager:
    """
    Manages coordinate frame transformations.
    
    Stores and applies homographies between different coordinate systems.
    """
    
    def __init__(self):
        """Initialize transform manager."""
        self.H_C2AV = None  # Camera → Arena Virtual
        self.H_AV2W = None  # Arena Virtual → World (scaling)
        self.H_C2W = None   # Camera → World (combined)
        self.H_W2Proj = None  # World → Projector display
        
        self.scale_m_per_av = None  # Metric scale factor
        
    def set_camera_to_av(self, src_points: np.ndarray, dst_points: np.ndarray):
        """
        Compute H_C2AV from corner correspondences.
        
        Args:
            src_points: 4x2 array of camera pixel coordinates
            dst_points: 4x2 array of arena virtual coordinates (e.g. unit square)
            
        Computes homography using cv2.findHomography.
        
        Logs:
            [TRANSFORM] H_C2AV computed from 4 corners
        """
        self.H_C2AV, _ = cv2.findHomography(src_points, dst_points)
        self._update_combined()
        
    def set_av_to_world_scale(self, scale: float):
        """
        Set scaling from Arena Virtual to World (meters).
        
        Args:
            scale: meters per AV unit
            
        Creates scaling transform H_AV2W.
        
        Logs:
            [TRANSFORM] Scale set: 1.15 m/AV_unit
        """
        self.scale_m_per_av = scale
        self.H_AV2W = np.array([
            [scale, 0, 0],
            [0, scale, 0],
            [0, 0, 1]
        ], dtype=np.float32)
        self._update_combined()
        
    def _update_combined(self):
        """Update H_C2W = H_AV2W @ H_C2AV."""
        if self.H_C2AV is not None and self.H_AV2W is not None:
            self.H_C2W = self.H_AV2W @ self.H_C2AV
            
    def camera_to_world(self, u: float, v: float) -> Tuple[float, float]:
        """
        Transform camera pixel to world meters.
        
        Args:
            u, v: Camera pixel coordinates
            
        Returns:
            (x, y) in meters
        """
        if self.H_C2W is None:
            raise ValueError("H_C2W not set, run calibration first")
        
        # Homogeneous coordinates
        p_cam = np.array([u, v, 1.0])
        p_world = self.H_C2W @ p_cam
        
        # Normalize
        x = p_world[0] / p_world[2]
        y = p_world[1] / p_world[2]
        
        return (x, y)
    
    def world_to_projector(self, x: float, y: float, 
                          arena_width_m: float, arena_height_m: float,
                          proj_width_px: int, proj_height_px: int,
                          margin_px: int = 50) -> Tuple[int, int]:
        """
        Transform world position to projector pixel.
        
        Args:
            x, y: World position in meters
            arena_width_m, arena_height_m: Arena size
            proj_width_px, proj_height_px: Projector resolution
            margin_px: Safe zone margin
            
        Returns:
            (px, py) projector pixel coordinates
        """
        # Scale to projector (with margin)
        draw_width = proj_width_px - 2 * margin_px
        draw_height = proj_height_px - 2 * margin_px
        
        scale_x = draw_width / arena_width_m
        scale_y = draw_height / arena_height_m
        scale = min(scale_x, scale_y)  # Maintain aspect ratio
        
        px = margin_px + int(x * scale)
        py = margin_px + int((arena_height_m - y) * scale)  # Flip Y (pygame origin top-left)
        
        return (px, py)
    
    def batch_camera_to_world(self, points_cam: np.ndarray) -> np.ndarray:
        """
        Transform multiple camera points to world.
        
        Args:
            points_cam: Nx2 array of camera coordinates
            
        Returns:
            Nx2 array of world coordinates
        """
        if self.H_C2W is None:
            raise ValueError("H_C2W not set")
        
        # Add homogeneous coordinate
        ones = np.ones((points_cam.shape[0], 1))
        points_h = np.hstack([points_cam, ones])
        
        # Transform
        points_world_h = (self.H_C2W @ points_h.T).T
        
        # Normalize
        points_world = points_world_h[:, :2] / points_world_h[:, 2:3]
        
        return points_world



################################################################################
PATH: ./core/world/inflation.py
################################################################################
"""
Inflation - Obstacle Cost Inflation

Inflates obstacles in the costmap for safe path planning:
- Adds safety margin around obstacles
- Creates gradient for smoother planning
- Accounts for robot size

Uses distance transform for efficient computation.

Logs: [INFLATION] Inflated with radius: Xm -> Y cells
"""

import numpy as np
from scipy.ndimage import distance_transform_edt


class CostmapInflation:
    """
    Inflates obstacles in costmap for safe planning.
    
    Creates a cost gradient around obstacles.
    """
    
    def __init__(self, inflation_radius_m: float, resolution_m: float):
        """
        Initialize inflation.
        
        Args:
            inflation_radius_m: How far to inflate in meters
            resolution_m: Grid resolution
        """
        self.inflation_radius_m = inflation_radius_m
        self.resolution = resolution_m
        self.inflation_cells = int(inflation_radius_m / resolution_m)
        
    def inflate(self, binary_grid: np.ndarray) -> np.ndarray:
        """
        Inflate obstacles in grid.
        
        Args:
            binary_grid: Grid with 0 = free, 1 = occupied
            
        Returns:
            Inflated costmap with gradient (0-1 float)
            
        Algorithm:
            1. Compute distance transform (distance to nearest obstacle)
            2. Convert distances to costs:
               - d = 0: cost = 1.0 (occupied)
               - d < inflation_radius: cost = 1.0 - (d / radius)
               - d >= inflation_radius: cost = 0.0 (free)
               
        Logs:
            [INFLATION] Inflated grid with radius: 0.24m (12 cells)
        """
        # Distance transform: each cell = distance to nearest obstacle
        distances = distance_transform_edt(1 - binary_grid) * self.resolution
        
        # Convert to costs
        costmap = np.zeros_like(distances, dtype=np.float32)
        
        # Occupied cells
        costmap[binary_grid == 1] = 1.0
        
        # Inflated region
        inflation_mask = (distances > 0) & (distances < self.inflation_radius_m)
        costmap[inflation_mask] = 1.0 - (distances[inflation_mask] / self.inflation_radius_m)
        
        return costmap
    
    def inflate_discrete(self, binary_grid: np.ndarray, 
                        lethal: int = 100, inscribed: int = 99) -> np.ndarray:
        """
        Inflate with discrete cost values (ROS-style costmap).
        
        Args:
            binary_grid: Binary occupancy grid
            lethal: Cost for occupied cells (default 100)
            inscribed: Cost for cells within inflation radius
            
        Returns:
            Costmap with values [0, inscribed, lethal]
        """
        distances = distance_transform_edt(1 - binary_grid)
        
        costmap = np.zeros_like(distances, dtype=np.uint8)
        
        # Lethal obstacles
        costmap[binary_grid == 1] = lethal
        
        # Inscribed region
        inflation_mask = (distances > 0) & (distances <= self.inflation_cells)
        costmap[inflation_mask] = inscribed
        
        return costmap



################################################################################
PATH: ./core/world/__init__.py
################################################################################



################################################################################
PATH: ./core/world/occupancy_grid.py
################################################################################
"""
Occupancy Grid - 2D Spatial Representation

Represents the arena as a 2D grid with metric resolution:
- Cell values: 0 = free, 1 = occupied, 0-1 = partial
- Resolution: typically 2cm (0.02m) per cell
- Dimensions: derived from calibration

The grid stores:
- Static obstacles (from calibration)
- Dynamic obstacles (robots, updated each frame)
- Inflated obstacles (safety margins)

Coordinate system: meters, origin at arena bottom-left.

Logs: [GRID] prefix for all grid operations
"""

import numpy as np
from typing import Tuple, List


class OccupancyGrid:
    """
    2D occupancy grid for spatial representation.
    
    Provides efficient collision checking and spatial queries.
    """
    
    def __init__(self, width_m: float, height_m: float, resolution_m: float = 0.02):
        """
        Initialize occupancy grid.
        
        Args:
            width_m: Arena width in meters
            height_m: Arena height in meters  
            resolution_m: Grid cell size in meters (default 2cm)
            
        The grid will have dimensions:
            n_cols = ceil(width_m / resolution_m)
            n_rows = ceil(height_m / resolution_m)
            
        Logs:
            [GRID] Created grid: 2.85m x 1.90m @ 0.02m -> 143 x 95 cells
        """
        self.width_m = width_m
        self.height_m = height_m
        self.resolution = resolution_m
        
        self.n_cols = int(np.ceil(width_m / resolution_m))
        self.n_rows = int(np.ceil(height_m / resolution_m))
        
        # Grid data: 0 = free, 1 = occupied
        self.grid = np.zeros((self.n_rows, self.n_cols), dtype=np.float32)
        
        # Static obstacles (from calibration, never change)
        self.static_grid = np.zeros((self.n_rows, self.n_cols), dtype=np.float32)
        
    def world_to_grid(self, x_m: float, y_m: float) -> Tuple[int, int]:
        """
        Convert world coordinates to grid cell.
        
        Args:
            x_m: X position in meters
            y_m: Y position in meters
            
        Returns:
            (row, col) grid cell indices
        """
        col = int(x_m / self.resolution)
        row = int(y_m / self.resolution)
        return (row, col)
    
    def grid_to_world(self, row: int, col: int) -> Tuple[float, float]:
        """
        Convert grid cell to world coordinates (cell center).
        
        Args:
            row: Grid row
            col: Grid column
            
        Returns:
            (x_m, y_m) in meters
        """
        x_m = (col + 0.5) * self.resolution
        y_m = (row + 0.5) * self.resolution
        return (x_m, y_m)
    
    def is_occupied(self, x_m: float, y_m: float, threshold: float = 0.5) -> bool:
        """
        Check if a world position is occupied.
        
        Args:
            x_m: X position in meters
            y_m: Y position in meters
            threshold: Occupancy threshold (0-1)
            
        Returns:
            True if occupied
        """
        row, col = self.world_to_grid(x_m, y_m)
        
        if not self._is_valid_cell(row, col):
            return True  # Out of bounds = occupied
        
        return self.grid[row, col] >= threshold
    
    def get_value(self, x_m: float, y_m: float) -> float:
        """
        Get occupancy value at a world position.
        
        Args:
            x_m: X position in meters
            y_m: Y position in meters
            
        Returns:
            Occupancy value 0-100 (0=free, 100=occupied)
        """
        row, col = self.world_to_grid(x_m, y_m)
        
        if not self._is_valid_cell(row, col):
            return 100  # Out of bounds = occupied
        
        return self.grid[row, col] * 100
    
    def set_static_obstacles(self, obstacle_cells: List[Tuple[int, int]]):
        """
        Set static obstacles from calibration.
        
        Args:
            obstacle_cells: List of (row, col) occupied cells
            
        Logs:
            [GRID] Static obstacles set: N cells
        """
        for row, col in obstacle_cells:
            if self._is_valid_cell(row, col):
                self.static_grid[row, col] = 1.0
        
        # Par défaut, inflated_grid = static_grid (pas d'inflation)
        self.inflated_grid = self.static_grid.copy()
        self.grid = self.inflated_grid.copy()
        
        print(f"[GRID] Static obstacles set: {len(obstacle_cells)} cells")
    
    def inflate_static_obstacles(self, robot_radius_m: float, safety_margin_m: float = 0.0):
        """
        Génère une Costmap en gonflant les obstacles avec cv2.dilate (RAPIDE).
        
        Utilise OpenCV pour un calcul instantané même sur Raspberry Pi.
        Crée une zone tampon autour de chaque obstacle basée sur la taille physique du robot.
        
        Args:
            robot_radius_m: Rayon physique du robot (ex: 0.09m pour Turtlebot)
            safety_margin_m: Marge de sécurité supplémentaire (ex: 0.05m)
            
        Logs:
            [GRID] Inflation calculée: rayon + marge = total => kernel size
        """
        import cv2
        
        # 1. Calcul du rayon total à gonfler
        total_inflation_m = robot_radius_m + safety_margin_m
        
        # 2. Conversion Mètres -> Cellules (dynamique selon résolution)
        radius_cells = int(np.ceil(total_inflation_m / self.resolution))
        
        # 3. Calcul du Kernel pour OpenCV (doit être impair: 3x3, 5x5, 7x7...)
        kernel_size = (radius_cells * 2) + 1
        
        print(f"[GRID] Inflation calculée:")
        print(f"       Robot: {robot_radius_m}m + Marge: {safety_margin_m}m = {total_inflation_m}m")
        print(f"       Cellules: {total_inflation_m:.3f}m / {self.resolution}m = {radius_cells} cells")
        print(f"       Kernel OpenCV: {kernel_size}x{kernel_size}")
        
        # 4. Création du Kernel Circulaire (forme du robot)
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
        
        # 5. Application de la dilatation (ULTRA RAPIDE vs boucles Python)
        static_u8 = (self.static_grid * 255).astype(np.uint8)
        inflated_u8 = cv2.dilate(static_u8, kernel)
        
        # 6. Sauvegarde dans costmap (pour A*) et inflated_grid
        self.costmap = (inflated_u8 > 127).astype(np.float32)
        self.inflated_grid = self.costmap.copy()
        
        # Appliquer à la grille courante
        self.grid = self.inflated_grid.copy()
    
    def update_dynamic_obstacles(self, robot_poses: List[Tuple[float, float, float]], 
                                 robot_radius_m: float):
        """
        Update grid with current robot positions.
        
        Args:
            robot_poses: List of (x, y, theta) for each robot
            robot_radius_m: Robot radius in meters
            
        Algorithm:
            1. Reset grid to inflated static obstacles
            2. For each robot, mark cells in radius as occupied
        """
        # Reset to inflated static (pas static_grid brut)
        self.grid = self.inflated_grid.copy() if hasattr(self, 'inflated_grid') else self.static_grid.copy()
        
        # Add robot footprints
        radius_cells = int(np.ceil(robot_radius_m / self.resolution))
        
        for x, y, _ in robot_poses:
            center_row, center_col = self.world_to_grid(x, y)
            
            # Mark circle of cells
            for dr in range(-radius_cells, radius_cells + 1):
                for dc in range(-radius_cells, radius_cells + 1):
                    if dr**2 + dc**2 <= radius_cells**2:
                        r, c = center_row + dr, center_col + dc
                        if self._is_valid_cell(r, c):
                            self.grid[r, c] = 1.0
    
    def _is_valid_cell(self, row: int, col: int) -> bool:
        """Check if cell is within grid bounds."""
        return 0 <= row < self.n_rows and 0 <= col < self.n_cols
    
    def get_costmap(self):
        """
        Return current costmap for planning.
        
        Returns:
            numpy array (n_rows x n_cols) with costs 0-100
        """
        return (self.grid * 100).astype(np.uint8)




################################################################################
PATH: ./core/world/world_model.py
################################################################################
"""
World Model - Unified World Representation

Central repository for all world state:
- Robot poses (filtered by Kalman)
- Occupancy grid (obstacles)
- Arena boundaries
- Coordinate frames

This is the single source of truth for spatial information.
All other modules query the world model.

Does NOT contain game logic (scores, etc.) - only spatial state.
"""

from typing import Dict, List, Tuple
from .occupancy_grid import OccupancyGrid
from .coordinate_frames import TransformManager


class WorldModel:
    """
    Complete spatial world representation.
    
    Manages:
    - Robot state (positions, velocities, orientations)
    - Obstacles (static + dynamic)
    - Coordinate transformations
    - Arena boundaries
    """
    
    def __init__(self, arena_width_m: float, arena_height_m: float, 
                 grid_resolution_m: float = 0.02,
                 robot_radius_m: float = 0.09,
                 inflation_margin_m: float = 0.05):
        """
        Initialize world model.
        
        Args:
            arena_width_m: Arena width from calibration
            arena_height_m: Arena height from calibration
            grid_resolution_m: Grid cell size
            robot_radius_m: Physical robot radius (from config)
            inflation_margin_m: Safety margin for pathfinding (from config)
        """
        self.arena_width = arena_width_m
        self.arena_height = arena_height_m
        self.robot_radius_m = robot_radius_m
        self.inflation_margin_m = inflation_margin_m
        
        # Occupancy grid
        self.grid = OccupancyGrid(arena_width_m, arena_height_m, grid_resolution_m)
        
        # Robot state
        self.robots = {
            4: {  # AI robot
                'pose': (0.0, 0.0, 0.0),  # (x, y, theta)
                'velocity': (0.0, 0.0, 0.0),  # (vx, vy, omega)
                'radius_m': robot_radius_m,
            },
            5: {  # Human robot
                'pose': (0.0, 0.0, 0.0),
                'velocity': (0.0, 0.0, 0.0),
                'radius_m': robot_radius_m,
            }
        }
        
        # Coordinate transforms
        self.transforms = TransformManager()
        
    def update_robot_pose(self, robot_id: int, pose: Tuple[float, float, float]):
        """
        Update robot pose from Kalman filter.
        
        Args:
            robot_id: 4 or 5
            pose: (x, y, theta) in meters/radians
        """
        if robot_id in self.robots:
            self.robots[robot_id]['pose'] = pose
    
    def update_robot_velocity(self, robot_id: int, 
                             velocity: Tuple[float, float, float]):
        """
        Update robot velocity from Kalman filter.
        
        Args:
            robot_id: 4 or 5
            velocity: (vx, vy, omega) in m/s and rad/s
        """
        if robot_id in self.robots:
            self.robots[robot_id]['velocity'] = velocity
    
    def update_occupancy(self):
        """
        Update occupancy grid with current robot positions.
        
        Called each frame after robot poses are updated.
        """
        robot_poses = [self.robots[rid]['pose'] for rid in [4, 5]]
        self.grid.update_dynamic_obstacles(robot_poses, self.robot_radius_m)
    
    def generate_costmap(self):
        """
        Génère la costmap gonflée pour le pathfinding A*.
        
        Appelle après avoir chargé les obstacles statiques.
        Utilise les paramètres robot du config.
        """
        self.grid.inflate_static_obstacles(self.robot_radius_m, self.inflation_margin_m)
        print(f"[WORLD] Costmap générée avec rayon={self.robot_radius_m}m, marge={self.inflation_margin_m}m")
    
    def get_robot_pose(self, robot_id: int) -> Tuple[float, float, float]:
        """Get current robot pose."""
        return self.robots[robot_id]['pose']
    
    def get_robot_velocity(self, robot_id: int) -> Tuple[float, float, float]:
        """Get current robot velocity."""
        return self.robots[robot_id]['velocity']
    
    def is_position_valid(self, x: float, y: float) -> bool:
        """
        Check if a position is within arena and not occupied.
        
        Args:
            x, y: Position in meters
            
        Returns:
            True if position is valid (in bounds and free)
        """
        # Check bounds
        if not (0 <= x <= self.arena_width and 0 <= y <= self.arena_height):
            return False
        
        # Check occupancy
        return not self.grid.is_occupied(x, y)
    
    def get_state_dict(self) -> Dict:
        """
        Export complete world state as dictionary.
        
        Used by AI, game engine, visualization.
        
        Returns:
            dict with all world information
        """
        return {
            'arena_size': (self.arena_width, self.arena_height),
            'robot_4_pose': self.robots[4]['pose'],
            'robot_5_pose': self.robots[5]['pose'],
            'robot_4_velocity': self.robots[4]['velocity'],
            'robot_5_velocity': self.robots[5]['velocity'],
            'occupancy_grid': self.grid,
        }



################################################################################
PATH: ./__main__.py
################################################################################
"""
Tank Project - Module Package
"""

__version__ = '1.0.0'
__author__ = 'Julien'

# Point d'entrée module
if __name__ == '__main__':
    from main import main
    main()



################################################################################
PATH: ./main.py
################################################################################
#!/usr/bin/env python3
"""
Tank Arena - Point d'Entrée Principal

Point d'entrée unique pour le projet tank arena.

Usage:
    python3 main.py [mode]
    
    Modes:
    - game        : Lancer le jeu (défaut)
    - calibration : Lancer l'assistant de calibration
    - export      : Exporter données debug
    
Exemples:
    python3 main.py
    python3 main.py game
    python3 main.py calibration
    python3 main.py export
    
Ou en tant que module:
    python3 -m tank_project.main
    cd /home/julien/ros2_ws/src && python3 -m tank_project.main
"""

import sys
import argparse
from pathlib import Path

# Ajouter le répertoire courant au path
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))


def main():
    """Point d'entrée principal."""
    parser = argparse.ArgumentParser(
        description='Tank Arena - Système de combat de chars robotiques',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  %(prog)s                  Lance le jeu
  %(prog)s game             Lance le jeu
  %(prog)s calibration      Lance la calibration
  %(prog)s export           Exporte les données debug
        """
    )
    
    parser.add_argument(
        'mode',
        nargs='?',
        default='game',
        choices=['game', 'calibration', 'export'],
        help='Mode de lancement (défaut: game)'
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("      TANK ARENA - Système Combat Chars Robotiques")
    print("=" * 60)
    print()
    
    # Lancer le mode approprié
    if args.mode == 'calibration':
        print("[MAIN] Mode: CALIBRATION")
        print("-" * 60)
        from scripts.run_calibration import main as run_calibration
        run_calibration()
        
    elif args.mode == 'export':
        print("[MAIN] Mode: EXPORT DEBUG DATA")
        print("-" * 60)
        from scripts.export_debug_data import main as run_export
        run_export()
        
    else:  # game
        print("[MAIN] Mode: GAME")
        print("-" * 60)
        from scripts.run_game import main as run_game
        run_game()


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n[MAIN] Interruption utilisateur (Ctrl+C)")
        sys.exit(0)
    except Exception as e:
        print("\n[MAIN] ERREUR: {}".format(e))
        import traceback
        traceback.print_exc()
        sys.exit(1)



################################################################################
PATH: ./perception/calibration/arena_solver.py
################################################################################
"""
Arena Solver - Calcul Dimensions Arène

Déduit les dimensions physiques de l'arène (Lx, Ly)
à partir de la calibration.

Logs: [ARENA_SOLVER] prefix
"""

import numpy as np
from typing import Tuple


class ArenaSolver:
    """
    Calcule dimensions arène depuis calibration.
    """
    
    def __init__(self):
        """Initialize arena solver."""
        self.width_m = None
        self.height_m = None
        
    def solve_from_av_and_scale(self,
                                av_width: float,
                                av_height: float,
                                scale: float) -> Tuple[float, float]:
        """
        Calcule dimensions arène depuis taille AV et échelle.
        
        Args:
            av_width: Largeur en unités AV (typiquement 1.0)
            av_height: Hauteur en unités AV (typiquement 1.0)
            scale: Échelle m/unité_av
            
        Returns:
            (width_m, height_m) dimensions en mètres
            
        Logs:
            [ARENA_SOLVER] Arena dimensions: Lx x Ly meters
        """
        self.width_m = av_width * scale
        self.height_m = av_height * scale
        
        print("[ARENA_SOLVER] Arena dimensions: "
              "{:.2f}m x {:.2f}m".format(self.width_m, self.height_m))
        
        return (self.width_m, self.height_m)
    
    def solve_from_corners(self,
                          corners_world: np.ndarray) -> Tuple[float, float]:
        """
        Calcule dimensions depuis coins arène en coordonnées monde.
        
        Args:
            corners_world: 4 coins en mètres (ordre: BL, BR, TR, TL)
            
        Returns:
            (width_m, height_m)
        """
        # Distance entre coins bas
        width = np.linalg.norm(corners_world[1] - corners_world[0])
        
        # Distance entre coins gauche
        height = np.linalg.norm(corners_world[3] - corners_world[0])
        
        self.width_m = width
        self.height_m = height
        
        print("[ARENA_SOLVER] Arena dimensions from corners: "
              "{:.2f}m x {:.2f}m".format(width, height))
        
        return (width, height)
    
    def get_dimensions(self) -> Tuple[float, float]:
        """
        Retourne dimensions calculées.
        
        Returns:
            (width_m, height_m)
            
        Raises:
            ValueError: Si dimensions pas encore calculées
        """
        if self.width_m is None:
            raise ValueError("Dimensions pas encore calculées")
        
        return (self.width_m, self.height_m)
    
    def get_aspect_ratio(self) -> float:
        """Retourne ratio aspect width/height."""
        if self.width_m is None:
            raise ValueError("Dimensions pas encore calculées")
        
        return self.width_m / self.height_m



################################################################################
PATH: ./perception/calibration/calibration_wizard.py
################################################################################
"""
Calibration Wizard - Interactive Setup Process

Guides user through calibration sequence:
1. Safe zone definition (projection margins)
2. Geometric calibration (H_C2AV from projected corners)
3. Metric calibration (scale from physical ArUco)
4. Obstacle mapping (static obstacles detection)

Saves calibration to config/arena.yaml for game phase.

Logs: [CALIB] prefix for all steps
"""

import cv2
import numpy as np
import yaml
import time
import sys
from typing import Tuple, List
from ..camera.aruco_detector import ArucoDetector
from core.world.coordinate_frames import TransformManager
from .projector_display import ProjectorDisplay


class CalibrationWizard:
    """
    Interactive calibration process for arena setup.
    
    Produces H_C2W transform and arena parameters.
    """
    
    def __init__(self, camera, projector_width=1024, projector_height=768, 
                 margin_px=50, monitor_offset_x=1920, monitor_offset_y=0,
                 borderless=True, hide_cursor=True, marker_size_m=0.10):
        """
        Initialize calibration wizard.
        
        Args:
            camera: RealSenseStream instance
            projector_width: Projector resolution width
            projector_height: Projector resolution height
            margin_px: Safety margin from edges (pixels)
            monitor_offset_x: X position for secondary monitor
            monitor_offset_y: Y position for secondary monitor
            borderless: Use borderless window mode
            hide_cursor: Hide mouse cursor
            marker_size_m: Physical marker size in meters
        """
        self.camera = camera
        self.proj_w = projector_width
        self.proj_h = projector_height
        self.marker_size_m = marker_size_m
        
        self.aruco = ArucoDetector()
        self.transform_mgr = TransformManager()
        
        # Initialize projector display with all config
        self.projector = ProjectorDisplay(
            width=projector_width,
            height=projector_height,
            margin=margin_px,
            monitor_offset_x=monitor_offset_x,
            monitor_offset_y=monitor_offset_y,
            borderless=borderless,
            hide_cursor=hide_cursor
        )
        
        # Calibration results
        self.margin_px = margin_px
        self.arena_width_m = None
        self.arena_height_m = None
        self.H_C2W = None
        self.static_obstacles = []
    
    def _wait_for_user_validation(self, message: str = "Appuyez sur ESPACE pour continuer..."):
        """
        Wait for SPACE key in Pygame window while keeping it responsive.
        
        This prevents the "not responding" error by processing all input
        exclusively through Pygame instead of mixing console/Pygame.
        
        Args:
            message: Message to display to user
        """
        print(f"[CALIB] ATTENTE: {message}")
        
        # Show message on projector
        self.projector.show_message(message, color=(255, 255, 255), bg_color=(50, 50, 50))
        
        import pygame
        waiting = True
        while waiting:
            # Process all Pygame events
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    print("[CALIB] User closed window, exiting...")
                    sys.exit(0)
                elif event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_SPACE:
                        waiting = False
                        print("[CALIB] Validé !")
                    elif event.key == pygame.K_ESCAPE:
                        print("[CALIB] Calibration cancelled by user")
                        sys.exit(0)
                    elif event.key == pygame.K_q:
                        print("[CALIB] Calibration cancelled by user")
                        sys.exit(0)
            
            # Small sleep to prevent CPU spinning
            time.sleep(0.01)
        
    def run(self) -> dict:
        """
        Run complete calibration wizard.
        
        Returns:
            dict: Calibration results to save to config
            
        Steps:
            1. Define safe zone (margins)
            2. Detect projected corners → H_C2AV
            3. Detect physical marker → scale → H_C2W
            4. Map static obstacles
            5. Calculate arena dimensions
            
        Logs:
            [CALIB] Step X/4: Description
        """
        print("[CALIB] ========== Starting Calibration Wizard ==========")
        
        # Start projector display
        print("[CALIB] Starting projector display...")
        self.projector.start()
        
        try:
            # Step 1: Safe zone
            self._step_safe_zone()
            
            # Step 2: Geometric calibration
            H_C2AV = self._step_geometric_calibration()
            
            # Step 3: Metric calibration
            scale = self._step_metric_calibration(H_C2AV)
            
            # Step 4: Obstacle mapping
            obstacles = self._step_obstacle_mapping()
            
            # Build results
            results = {
                'projector': {
                    'width': self.proj_w,
                    'height': self.proj_h,
                    'margin': self.margin_px,
                    'margin_px': self.margin_px  # Alias for compatibility
                },
                'display': {
                    'fullscreen': False,
                    'display_index': 0
                },
                'arena': {
                    'width_m': self.arena_width_m,
                    'height_m': self.arena_height_m
                },
                'transform': {
                    'H_C2W': self.H_C2W.tolist() if self.H_C2W is not None else None,
                    'scale': scale,
                    'scale_m_per_av': scale  # Alias for compatibility
                },
                'grid': {
                    'resolution_m': 0.02,  # 2cm grid resolution
                    'inflation_radius_m': 0.15  # 15cm safety margin
                },
                'obstacles': obstacles
            }
            
            print("[CALIB] Calibration complete!")
            return results
            
        finally:
            # Always stop projector
            print("[CALIB] Stopping projector display...")
            self.projector.stop()
    
    def _step_safe_zone(self):
        """
        Step 1: Define safe zone for projection.
        
        Logs:
            [CALIB] MARGIN set to X px
            [CALIB] Arena rect in projector: (x1,y1) -> (x2,y2)
        """
        print("[CALIB] MARGIN set to {} px".format(self.margin_px))
        
        x1, y1 = self.margin_px, self.margin_px
        x2 = self.proj_w - self.margin_px
        y2 = self.proj_h - self.margin_px
        
        print("[CALIB] Arena rect in projector: ({},{}) -> ({},{})".format(x1, y1, x2, y2))
        
    def _step_geometric_calibration(self) -> np.ndarray:
        """
        Step 2: Detect projected corners and compute H_C2AV.
        
        Returns:
            H_C2AV: Homography matrix
            
        Logs:
            [CALIB] Detected 4 projected corners
            [CALIB] H_C2AV computed successfully
        """
        print("[CALIB] Step 2/4: Geometric calibration (detecting projected corners)")
        
        # Project ArUco markers at corners
        print("[CALIB] Projecting ArUco markers (IDs 0-3) at arena corners...")
        self.projector.show_corner_markers(marker_size_px=200)
        
        print("[CALIB] ArUco markers displayed on projector")
        print("[CALIB] Verify that camera can see all 4 markers, then press SPACE...")
        
        self._wait_for_user_validation("Vérifiez les 4 marqueurs et appuyez sur ESPACE")
        
        # Capture frame
        color, _ = self.camera.get_frames()
        
        # Detect ArUco
        detections = self.aruco.detect(color)
        
        # Check for corners (IDs 0-3)
        corner_ids = [0, 1, 2, 3]
        detected_corners = {k: v for k, v in detections.items() if k in corner_ids}
        
        if len(detected_corners) != 4:
            print("[CALIB] ERROR: Expected 4 corners, found {}".format(len(detected_corners)))
            print("[CALIB] Detected IDs: {}".format(list(detected_corners.keys())))
            raise ValueError("Missing projected corners")
        
        print("[CALIB] Detected 4 projected corners")
        
        # Build correspondences
        src_points = []
        dst_points = []
        
        # Arena virtual coordinates (unit square)
        av_coords = {
            0: (0.0, 0.0),  # Bottom-left
            1: (1.0, 0.0),  # Bottom-right
            2: (1.0, 1.0),  # Top-right
            3: (0.0, 1.0)   # Top-left
        }
        
        for marker_id in corner_ids:
            center = detected_corners[marker_id]['center']
            src_points.append(center)
            dst_points.append(av_coords[marker_id])
        
        src_points = np.array(src_points, dtype=np.float32)
        dst_points = np.array(dst_points, dtype=np.float32)
        
        # Compute homography
        H_C2AV, _ = cv2.findHomography(src_points, dst_points)
        
        print("[CALIB] H_C2AV computed successfully")
        
        return H_C2AV
    
    def _step_metric_calibration(self, H_C2AV: np.ndarray) -> float:
        """
        Step 3: Estimate metric scale from physical marker using homography.
        
        This method uses the homography to correctly account for camera perspective,
        ensuring accurate measurements regardless of where the marker is placed.
        
        Args:
            H_C2AV: Homography from step 2 (camera pixels -> arena virtual)
            
        Returns:
            scale: meters per AV unit
            
        Logs:
            [CALIB] Real marker size: X m
            [CALIB] Marker size in AV: Y units
            [CALIB] Scale: Z m / AV_unit
        """
        print("[CALIB] Step 3/4: Metric calibration (place physical marker in arena)")
        
        # Display instructions on projector
        self.projector.show_message("Placez le robot (ID 4 ou 5) au centre", 
                                    color=(255, 255, 255), bg_color=(50, 50, 50))
        
        # Use configured marker size
        marker_size_real = self.marker_size_m
        print(f"[CALIB] Using marker size: {marker_size_real} m (depuis config/projector.yaml)")
        
        self._wait_for_user_validation("Placez le marqueur robot et appuyez sur ESPACE")
        
        # Capture frame
        color, _ = self.camera.get_frames()
        
        # Detect markers
        detections = self.aruco.detect(color)
        
        # Find robot marker (ID 4 or 5)
        marker_data = None
        robot_marker_id = None
        for mid in [4, 5]:
            if mid in detections:
                marker_data = detections[mid]
                robot_marker_id = mid
                break
        
        if marker_data is None:
            raise ValueError("Aucun marqueur robot détecté (ID 4 ou 5) !")
        
        print(f"[CALIB] Marqueur robot ID {robot_marker_id} détecté")
        
        # ============================================================
        # CORRECT METHOD: Use homography to transform marker corners
        # This accounts for camera perspective distortion
        # ============================================================
        
        corners_pix = marker_data['corners']  # List of 4 corner tuples in pixels
        
        # Transform corners from camera pixels to Arena Virtual space using H_C2AV
        pts_src = np.array([corners_pix], dtype=np.float32)
        pts_av = cv2.perspectiveTransform(pts_src, H_C2AV)  # Output: AV coordinates
        
        corners_av = pts_av[0]  # Shape: (4, 2)
        
        # Calculate marker size in AV space (average of all 4 sides)
        side_lengths = []
        for i in range(4):
            j = (i + 1) % 4
            length = np.linalg.norm(corners_av[j] - corners_av[i])
            side_lengths.append(length)
        
        size_av = np.mean(side_lengths)
        
        print(f"[CALIB] Taille du marqueur dans l'espace virtuel: {size_av:.4f} unités")
        
        # Compute scale: meters per AV unit
        scale_m_per_av = marker_size_real / size_av
        
        print(f"[CALIB] ÉCHELLE VALIDÉE: 1.0 unité virtuelle = {scale_m_per_av:.4f} mètres")
        
        # ============================================================
        # Build H_C2W using corner markers (IDs 0-3)
        # ============================================================
        
        corner_ids = [0, 1, 2, 3]
        av_coords = {
            0: [0.0, 0.0],  # Bottom-left
            1: [1.0, 0.0],  # Bottom-right
            2: [1.0, 1.0],  # Top-right
            3: [0.0, 1.0]   # Top-left
        }
        
        src_centers = []
        dst_coords = []
        for mid in corner_ids:
            if mid in detections:
                src_centers.append(detections[mid]['center'])
                dst_coords.append(av_coords[mid])
        
        if len(src_centers) < 4:
            print(f"[CALIB] WARNING: Seulement {len(src_centers)}/4 marqueurs de coin détectés")
        
        self.transform_mgr.set_camera_to_av(
            np.array(src_centers, dtype=np.float32),
            np.array(dst_coords, dtype=np.float32)
        )
        self.transform_mgr.set_av_to_world_scale(scale_m_per_av)
        
        self.H_C2W = self.transform_mgr.H_C2W
        
        # ============================================================
        # Calculate arena dimensions using homography-corrected scale
        # In AV space, the arena is always 1.0 x 1.0 (unit square)
        # So real dimensions = scale * 1.0
        # ============================================================
        
        # The arena width in meters is simply the scale (since AV width = 1.0)
        self.arena_width_m = scale_m_per_av * 1.0
        
        # For height, we need to account for the projector/camera aspect ratio
        # The AV space maps 1.0 to both width and height, but the physical
        # arena may not be square (depends on projector aspect ratio)
        aspect_ratio = self.proj_w / self.proj_h  # e.g., 1024/768 = 1.33
        self.arena_height_m = self.arena_width_m / aspect_ratio
        
        print(f"[CALIB] Dimensions de l'arène: {self.arena_width_m:.2f}m x {self.arena_height_m:.2f}m")
        print(f"[CALIB] Ratio d'aspect: {aspect_ratio:.2f}")
        
        print("[CALIB] H_C2W calculé")
        print("[CALIB] Calibration métrique OK")
        
        return scale_m_per_av
    
    def _step_obstacle_mapping(self) -> List:
        """
        Step 4: Map static obstacles.
        
        Returns:
            List of obstacle regions
            
        Logs:
            [CALIB] Arena size estimated: XmxYm
            [CALIB] Static obstacles mapped
        """
        print("[CALIB] Step 4/4: Obstacle mapping")
        
        # Display white screen for obstacle contrast
        print("[CALIB] Displaying white screen for obstacle detection...")
        self.projector.show_white_screen()
        
        print("[CALIB] Place obstacles in arena, then press SPACE...")
        
        self._wait_for_user_validation("Placez les obstacles et appuyez sur ESPACE")
        
        # Simplified: return empty for now
        # Full implementation would do thresholding and contour detection
        
        print("[CALIB] Arena size estimated: {:.2f}m x {:.2f}m".format(self.arena_width_m, self.arena_height_m))
        print("[CALIB] Static obstacles mapped")
        
        return []



################################################################################
PATH: ./perception/calibration/__init__.py
################################################################################



################################################################################
PATH: ./perception/calibration/projector_display.py
################################################################################
"""
Projector Display for Calibration - FINAL VERSION

Displays ArUco markers on projector for calibration wizard.
Features:
- Automatic positioning on secondary screen (VGA/HDMI)
- Borderless mode to hide window decorations
- Hidden cursor for immersion

Logs: [PROJ_DISPLAY] prefix
"""

import os
import pygame
import cv2
import numpy as np
from typing import Tuple, Optional


class ProjectorDisplay:
    """
    Pygame window for projecting calibration patterns.
    Forces display on secondary monitor using SDL environment variables.
    """
    
    def __init__(self, width=1024, height=768, margin=50, monitor_offset_x=1920, monitor_offset_y=0,
                 borderless=True, hide_cursor=True):
        """
        Initialize projector display.
        
        Args:
            width: Projector resolution width (ex: 1024 for VGA)
            height: Projector resolution height (ex: 768 for VGA)
            margin: Safety margin from edges (px)
            monitor_offset_x: X position of the projector (usually width of main screen, e.g. 1920)
            monitor_offset_y: Y position (usually 0)
            borderless: Use borderless window mode (NOFRAME)
            hide_cursor: Hide mouse cursor
        """
        self.width = width
        self.height = height
        self.margin = margin
        
        # --- CONFIGURATION MULTI-ECRAN ---
        self.monitor_x = monitor_offset_x
        self.monitor_y = monitor_offset_y
        self.borderless = borderless
        self.hide_cursor = hide_cursor
        
        self.screen = None
        self.running = False
        
        # Arena rectangle (with margins)
        self.arena_x1 = margin
        self.arena_y1 = margin
        self.arena_x2 = width - margin
        self.arena_y2 = height - margin
        self.arena_w = self.arena_x2 - self.arena_x1
        self.arena_h = self.arena_y2 - self.arena_y1
        
        print("[PROJ_DISPLAY] Init: {}x{}, margin={}px".format(width, height, margin))
        print("[PROJ_DISPLAY] Target Monitor Offset: X={}, Y={}".format(self.monitor_x, self.monitor_y))
    
    def start(self):
        """Start Pygame and create window on the projector."""
        
        # 1. LE HACK: On force la position avant l'init de l'écran
        os.environ['SDL_VIDEO_WINDOW_POS'] = "%d,%d" % (self.monitor_x, self.monitor_y)
        
        pygame.init()
        
        # 2. Configure window flags based on settings
        flags = pygame.DOUBLEBUF
        if self.borderless:
            flags |= pygame.NOFRAME
        
        self.screen = pygame.display.set_mode((self.width, self.height), flags)
        
        # 3. IMMERSION - Hide cursor if configured
        if self.hide_cursor:
            pygame.mouse.set_visible(False)
        
        pygame.display.set_caption("Tank Arena - Projector View")
        self.running = True
        
        mode_str = "borderless" if self.borderless else "windowed"
        print(f"[PROJ_DISPLAY] Window opened at offset {self.monitor_x} ({mode_str})")
    
    def stop(self):
        """Close display."""
        if self.running:
            pygame.quit()
            self.running = False
            print("[PROJ_DISPLAY] Display closed")
    
    def clear(self, color=(0, 0, 0)):
        """Clear screen to solid color."""
        if self.screen:
            self.screen.fill(color)
    
    def get_events(self):
        """
        Return events to external controller (Wizard).
        Use this instead of handle_events when logic is controlled outside.
        """
        if not self.running:
            return []
        return pygame.event.get()

    def show_corner_markers(self, marker_size_px=200, aruco_dict=cv2.aruco.DICT_4X4_100):
        """
        Display ArUco markers at 4 corners of arena.
        """
        if not self.running:
            return
        
        # Clear to white background
        self.clear((255, 255, 255))
        
        # Generate markers
        aruco_dict_obj = cv2.aruco.getPredefinedDictionary(aruco_dict)
        
        # Corner positions (center of marker)
        corners = {
            0: (self.arena_x1 + marker_size_px // 2, self.arena_y2 - marker_size_px // 2),  # Bottom-left
            1: (self.arena_x2 - marker_size_px // 2, self.arena_y2 - marker_size_px // 2),  # Bottom-right
            2: (self.arena_x2 - marker_size_px // 2, self.arena_y1 + marker_size_px // 2),  # Top-right
            3: (self.arena_x1 + marker_size_px // 2, self.arena_y1 + marker_size_px // 2),  # Top-left
        }
        
        print("[PROJ_DISPLAY] Projecting 4 corner markers (IDs 0-3)")
        
        for marker_id, (cx, cy) in corners.items():
            # Generate marker image
            marker_img = cv2.aruco.generateImageMarker(aruco_dict_obj, marker_id, marker_size_px)
            
            # Convert to pygame surface
            marker_img_rgb = cv2.cvtColor(marker_img, cv2.COLOR_GRAY2RGB)
            marker_surface = pygame.surfarray.make_surface(
                np.transpose(marker_img_rgb, (1, 0, 2))
            )
            
            # Calculate top-left corner
            x = cx - marker_size_px // 2
            y = cy - marker_size_px // 2
            
            # Draw marker
            self.screen.blit(marker_surface, (x, y))
            
            # Add ID label below marker
            font = pygame.font.Font(None, 36)
            text = font.render(f"ID {marker_id}", True, (0, 0, 0))
            text_rect = text.get_rect(center=(cx, cy + marker_size_px // 2 + 30))
            self.screen.blit(text, text_rect)
        
        # Update display
        pygame.display.flip()
        
        print("[PROJ_DISPLAY] Corner markers displayed")
    
    def show_white_screen(self):
        """Display solid white screen (for obstacle detection)."""
        if not self.running:
            return
        
        self.clear((255, 255, 255))
        
        # Add text instruction
        font = pygame.font.Font(None, 48)
        text = font.render("Place obstacles in arena", True, (0, 0, 0))
        text_rect = text.get_rect(center=(self.width // 2, 100))
        self.screen.blit(text, text_rect)
        
        pygame.display.flip()
        print("[PROJ_DISPLAY] White screen displayed")
    
    def show_message(self, message: str, color=(255, 255, 255), bg_color=(0, 0, 0)):
        """
        Display a text message.
        """
        if not self.running:
            return
        
        self.clear(bg_color)
        
        font = pygame.font.Font(None, 72)
        text = font.render(message, True, color)
        text_rect = text.get_rect(center=(self.width // 2, self.height // 2))
        self.screen.blit(text, text_rect)
        
        pygame.display.flip()
    
    def _pump_events(self):
        """
        Internal: Keep window responsive without consuming events.
        Call this in long operations if not using get_events().
        """
        pygame.event.pump()



################################################################################
PATH: ./perception/calibration/projector_mapping.py
################################################################################
"""
Projector Mapping - Transformation Monde → Projecteur

Gère la conversion des coordonnées monde (mètres)
vers pixels projecteur pour affichage Pygame.

Logs: [PROJ_MAP] prefix
"""

import numpy as np
from typing import Tuple


class ProjectorMapping:
    """
    Mapping Monde → Pixels Projecteur.
    """
    
    def __init__(self,
                 projector_width: int = 1024,
                 projector_height: int = 768,
                 margin: int = 50):
        """
        Initialize projector mapping.
        
        Args:
            projector_width: Résolution projecteur largeur
            projector_height: Résolution projecteur hauteur
            margin: Marge sécurité (pixels)
        """
        self.proj_w = projector_width
        self.proj_h = projector_height
        self.margin = margin
        
        self.draw_w = projector_width - 2 * margin
        self.draw_h = projector_height - 2 * margin
        
        # Paramètres monde (à définir après calibration)
        self.arena_width_m = None
        self.arena_height_m = None
        self.scale = None
        
    def set_arena_dimensions(self, width_m: float, height_m: float):
        """
        Définit dimensions arène et calcule échelle d'affichage.
        
        Args:
            width_m: Largeur arène en mètres
            height_m: Hauteur arène en mètres
            
        Logs:
            [PROJ_MAP] Arena set: WxH m, scale: S px/m
        """
        self.arena_width_m = width_m
        self.arena_height_m = height_m
        
        # Calculer échelle (maintenir aspect ratio)
        scale_x = self.draw_w / width_m
        scale_y = self.draw_h / height_m
        self.scale = min(scale_x, scale_y)
        
        print("[PROJ_MAP] Arena set: {:.2f}x{:.2f}m, "
              "scale: {:.1f} px/m".format(width_m, height_m, self.scale))
        
    def world_to_projector(self, x_m: float, y_m: float) -> Tuple[int, int]:
        """
        Convertit coordonnées monde → pixels projecteur.
        
        Args:
            x_m, y_m: Position en mètres
            
        Returns:
            (px, py) position en pixels projecteur
        """
        if self.scale is None:
            raise ValueError("Appeler set_arena_dimensions d'abord")
        
        # Conversion avec flip Y (pygame origin top-left)
        px = self.margin + int(x_m * self.scale)
        py = self.margin + int((self.arena_height_m - y_m) * self.scale)
        
        return (px, py)
    
    def projector_to_world(self, px: int, py: int) -> Tuple[float, float]:
        """
        Convertit pixels projecteur → coordonnées monde.
        
        Args:
            px, py: Position en pixels
            
        Returns:
            (x_m, y_m) position en mètres
        """
        if self.scale is None:
            raise ValueError("Appeler set_arena_dimensions d'abord")
        
        x_m = (px - self.margin) / self.scale
        y_m = self.arena_height_m - (py - self.margin) / self.scale
        
        return (x_m, y_m)
    
    def scale_length(self, length_m: float) -> int:
        """
        Convertit longueur mètres → pixels.
        
        Args:
            length_m: Longueur en mètres
            
        Returns:
            Longueur en pixels
        """
        if self.scale is None:
            raise ValueError("Appeler set_arena_dimensions d'abord")
        
        return int(length_m * self.scale)
    
    def get_safe_zone_rect(self) -> Tuple[int, int, int, int]:
        """
        Retourne rectangle zone sécurité.
        
        Returns:
            (x, y, width, height) en pixels
        """
        return (self.margin, self.margin, self.draw_w, self.draw_h)



################################################################################
PATH: ./perception/calibration/scale_estimator.py
################################################################################
"""
Scale Estimator - Estimation Échelle Métrique

Estime le facteur d'échelle de AV → Monde en mètres
à partir d'un marqueur ArUco physique de taille connue.

Logs: [SCALE_EST] prefix
"""

import numpy as np
from typing import List, Tuple


class ScaleEstimator:
    """
    Estime l'échelle métrique depuis marqueur physique.
    """
    
    def __init__(self, marker_real_size_m: float = 0.10):
        """
        Initialize scale estimator.
        
        Args:
            marker_real_size_m: Taille réelle marqueur en mètres
        """
        self.marker_size_real = marker_real_size_m
        self.samples = []
        
    def estimate_from_corners(self,
                            corners_av: np.ndarray) -> float:
        """
        Estime échelle depuis coins marqueur en coordonnées AV.
        
        Args:
            corners_av: 4 coins en unités AV (4x2)
            
        Returns:
            Échelle en m/unité_av
        """
        # Calculer longueur moyenne des côtés en AV
        side_lengths = []
        for i in range(4):
            j = (i + 1) % 4
            length = np.linalg.norm(corners_av[j] - corners_av[i])
            side_lengths.append(length)
        
        avg_size_av = np.mean(side_lengths)
        
        # Échelle
        scale = self.marker_size_real / avg_size_av
        
        return scale
    
    def add_sample(self, corners_av: np.ndarray):
        """
        Ajoute une mesure d'échelle.
        
        Args:
            corners_av: Coins marqueur en AV
        """
        scale = self.estimate_from_corners(corners_av)
        self.samples.append(scale)
        print("[SCALE_EST] Sample {}: scale={:.4f} m/unit".format(len(self.samples), scale))
        
    def get_average_scale(self) -> float:
        """
        Retourne échelle moyenne de tous les échantillons.
        
        Returns:
            Échelle moyenne
            
        Logs:
            [SCALE_EST] Average scale from N samples: X m/unit (std=Y)
        """
        if not self.samples:
            raise ValueError("Aucun échantillon disponible")
        
        avg = np.mean(self.samples)
        std = np.std(self.samples)
        
        print("[SCALE_EST] Average scale from {} samples: "
              "{:.4f} m/unit (std={:.4f})".format(len(self.samples), avg, std))
        
        return avg
    
    def reset(self):
        """Réinitialise les échantillons."""
        self.samples = []



################################################################################
PATH: ./perception/camera/aruco_detector.py
################################################################################
"""
ArUco Detector - ArUco Marker Detection & Pose Estimation

Detects ArUco markers in camera images:
- Projected markers (ID 0-3): arena corners for calibration
- Robot markers (ID 4, 5): robot tracking

Provides:
- Marker center positions (pixels)
- Marker orientations (radians)
- Corner positions for scale estimation

Logs: [ARUCO] Detected N markers: [IDs]
"""

import cv2
import numpy as np
from typing import List, Dict, Tuple, Optional


class ArucoDetector:
    """
    ArUco marker detection and pose estimation.
    
    Uses cv2.aruco for marker detection.
    """
    
    def __init__(self, 
                 dictionary_type=cv2.aruco.DICT_4X4_50,
                 marker_size_m: float = 0.10):
        """
        Initialize ArUco detector.
        
        Args:
            dictionary_type: ArUco dictionary (default: 4x4, 50 markers)
            marker_size_m: Physical marker size in meters (for scale estimation)
        """
        self.dictionary = cv2.aruco.getPredefinedDictionary(dictionary_type)
        self.parameters = cv2.aruco.DetectorParameters()
        self.detector = cv2.aruco.ArucoDetector(self.dictionary, self.parameters)
        
        self.marker_size_m = marker_size_m
        
    def detect(self, image: np.ndarray) -> Dict[int, Dict]:
        """
        Detect ArUco markers in image.
        
        Args:
            image: Input image (BGR or grayscale)
            
        Returns:
            dict: {
                marker_id: {
                    'center': (u, v),  # pixel coordinates
                    'corners': [(u1,v1), (u2,v2), (u3,v3), (u4,v4)],
                    'orientation': theta  # radians
                }
            }
            
        Logs:
            [ARUCO] Detected N markers: [IDs]
        """
        # Convert to grayscale if needed
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image
        
        # Detect markers
        corners, ids, rejected = self.detector.detectMarkers(gray)
        
        results = {}
        
        if ids is not None:
            for i, marker_id in enumerate(ids.flatten()):
                marker_corners = corners[i][0]  # Shape: (4, 2)
                
                # Calculate center
                center = marker_corners.mean(axis=0)
                
                # Calculate orientation (from corner 0 to corner 1)
                # Corner order: top-left, top-right, bottom-right, bottom-left
                dx = marker_corners[1][0] - marker_corners[0][0]
                dy = marker_corners[1][1] - marker_corners[0][1]
                orientation = np.arctan2(dy, dx)
                
                results[marker_id] = {
                    'center': tuple(center),
                    'corners': [tuple(c) for c in marker_corners],
                    'orientation': orientation
                }
            
            print("[ARUCO] Detected {} markers: {}".format(len(ids), ids.flatten().tolist()))
        
        return results
    
    def estimate_marker_size_av(self, marker_corners, H_C2AV):
        """
        Estimate marker size in Arena Virtual units.
        
        Used during calibration to compute metric scale.
        
        Args:
            marker_corners: List of 4 corner positions in pixels
            H_C2AV: Homography camera → arena virtual
            
        Returns:
            float: Marker side length in AV units
        """
        # Transform corners to AV space
        corners_av = []
        for u, v in marker_corners:
            p_cam = np.array([u, v, 1.0])
            p_av = H_C2AV @ p_cam
            p_av = p_av[:2] / p_av[2]  # Normalize
            corners_av.append(p_av)
        
        # Calculate average side length
        corners_av = np.array(corners_av)
        side_lengths = []
        for i in range(4):
            j = (i + 1) % 4
            length = np.linalg.norm(corners_av[j] - corners_av[i])
            side_lengths.append(length)
        
        avg_length = np.mean(side_lengths)
        
        return avg_length
    
    def draw_detections(self, image: np.ndarray, detections: Dict) -> np.ndarray:
        """
        Draw detected markers on image (for debugging).
        
        Args:
            image: Input image
            detections: Detection results from detect()
            
        Returns:
            Image with drawn markers
        """
        img_draw = image.copy()
        
        for marker_id, data in detections.items():
            center = data['center']
            corners = data['corners']
            
            # Draw corners
            corners_array = np.array(corners, dtype=np.int32)
            cv2.polylines(img_draw, [corners_array], True, (0, 255, 0), 2)
            
            # Draw center
            cv2.circle(img_draw, (int(center[0]), int(center[1])), 5, (0, 0, 255), -1)
            
            # Draw ID
            cv2.putText(img_draw, f"ID:{marker_id}", 
                       (int(center[0]), int(center[1]) - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)
        
        return img_draw



################################################################################
PATH: ./perception/camera/color_segmentation.py
################################################################################
"""
Color Segmentation - Détection Obstacles par Seuillage

Segmente les obstacles sur fond blanc par seuillage couleur:
- Détection zones sombres (obstacles)
- Masques binaires
- Filtrage bruit

Utilisé pendant la calibration pour cartographier obstacles statiques.

Logs: [SEGMENT] prefix
"""

import cv2
import numpy as np
from typing import Tuple


def threshold_obstacles(image: np.ndarray, 
                       threshold_value: int = 200) -> np.ndarray:
    """
    Seuillage simple pour détecter obstacles sur fond blanc.
    
    Args:
        image: Image BGR ou grayscale
        threshold_value: Seuil (pixels < threshold = obstacles)
        
    Returns:
        Masque binaire (0 = libre, 255 = obstacle)
    """
    # Convertir en niveaux de gris si nécessaire
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image
    
    # Seuillage inverse (obstacles sont sombres)
    _, binary = cv2.threshold(gray, threshold_value, 255, cv2.THRESH_BINARY_INV)
    
    return binary


def adaptive_threshold_obstacles(image: np.ndarray) -> np.ndarray:
    """
    Seuillage adaptatif pour conditions éclairage variables.
    
    Args:
        image: Image BGR ou grayscale
        
    Returns:
        Masque binaire
    """
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image
    
    # Seuillage adaptatif
    binary = cv2.adaptiveThreshold(
        gray, 255, 
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY_INV,
        blockSize=11,
        C=2
    )
    
    return binary


def remove_noise(binary_mask: np.ndarray, 
                kernel_size: int = 5) -> np.ndarray:
    """
    Retire le bruit du masque binaire.
    
    Args:
        binary_mask: Masque binaire
        kernel_size: Taille kernel morphologie
        
    Returns:
        Masque filtré
    """
    kernel = np.ones((kernel_size, kernel_size), np.uint8)
    
    # Opening (erosion puis dilatation) pour retirer petits points
    opened = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel)
    
    # Closing (dilatation puis erosion) pour remplir trous
    closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel)
    
    return closed


def segment_obstacles(image: np.ndarray, 
                     method: str = 'simple',
                     denoise: bool = True) -> np.ndarray:
    """
    Pipeline complet de segmentation obstacles.
    
    Args:
        image: Image source
        method: 'simple' ou 'adaptive'
        denoise: Appliquer filtrage bruit
        
    Returns:
        Masque binaire nettoyé
        
    Logs:
        [SEGMENT] Obstacles detected: N pixels
    """
    if method == 'adaptive':
        mask = adaptive_threshold_obstacles(image)
    else:
        mask = threshold_obstacles(image)
    
    if denoise:
        mask = remove_noise(mask)
    
    # Compter pixels obstacles
    obstacle_pixels = np.count_nonzero(mask)
    print("[SEGMENT] Obstacles detected: {} pixels".format(obstacle_pixels))
    
    return mask



################################################################################
PATH: ./perception/camera/homography.py
################################################################################
"""
Homography - Calculs Transformations Homographiques

Calcule et applique les homographies:
- H_C2AV: Caméra → Arène Virtuelle
- H_AV2W: Arène Virtuelle → Monde (scaling)
- H_C2W: Caméra → Monde (combinée)

Utilisé par calibration_wizard et coordinate_frames.

Logs: [HOMOGRAPHY] prefix
"""

import cv2
import numpy as np
from typing import List, Tuple


def compute_homography(src_points: np.ndarray,
                      dst_points: np.ndarray) -> np.ndarray:
    """
    Calcule homographie entre points source et destination.
    
    Args:
        src_points: Points source (Nx2) en pixels caméra
        dst_points: Points destination (Nx2) en coordonnées cible
        
    Returns:
        Matrice homographie 3x3
        
    Raises:
        ValueError: Si moins de 4 points
        
    Logs:
        [HOMOGRAPHY] Computed from N points
    """
    if len(src_points) < 4 or len(dst_points) < 4:
        raise ValueError("Au moins 4 points requis pour homographie")
    
    # Assurer type float32
    src = np.array(src_points, dtype=np.float32)
    dst = np.array(dst_points, dtype=np.float32)
    
    # Calculer homographie
    H, _ = cv2.findHomography(src, dst)
    
    print("[HOMOGRAPHY] Computed from {} points".format(len(src_points)))
    
    return H


def apply_homography(points: np.ndarray, H: np.ndarray) -> np.ndarray:
    """
    Applique homographie à des points.
    
    Args:
        points: Points à transformer (Nx2)
        H: Matrice homographie 3x3
        
    Returns:
        Points transformés (Nx2)
    """
    # Convertir en coordonnées homogènes
    ones = np.ones((points.shape[0], 1))
    points_h = np.hstack([points, ones])
    
    # Appliquer transformation
    transformed_h = (H @ points_h.T).T
    
    # Normaliser (diviser par coordonnée w)
    transformed = transformed_h[:, :2] / transformed_h[:, 2:3]
    
    return transformed


def apply_homography_single(point: Tuple[float, float], 
                           H: np.ndarray) -> Tuple[float, float]:
    """
    Applique homographie à un point unique.
    
    Args:
        point: (x, y) point source
        H: Matrice homographie
        
    Returns:
        (x', y') point transformé
    """
    # Coordonnées homogènes
    p_h = np.array([point[0], point[1], 1.0])
    
    # Transformation
    p_transformed = H @ p_h
    
    # Normalisation
    x = p_transformed[0] / p_transformed[2]
    y = p_transformed[1] / p_transformed[2]
    
    return (x, y)


def create_scaling_matrix(scale: float) -> np.ndarray:
    """
    Crée matrice de scaling homogène.
    
    Args:
        scale: Facteur d'échelle
        
    Returns:
        Matrice 3x3
    """
    S = np.array([
        [scale, 0, 0],
        [0, scale, 0],
        [0, 0, 1]
    ], dtype=np.float32)
    
    return S


def combine_homographies(H1: np.ndarray, H2: np.ndarray) -> np.ndarray:
    """
    Combine deux homographies: H_combined = H2 @ H1.
    
    Args:
        H1: Première transformation
        H2: Deuxième transformation
        
    Returns:
        Homographie combinée
    """
    return H2 @ H1


def estimate_scale_from_marker(marker_corners_px: List[Tuple[float, float]],
                               H_C2AV: np.ndarray,
                               real_size_m: float) -> float:
    """
    Estime échelle métrique depuis marqueur ArUco.
    
    Args:
        marker_corners_px: 4 coins marqueur en pixels caméra
        H_C2AV: Homographie Caméra → Arène Virtuelle
        real_size_m: Taille réelle marqueur en mètres
        
    Returns:
        Scale en mètres/unité AV
        
    Algorithm:
        1. Transformer coins en AV
        2. Calculer taille moyenne en AV
        3. scale = real_size_m / size_av
        
    Logs:
        [HOMOGRAPHY] Scale estimation: real=Xm, av=Y units → scale=Z m/unit
    """
    # Transformer coins en AV
    corners_av = apply_homography(np.array(marker_corners_px), H_C2AV)
    
    # Calculer longueurs des 4 côtés
    side_lengths = []
    for i in range(4):
        j = (i + 1) % 4
        length = np.linalg.norm(corners_av[j] - corners_av[i])
        side_lengths.append(length)
    
    # Moyenne
    avg_size_av = np.mean(side_lengths)
    
    # Échelle
    scale = real_size_m / avg_size_av
    
    print("[HOMOGRAPHY] Scale estimation: real={:.3f}m, "
          "av={:.3f} units -> scale={:.3f} m/unit".format(real_size_m, avg_size_av, scale))
    
    return scale



################################################################################
PATH: ./perception/camera/__init__.py
################################################################################



################################################################################
PATH: ./perception/camera/kalman_filter.py
################################################################################
"""
Kalman Filter - Robot Pose Tracking

Extended Kalman filter for robot state estimation:
- State: [x, y, vx, vy, theta, omega]
- Measurements: [x, y, theta] from ArUco
- Prediction: constant velocity model

Smooths noisy ArUco detections and estimates velocities.

Logs: [KALMAN] RobotX state: x=X, y=Y, theta=T, vx=VX, vy=VY
"""

import numpy as np
from typing import Tuple


class KalmanFilter:
    """
    Extended Kalman Filter for 2D robot pose and velocity estimation.
    
    State vector: [x, y, vx, vy, theta, omega]
    """
    
    def __init__(self, dt: float = 1/30.0):
        """
        Initialize Kalman filter.
        
        Args:
            dt: Time step (default 30 FPS = 0.033s)
        """
        self.dt = dt
        
        # State: [x, y, vx, vy, theta, omega]
        self.state = np.zeros(6)
        
        # State covariance
        self.P = np.eye(6) * 1.0
        
        # Process noise
        self.Q = np.diag([0.01, 0.01, 0.1, 0.1, 0.01, 0.1])
        
        # Measurement noise
        self.R = np.diag([0.05, 0.05, 0.1])  # [x, y, theta]
        
    def predict(self):
        """
        Prediction step: propagate state forward.
        
        State transition:
            x += vx * dt
            y += vy * dt
            vx (constant)
            vy (constant)
            theta += omega * dt
            omega (constant)
        """
        # State transition matrix
        F = np.array([
            [1, 0, self.dt, 0, 0, 0],
            [0, 1, 0, self.dt, 0, 0],
            [0, 0, 1, 0, 0, 0],
            [0, 0, 0, 1, 0, 0],
            [0, 0, 0, 0, 1, self.dt],
            [0, 0, 0, 0, 0, 1]
        ])
        
        # Predict state
        self.state = F @ self.state
        
        # Normalize theta
        self.state[4] = np.arctan2(np.sin(self.state[4]), np.cos(self.state[4]))
        
        # Predict covariance
        self.P = F @ self.P @ F.T + self.Q
        
    def update(self, measurement: Tuple[float, float, float]):
        """
        Update step: incorporate measurement.
        
        Args:
            measurement: (x, y, theta) from ArUco detection
        """
        # Measurement matrix (observe x, y, theta)
        H = np.array([
            [1, 0, 0, 0, 0, 0],
            [0, 1, 0, 0, 0, 0],
            [0, 0, 0, 0, 1, 0]
        ])
        
        z = np.array(measurement)
        
        # Innovation
        y = z - H @ self.state
        
        # Normalize angle innovation
        y[2] = np.arctan2(np.sin(y[2]), np.cos(y[2]))
        
        # Innovation covariance
        S = H @ self.P @ H.T + self.R
        
        # Kalman gain
        K = self.P @ H.T @ np.linalg.inv(S)
        
        # Update state
        self.state = self.state + K @ y
        
        # Normalize theta
        self.state[4] = np.arctan2(np.sin(self.state[4]), np.cos(self.state[4]))
        
        # Update covariance
        self.P = (np.eye(6) - K @ H) @ self.P
        
    def get_pose(self) -> Tuple[float, float, float]:
        """
        Get current pose estimate.
        
        Returns:
            (x, y, theta)
        """
        return (self.state[0], self.state[1], self.state[4])
    
    def get_velocity(self) -> Tuple[float, float, float]:
        """
        Get current velocity estimate.
        
        Returns:
            (vx, vy, omega)
        """
        return (self.state[2], self.state[3], self.state[5])
    
    def get_full_state(self) -> np.ndarray:
        """
        Get complete state vector.
        
        Returns:
            [x, y, vx, vy, theta, omega]
        """
        return self.state.copy()
    
    def reset(self, initial_pose: Tuple[float, float, float]):
        """
        Reset filter with new initial pose.
        
        Args:
            initial_pose: (x, y, theta)
        """
        self.state = np.array([
            initial_pose[0],  # x
            initial_pose[1],  # y
            0.0,              # vx
            0.0,              # vy
            initial_pose[2],  # theta
            0.0               # omega
        ])
        
        self.P = np.eye(6) * 1.0



################################################################################
PATH: ./perception/camera/realsense_stream.py
################################################################################
"""
RealSense Stream - Intel RealSense Camera Interface

Manages RealSense D435/D455 camera:
- Color stream acquisition
- Depth stream (optional)
- Camera configuration
- Frame rate management

Provides synchronized color and depth frames at 30 FPS.

Logs: [REALSENSE] prefix for camera operations
"""

import pyrealsense2 as rs
import numpy as np
from typing import Tuple, Optional


class RealSenseStream:
    """
    Interface for Intel RealSense camera.
    
    Handles camera initialization and frame acquisition.
    """
    
    def __init__(self, 
                 width: int = 640, 
                 height: int = 480, 
                 fps: int = 30,
                 enable_depth: bool = False):
        """
        Initialize RealSense camera.
        
        Args:
            width: Frame width
            height: Frame height
            fps: Frame rate
            enable_depth: Enable depth stream
            
        Logs:
            [REALSENSE] Camera initialized: WxH @ FPS fps
        """
        self.width = width
        self.height = height
        self.fps = fps
        self.enable_depth = enable_depth
        
        self.pipeline = None
        self.config = None
        
    def start(self):
        """
        Start camera pipeline.
        
        Logs:
            [REALSENSE] Pipeline started
            [REALSENSE] Failed to start: error
        """
        try:
            self.pipeline = rs.pipeline()
            self.config = rs.config()
            
            # Configure streams
            self.config.enable_stream(rs.stream.color, 
                                     self.width, self.height, 
                                     rs.format.bgr8, self.fps)
            
            if self.enable_depth:
                self.config.enable_stream(rs.stream.depth, 
                                         self.width, self.height, 
                                         rs.format.z16, self.fps)
            
            # Start pipeline
            self.pipeline.start(self.config)
            
            print("[REALSENSE] Pipeline started: {}x{} @ {} fps".format(self.width, self.height, self.fps))
            
        except Exception as e:
            print("[REALSENSE] Failed to start: {}".format(e))
            raise
    
    def stop(self):
        """Stop camera pipeline."""
        if self.pipeline:
            self.pipeline.stop()
            print("[REALSENSE] Pipeline stopped")
    
    def get_frames(self) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
        """
        Get latest color and depth frames.
        
        Returns:
            (color_frame, depth_frame): numpy arrays
            color_frame: HxWx3 BGR image
            depth_frame: HxW depth map (mm) or None
        """
        if not self.pipeline:
            return (None, None)
        
        try:
            # Wait for frames
            frames = self.pipeline.wait_for_frames()
            
            # Get color frame
            color_frame = frames.get_color_frame()
            color_image = np.asanyarray(color_frame.get_data()) if color_frame else None
            
            # Get depth frame (if enabled)
            depth_image = None
            if self.enable_depth:
                depth_frame = frames.get_depth_frame()
                depth_image = np.asanyarray(depth_frame.get_data()) if depth_frame else None
            
            return (color_image, depth_image)
            
        except Exception as e:
            print("[REALSENSE] Frame acquisition error: {}".format(e))
            return (None, None)
    
    def get_intrinsics(self):
        """
        Get camera intrinsic parameters.
        
        Returns:
            rs.intrinsics object with fx, fy, cx, cy
        """
        if self.pipeline:
            profile = self.pipeline.get_active_profile()
            color_stream = profile.get_stream(rs.stream.color)
            intrinsics = color_stream.as_video_stream_profile().get_intrinsics()
            return intrinsics
        return None



################################################################################
PATH: ./perception/__init__.py
################################################################################



################################################################################
PATH: ./perception/preprocessing/contours.py
################################################################################
"""
Contours - Extraction Contours

Extraction et traitement des contours d'image.

Logs: [CONTOURS] prefix
"""

import cv2
import numpy as np
from typing import List, Tuple


def find_contours(binary_image: np.ndarray) -> List[np.ndarray]:
    """
    Trouve contours dans image binaire.
    
    Args:
        binary_image: Image binaire
        
    Returns:
        Liste de contours
    """
    contours, _ = cv2.findContours(
        binary_image,
        cv2.RETR_EXTERNAL,
        cv2.CHAIN_APPROX_SIMPLE
    )
    
    print("[CONTOURS] Found {} contours".format(len(contours)))
    
    return contours


def filter_contours_by_area(contours: List[np.ndarray],
                           min_area: float = 100.0,
                           max_area: float = np.inf) -> List[np.ndarray]:
    """
    Filtre contours par aire.
    
    Args:
        contours: Liste contours
        min_area: Aire minimum
        max_area: Aire maximum
        
    Returns:
        Contours filtrés
    """
    filtered = []
    for cnt in contours:
        area = cv2.contourArea(cnt)
        if min_area <= area <= max_area:
            filtered.append(cnt)
    
    print("[CONTOURS] Filtered {} -> {} contours".format(len(contours), len(filtered)))
    
    return filtered


def get_bounding_boxes(contours: List[np.ndarray]) -> List[Tuple[int, int, int, int]]:
    """
    Extrait rectangles englobants.
    
    Args:
        contours: Liste contours
        
    Returns:
        Liste (x, y, w, h) rectangles
    """
    boxes = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        boxes.append((x, y, w, h))
    
    return boxes


def approximate_polygons(contours: List[np.ndarray],
                        epsilon_factor: float = 0.02) -> List[np.ndarray]:
    """
    Approxime contours par polygones.
    
    Args:
        contours: Liste contours
        epsilon_factor: Facteur précision (% périmètre)
        
    Returns:
        Contours approximés
    """
    approximated = []
    for cnt in contours:
        epsilon = epsilon_factor * cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, epsilon, True)
        approximated.append(approx)
    
    return approximated



################################################################################
PATH: ./perception/preprocessing/image_utils.py
################################################################################
"""
Image Utils - Utilitaires Traitement Image

Fonctions utilitaires pour traitement d'images.

Logs: [IMG_UTILS] prefix
"""

import cv2
import numpy as np
from typing import Tuple


def convert_to_grayscale(image: np.ndarray) -> np.ndarray:
    """
    Convertit image en niveaux de gris.
    
    Args:
        image: Image BGR ou RGB
        
    Returns:
        Image grayscale
    """
    if len(image.shape) == 2:
        return image  # Déjà grayscale
    
    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)


def resize_image(image: np.ndarray,
                width: int,
                height: int,
                interpolation=cv2.INTER_LINEAR) -> np.ndarray:
    """
    Redimensionne image.
    
    Args:
        image: Image source
        width: Nouvelle largeur
        height: Nouvelle hauteur
        interpolation: Méthode interpolation
        
    Returns:
        Image redimensionnée
    """
    return cv2.resize(image, (width, height), interpolation=interpolation)


def gaussian_blur(image: np.ndarray, kernel_size: int = 5) -> np.ndarray:
    """
    Applique flou gaussien.
    
    Args:
        image: Image source
        kernel_size: Taille kernel (impair)
        
    Returns:
        Image floutée
    """
    return cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)


def morphological_open(image: np.ndarray, kernel_size: int = 5) -> np.ndarray:
    """
    Opening morphologique (erosion puis dilatation).
    
    Args:
        image: Image binaire
        kernel_size: Taille kernel
        
    Returns:
        Image traitée
    """
    kernel = np.ones((kernel_size, kernel_size), np.uint8)
    return cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)


def morphological_close(image: np.ndarray, kernel_size: int = 5) -> np.ndarray:
    """
    Closing morphologique (dilatation puis erosion).
    
    Args:
        image: Image binaire
        kernel_size: Taille kernel
        
    Returns:
        Image traitée
    """
    kernel = np.ones((kernel_size, kernel_size), np.uint8)
    return cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)


def warp_perspective(image: np.ndarray,
                     H: np.ndarray,
                     output_size: Tuple[int, int]) -> np.ndarray:
    """
    Applique transformation perspective (warp).
    
    Args:
        image: Image source
        H: Matrice homographie 3x3
        output_size: (width, height) sortie
        
    Returns:
        Image transformée
    """
    return cv2.warpPerspective(image, H, output_size)


def equalize_histogram(image: np.ndarray) -> np.ndarray:
    """
    Égalisation histogramme.
    
    Args:
        image: Image grayscale
        
    Returns:
        Image égalisée
    """
    return cv2.equalizeHist(image)



################################################################################
PATH: ./perception/preprocessing/__init__.py
################################################################################



################################################################################
PATH: ./perception/preprocessing/thresholding.py
################################################################################
"""
Thresholding - Seuillage Image

Fonctions de seuillage pour détection obstacles.

Logs: [THRESHOLD] prefix
"""

import cv2
import numpy as np


def simple_threshold(image: np.ndarray,
                    threshold: int = 200,
                    max_value: int = 255) -> np.ndarray:
    """
    Seuillage binaire simple.
    
    Args:
        image: Image grayscale
        threshold: Valeur seuil
        max_value: Valeur maximum
        
    Returns:
        Image binaire
    """
    _, binary = cv2.threshold(image, threshold, max_value, cv2.THRESH_BINARY)
    return binary


def inverse_threshold(image: np.ndarray,
                     threshold: int = 200) -> np.ndarray:
    """
    Seuillage inverse (pour obstacles sombres sur fond clair).
    
    Args:
        image: Image grayscale
        threshold: Valeur seuil
        
    Returns:
        Image binaire (obstacles = 255)
    """
    _, binary = cv2.threshold(image, threshold, 255, cv2.THRESH_BINARY_INV)
    return binary


def otsu_threshold(image: np.ndarray) -> np.ndarray:
    """
    Seuillage automatique Otsu.
    
    Args:
        image: Image grayscale
        
    Returns:
        Image binaire
    """
    _, binary = cv2.threshold(image, 0, 255, 
                              cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    return binary


def adaptive_threshold(image: np.ndarray,
                      block_size: int = 11,
                      C: int = 2) -> np.ndarray:
    """
    Seuillage adaptatif.
    
    Args:
        image: Image grayscale
        block_size: Taille bloc voisinage (impair)
        C: Constante soustraite de moyenne
        
    Returns:
        Image binaire
    """
    binary = cv2.adaptiveThreshold(
        image, 255,
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY,
        block_size, C
    )
    return binary



################################################################################
PATH: ./QUICKSTART.md
################################################################################
# Guide Rapide - Lancement du Projet

## Methodes de Lancement

### 1. Depuis le repertoire du projet

```bash
cd /home/julien/ros2_ws/src/VA-51/tank_project

# Afficher l'aide
python3 main.py --help

# Lancer le jeu
python3 main.py
python3 main.py game

# Lancer la calibration
python3 main.py calibration

# Exporter donnees debug
python3 main.py export
```

### 2. En tant que module Python

```bash
cd /home/julien/ros2_ws/src/VA-51

# Afficher l'aide
python3 -m tank_project --help

# Lancer le jeu
python3 -m tank_project
python3 -m tank_project game

# Lancer la calibration
python3 -m tank_project calibration

# Exporter donnees debug
python3 -m tank_project export
```

### 3. Via les scripts directs

```bash
cd /home/julien/ros2_ws/src/VA-51/tank_project

# Calibration
python3 scripts/run_calibration.py

# Jeu
python3 scripts/run_game.py

# Export debug
python3 scripts/export_debug_data.py --output-dir logs/debug_custom
```

## Prerequisites

### Installation des dependances

```bash
cd /home/julien/ros2_ws/src/VA-51/tank_project
pip3 install -r requirements.txt
```

Dependances principales:
- `numpy` - Calculs numeriques
- `opencv-python` - Vision par ordinateur
- `pyrealsense2` - Interface camera RealSense
- `pygame` - Rendu graphique
- `scipy` - Traitement signal
- `pyyaml` - Configuration

## Workflow Complet

### Premiere Utilisation

```bash
# 1. Installer dependances
pip3 install -r requirements.txt

# 2. Lancer calibration (obligatoire la premiere fois)
python3 scripts/run_calibration.py

# Suivre les instructions a l'ecran:
# - Appuyer sur ESPACE pour valider chaque etape
# - Detecter coins projetes (ArUco 0-3)
# - Mesurer marqueur physique (ArUco 4 ou 5)
# - Cartographier obstacles

# 3. Verifier calibration sauvegardee
cat config/arena.yaml

# 4. Valider visuellement l'homographie (RECOMMANDE)
python3 scripts/show_grid.py
# - Poser un marqueur dans un coin -> la cible cyan doit etre au bon endroit
# - Appuyer sur D pour voir l'inflation (zones oranges)
# - ESC pour quitter

# 5. Lancer le jeu
python3 scripts/run_game.py
```

### Utilisation Normale

```bash
# Lancer directement le jeu (calibration deja faite)
python3 main.py
```

## Debug et Diagnostic

### Export donnees debug

```bash
# Exporter snapshot complet
python3 main.py export

# Donnees exportees dans:
# logs/debug_YYYYMMDD_HHMMSS/
```

### Verifier imports

```bash
# Tester que tous les modules s'importent correctement
python3 -c "
from core.game import game_engine
from core.ia import strategy
from perception.camera import aruco_detector
from visualization import pygame_renderer
print('Tous les imports OK')
"
```

### Logs

Les logs sont stockes dans `logs/`:
- `runtime.log` - Logs d'execution jeu
- `calibration.log` - Logs calibration
- `debug.log` - Logs debug general

Filtrer logs par module:
```bash
# Logs IA uniquement
python3 main.py 2>&1 | grep "\[AI\]"

# Logs vision uniquement
python3 main.py 2>&1 | grep "\[VISION\]"

# Logs calibration
python3 main.py calibration 2>&1 | tee logs/calibration.log
```

## Configuration

Modifier les parametres dans `config/*.yaml`:

- `arena.yaml` - Dimensions arene, transformations, affichage
- `camera.yaml` - Parametres RealSense, ArUco
- `game.yaml` - Regles jeu (duree, cooldowns)
- `ia.yaml` - Comportement IA (distances, seuils)
- `robot.yaml` - Specs Turtlebot (vitesses, dimensions)

Exemple:
```bash
# Editer duree match (defaut: 180s)
nano config/game.yaml
```

## Depannage

### Erreur "No module named 'cv2'"
```bash
pip3 install opencv-python
```

### Erreur "No module named 'pyrealsense2'"
```bash
# Installer SDK RealSense
sudo apt-get install librealsense2-dev
pip3 install pyrealsense2
```

### Pygame ne demarre pas
```bash
pip3 install --upgrade pygame
```

### ROS bridge connection failed
Verifier que le pont ROS est actif sur les robots:
```bash
# Sur le robot
ros2 run ros_bridge server
```

## Plus d'Informations

Voir le [README.md](README.md) complet pour:
- Architecture detaillee
- Description modules
- Mecaniques de jeu
- Details techniques



################################################################################
PATH: ./README.md
################################################################################
# Tank Project

Moteur de jeu principal pour Tank Arena avec IA comportementale, vision par ordinateur, et contrôle robot.

## Structure

```
tank_project/
├── config/              # Configuration YAML
├── core/
│   ├── control/         # Cinématique, trajectoire, ROS bridge
│   ├── game/            # GameEngine, Raycast, Hits
│   ├── ia/              # Behavior Tree, A*, Decisions
│   └── world/           # WorldModel, OccupancyGrid
├── perception/
│   ├── calibration/     # Calibration arène/projecteur
│   └── camera/          # ArUco, Kalman, RealSense
├── visualization/       # Pygame renderer, HUD
└── scripts/
    └── run_game.py      # Point d'entrée
```

## Lancement

```bash
cd tank_project
python3 scripts/run_game.py
```

## Configuration

| Fichier       | Description                  |
| ------------- | ---------------------------- |
| `arena.yaml`  | Dimensions arène, projecteur |
| `camera.yaml` | RealSense, ArUco, Kalman     |
| `game.yaml`   | Règles du jeu, cooldowns     |
| `ia.yaml`     | Comportement IA              |
| `robot.yaml`  | Cinématique, port 8765       |

## Connexion au Bridge

Le `ROSBridgeClient` se connecte au Safety Bridge avec retry automatique :

```python
client = ROSBridgeClient(host='localhost', port=8765)
client.connect(max_retries=0, retry_interval=8.0)  # Retry infini
```

## Contrôles

| Touche    | Action                     |
| --------- | -------------------------- |
| `Espace`  | Démarrer match             |
| `Flèches` | Contrôler robot humain     |
| `F`       | Tirer                      |
| `D`       | Toggle debug paths / inflation |
| `ESC`     | Quitter                    |

## Scripts Utiles

```bash
# Calibration (obligatoire 1ère fois)
python3 scripts/run_calibration.py

# Validation visuelle homographie
python3 scripts/show_grid.py

# Lancer le jeu
python3 scripts/run_game.py
```



################################################################################
PATH: ./requirements.txt
################################################################################
# Dépendances Projet Tank Arena

# Vision & Traitement Image
numpy>=1.20.0
opencv-python>=4.5.0
pyrealsense2>=2.50.0

# Visualisation
pygame>=2.1.0

# Traitement Signal & Math
scipy>=1.7.0

# Configuration
pyyaml>=5.4.0

# Utilitaires
matplotlib>=3.3.0



################################################################################
PATH: ./scripts/export_debug_data.py
################################################################################
#!/usr/bin/env python3
"""
Export Debug Data - Sauvegarde Donnees Debug

Export snapshots debug pour analyse offline:
- Images camera (capture live ou test)
- Grilles occupation (NumPy + visualisation)
- Homographies et calibration
- Etat jeu (JSON)
- Logs

Modes:
    - Standalone : Export config + logs (sans systeme actif)
    - Live       : Capture snapshot depuis camera (avec --live)

Usage:
    python3 export_debug_data.py [--output-dir DIR] [--live]
    
Examples:
    # Export config + logs seulement
    python3 export_debug_data.py
    
    # Capture live depuis camera
    python3 export_debug_data.py --live
    
    # Export vers repertoire specifique
    python3 export_debug_data.py --output-dir ~/mon_debug --live
"""

import sys
import argparse
import json
import time
import cv2
import numpy as np
import yaml
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, Any

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))


def create_debug_export(output_dir: str = None, live_capture: bool = False):
    """
    Cree export debug complet.
    
    Args:
        output_dir: Repertoire sortie (defaut: logs/debug_TIMESTAMP/)
        live_capture: Si True, tente capture live depuis camera
    """
    # Creer repertoire output
    if output_dir is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path(__file__).parent.parent / 'logs' / ('debug_' + timestamp)
    else:
        output_dir = Path(output_dir)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("[EXPORT] ========== Debug Export ==========")
    print("[EXPORT] Output directory: {}".format(output_dir.absolute()))
    print("[EXPORT] Live capture: {}".format('ENABLED' if live_capture else 'DISABLED'))
    print()
    
    # Creer fichier manifest
    manifest = {
        'export_timestamp': datetime.now().isoformat(),
        'mode': 'live' if live_capture else 'standalone',
        'exported_items': []
    }
    
    # 1. Sauvegarder configuration
    config_success = _export_config(output_dir)
    if config_success:
        manifest['exported_items'].append('config')
    
    # 2. Sauvegarder frame camera + detections ArUco
    if live_capture:
        camera_success = _export_camera_frame(output_dir)
        if camera_success:
            manifest['exported_items'].append('camera_frame')
            manifest['exported_items'].append('aruco_detections')
    
    # 3. Sauvegarder grille occupation (exemple synthetique)
    grid_success = _export_occupancy_grid(output_dir, live_capture)
    if grid_success:
        manifest['exported_items'].append('occupancy_grid')
    
    # 4. Sauvegarder etat jeu (exemple synthetique)
    state_success = _export_game_state(output_dir, live_capture)
    if state_success:
        manifest['exported_items'].append('game_state')
    
    # 5. Copier logs recents
    logs_count = _export_logs(output_dir)
    if logs_count > 0:
        manifest['exported_items'].append('logs')
        manifest['logs_count'] = logs_count
    
    # 6. Sauvegarder manifest
    with open(output_dir / 'manifest.json', 'w') as f:
        json.dump(manifest, f, indent=2)
    
    print()
    print("[EXPORT] ========== Export Complete ==========")
    print("[EXPORT] Location: {}".format(output_dir.absolute()))
    print("[EXPORT] Items exported: {}".format(len(manifest['exported_items'])))
    print("[EXPORT] Manifest: manifest.json")


def _export_config(output_dir: Path) -> bool:
    """
    Export fichiers configuration.
    
    Returns:
        True si au moins un fichier exporte
    """
    print("[EXPORT] Exporting configuration...")
    
    config_dir = Path(__file__).parent.parent / 'config'
    
    if not config_dir.exists():
        print("[EXPORT]   [FAIL] config/ directory not found")
        return False
    
    config_export = output_dir / 'config'
    config_export.mkdir(exist_ok=True)
    
    exported = 0
    for config_file in config_dir.glob('*.yaml'):
        try:
            with open(config_file) as f:
                data = yaml.safe_load(f)
            
            out_file = config_export / config_file.name
            with open(out_file, 'w') as f:
                yaml.dump(data, f, default_flow_style=False, sort_keys=False)
            
            print("[EXPORT]   [OK] {}".format(config_file.name))
            exported += 1
        except Exception as e:
            print("[EXPORT]   [FAIL] {}: {}".format(config_file.name, e))
    
    return exported > 0


def _export_camera_frame(output_dir: Path) -> bool:
    """
    Export capture camera live + detections ArUco.
    
    Returns:
        True si capture reussie
    """
    print("[EXPORT] Capturing live camera frame...")
    
    try:
        from perception.camera.realsense_stream import RealSenseStream
        from perception.camera.aruco_detector import ArucoDetector
        
        # Initialiser camera
        print("[EXPORT]   Initializing RealSense camera...")
        camera = RealSenseStream(width=640, height=480, fps=30)
        camera.start()
        
        # Attendre stabilisation
        time.sleep(1.0)
        
        # Capturer frame
        color_frame, depth_frame = camera.get_frames()
        
        if color_frame is None:
            print("[EXPORT]   [FAIL] Failed to capture frame")
            camera.stop()
            return False
        
        # Sauvegarder frame couleur
        cv2.imwrite(str(output_dir / 'camera_frame.png'), color_frame)
        print("[EXPORT]   [OK] camera_frame.png ({}x{})".format(
            color_frame.shape[1], color_frame.shape[0]))
        
        # Sauvegarder depth si disponible
        if depth_frame is not None:
            np.save(str(output_dir / 'depth_frame.npy'), depth_frame)
            
            # Visualisation depth
            depth_colormap = cv2.applyColorMap(
                cv2.convertScaleAbs(depth_frame, alpha=0.03),
                cv2.COLORMAP_JET
            )
            cv2.imwrite(str(output_dir / 'depth_frame_viz.png'), depth_colormap)
            print("[EXPORT]   [OK] depth_frame.npy")
        
        # Detecter ArUco
        print("[EXPORT]   Detecting ArUco markers...")
        aruco = ArucoDetector()
        detections = aruco.detect(color_frame)
        
        # Dessiner detections sur frame
        frame_annotated = color_frame.copy()
        for marker_id, data in detections.items():
            center = data['center']
            corners = data['corners']
            
            # Dessiner contour
            cv2.polylines(frame_annotated, [corners.astype(int)], True, (0, 255, 0), 2)
            
            # Dessiner centre + ID
            cv2.circle(frame_annotated, tuple(center.astype(int)), 5, (0, 0, 255), -1)
            cv2.putText(
                frame_annotated,
                "ID:{}".format(marker_id),
                tuple((center + [10, -10]).astype(int)),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.6,
                (255, 255, 0),
                2
            )
        
        cv2.imwrite(str(output_dir / 'camera_frame_annotated.png'), frame_annotated)
        
        # Sauvegarder detections JSON
        detections_json = {}
        for marker_id, data in detections.items():
            detections_json[int(marker_id)] = {
                'center': data['center'].tolist(),
                'corners': data['corners'].tolist()
            }
        
        with open(output_dir / 'aruco_detections.json', 'w') as f:
            json.dump(detections_json, f, indent=2)
        
        print("[EXPORT]   [OK] ArUco detected: {} markers".format(len(detections)))
        print("[EXPORT]   [OK] camera_frame_annotated.png")
        print("[EXPORT]   [OK] aruco_detections.json")
        
        camera.stop()
        return True
        
    except ImportError as e:
        print("[EXPORT]   [FAIL] Import error: {}".format(e))
        print("[EXPORT]   -> Live capture requires RealSense dependencies")
        return False
    except Exception as e:
        print("[EXPORT]   [FAIL] Camera capture failed: {}".format(e))
        return False


def _export_occupancy_grid(output_dir: Path, live_capture: bool) -> bool:
    """
    Export grille occupation.
    
    Cree exemple synthetique si pas de systeme actif.
    
    Args:
        output_dir: Repertoire sortie
        live_capture: Si True, tente connexion a systeme actif
        
    Returns:
        True si export reussi
    """
    print("[EXPORT] Exporting occupancy grid...")
    
    # Parametres grille par defaut
    width_m = 2.85
    height_m = 1.90
    resolution_m = 0.02
    grid = None
    
    # Tenter connexion a WorldModel actif si en mode live
    if live_capture:
        try:
            from core.world.world_model import WorldModel
            
            # Charger config pour dimensions arena
            config_dir = Path(__file__).parent.parent / 'config'
            with open(config_dir / 'arena.yaml') as f:
                arena_cfg = yaml.safe_load(f)
            
            width_m = arena_cfg['arena']['width_m']
            height_m = arena_cfg['arena']['height_m']
            resolution_m = arena_cfg['grid']['resolution_m']
            
            # Creer WorldModel avec vraies dimensions
            world = WorldModel(width_m, height_m, resolution_m)
            grid = (world.grid.grid * 100).astype(np.uint8)
            
            print("[EXPORT]   Connected to WorldModel ({}x{}m @ {}m resolution)".format(
                width_m, height_m, resolution_m))
        except Exception as e:
            print("[EXPORT]   Could not connect to WorldModel: {}".format(e))
            print("[EXPORT]   Falling back to synthetic grid")
            grid = None
    
    nx = int(width_m / resolution_m)
    ny = int(height_m / resolution_m)
    
    # Creer grille synthetique si pas de WorldModel actif
    if grid is None:
        grid = np.zeros((ny, nx), dtype=np.uint8)
    
    # Ajouter quelques obstacles exemple
    # Obstacle central
    grid[40:60, 60:80] = 100
    
    # Murs bordure
    grid[0:5, :] = 100  # Bas
    grid[-5:, :] = 100  # Haut
    grid[:, 0:5] = 100  # Gauche
    grid[:, -5:] = 100  # Droite
    
    # Sauvegarder array NumPy
    np.save(str(output_dir / 'occupancy_grid.npy'), grid)
    print("[EXPORT]   [OK] occupancy_grid.npy ({}x{}, {}m/cell)".format(
        nx, ny, resolution_m))
    
    # Creer visualisation
    grid_viz = np.zeros((ny, nx, 3), dtype=np.uint8)
    grid_viz[grid == 0] = [240, 240, 240]    # Libre = gris clair
    grid_viz[grid == 100] = [50, 50, 50]     # Obstacle = gris fonce
    
    # Agrandir pour visualisation
    scale = 4
    grid_viz_large = cv2.resize(
        grid_viz,
        (nx * scale, ny * scale),
        interpolation=cv2.INTER_NEAREST
    )
    
    cv2.imwrite(str(output_dir / 'occupancy_grid.png'), grid_viz_large)
    print("[EXPORT]   [OK] occupancy_grid.png (visualization)")
    
    # Sauvegarder metadonnees
    grid_meta = {
        'width_m': width_m,
        'height_m': height_m,
        'resolution_m': resolution_m,
        'grid_size': [nx, ny],
        'note': 'Synthetic example grid' if not live_capture else 'Live captured grid'
    }
    
    with open(output_dir / 'occupancy_grid_meta.json', 'w') as f:
        json.dump(grid_meta, f, indent=2)
    
    print("[EXPORT]   [OK] occupancy_grid_meta.json")
    
    return True


def _export_game_state(output_dir: Path, live_capture: bool) -> bool:
    """
    Export etat jeu.
    
    Cree exemple synthetique si pas de jeu actif.
    
    Args:
        output_dir: Repertoire sortie
        live_capture: Si True, tente connexion a jeu actif
        
    Returns:
        True si export reussi
    """
    print("[EXPORT] Exporting game state...")
    
    state_dict = None
    
    # Tenter connexion a GameEngine actif si en mode live
    if live_capture:
        try:
            from core.game.game_engine import GameEngine
            from core.game.state import GameStatus
            
            # Charger config jeu
            config_dir = Path(__file__).parent.parent / 'config'
            with open(config_dir / 'game.yaml') as f:
                game_cfg = yaml.safe_load(f)
            
            # Creer GameEngine pour avoir acces aux regles
            engine = GameEngine(game_cfg)
            
            # Exporter etat actuel (note: sans WorldModel actif, c'est un etat initial)
            state_dict = {
                'timestamp': datetime.now().isoformat(),
                'mode': 'live',
                'engine_status': engine.state.value if engine.state else 'unknown',
                'rules': {
                    'match_duration_s': engine.rules.match_duration_seconds,
                    'max_hits_to_win': engine.rules.max_hits_to_win,
                    'ai_cooldown_s': engine.rules.ai_shot_cooldown,
                    'human_cooldown_s': engine.rules.human_shot_cooldown
                },
                'note': 'GameEngine connected - rules exported'
            }
            
            print("[EXPORT]   Connected to GameEngine (status: {})".format(engine.state.value))
        except Exception as e:
            print("[EXPORT]   Could not connect to GameEngine: {}".format(e))
            print("[EXPORT]   Falling back to synthetic state")
            state_dict = None
    
    # Creer etat synthetique si pas de GameEngine actif
    if state_dict is None:
        state_dict = {
            'timestamp': datetime.now().isoformat(),
            'mode': 'live' if live_capture else 'synthetic_example',
            'match': {
                'duration_s': 180,
                'elapsed_s': 67.3,
                'time_remaining_s': 112.7,
                'status': 'in_progress'
            },
            'robots': {
                'robot_4': {
                    'name': 'AI Robot',
                    'pose': {
                        'x_m': 1.23,
                        'y_m': 0.87,
                        'theta_rad': 1.57
                    },
                    'velocity': {
                        'vx_m_s': 0.03,
                        'vy_m_s': -0.01,
                        'omega_rad_s': 0.02
                    },
                    'hits_received': 2,
                    'hits_inflicted': 3,
                    'last_shot_time': 61.2
                },
                'robot_5': {
                    'name': 'Human Robot',
                    'pose': {
                        'x_m': 1.85,
                        'y_m': 1.42,
                        'theta_rad': -0.52
                    },
                    'velocity': {
                        'vx_m_s': -0.08,
                        'vy_m_s': 0.05,
                        'omega_rad_s': -0.10
                    },
                    'hits_received': 3,
                    'hits_inflicted': 2,
                    'last_shot_time': 58.7
                }
            },
            'ai_state': {
                'current_behavior': 'ATTACK',
                'target_position': [1.85, 1.42],
                'path_waypoints_count': 12,
                'has_line_of_sight': True,
                'fire_request': True,
                'safe_distance_m': 0.8
            },
            'cooldowns': {
                'human_next_shot': 72.7,
                'ai_next_shot': 64.2
            }
        }
    
    with open(output_dir / 'game_state.json', 'w') as f:
        json.dump(state_dict, f, indent=2)
    
    print("[EXPORT]   [OK] game_state.json")
    
    # Afficher details selon le mode
    if 'match' in state_dict:
        print("[EXPORT]     - Time: {:.1f}s / {}s".format(
            state_dict['match']['elapsed_s'], state_dict['match']['duration_s']))
        print("[EXPORT]     - AI state: {}".format(state_dict['ai_state']['current_behavior']))
        print("[EXPORT]     - Hits: R4={}, R5={}".format(
            state_dict['robots']['robot_4']['hits_received'],
            state_dict['robots']['robot_5']['hits_received']))
    else:
        print("[EXPORT]     - Rules exported from GameEngine")
    
    return True


def _export_logs(output_dir: Path) -> int:
    """
    Copie logs recents.
    
    Returns:
        Nombre de fichiers copies
    """
    print("[EXPORT] Copying logs...")
    
    logs_dir = Path(__file__).parent.parent / 'logs'
    
    if not logs_dir.exists():
        logs_dir.mkdir(exist_ok=True)
        print("[EXPORT]   [FAIL] No logs found (logs/ directory created)")
        return 0
    
    # Copier fichiers .log
    copied = 0
    for log_file in logs_dir.glob('*.log'):
        try:
            out_file = output_dir / log_file.name
            with open(log_file) as f:
                content = f.read()
            with open(out_file, 'w') as f:
                f.write(content)
            
            # Compter lignes
            lines = len(content.splitlines())
            size_kb = len(content) / 1024
            
            print("[EXPORT]   [OK] {} ({} lines, {:.1f}KB)".format(
                log_file.name, lines, size_kb))
            copied += 1
        except Exception as e:
            print("[EXPORT]   [FAIL] {}: {}".format(log_file.name, e))
    
    if copied == 0:
        print("[EXPORT]   [FAIL] No .log files found")
    
    return copied


def main():
    """Point d'entree script."""
    parser = argparse.ArgumentParser(
        description='Export donnees debug pour analyse offline',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s                        Export config + logs
  %(prog)s --live                 Capture live depuis camera
  %(prog)s --output-dir ~/debug   Export vers repertoire specifique
        """
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default=None,
        help='Repertoire sortie (defaut: logs/debug_TIMESTAMP/)'
    )
    parser.add_argument(
        '--live',
        action='store_true',
        help='Activer capture live depuis camera'
    )
    
    args = parser.parse_args()
    
    create_debug_export(args.output_dir, args.live)


if __name__ == '__main__':
    main()



################################################################################
PATH: ./scripts/install_deps.sh
################################################################################
#!/bin/bash
# Script pour installer les dépendances dans l'environnement pyenv ubuntu

echo "Installation des dépendances Python pour tank_project..."
echo "Environnement: pyenv ubuntu"
echo ""

# S'assurer d'être dans le bon répertoire
cd /home/julien/ros2_ws/src/VA50/tank_project

# Installer les dépendances
pip3 install -r requirements.txt

echo ""
echo "Installation terminée!"
echo "Vous pouvez maintenant lancer la calibration avec:"
echo "  python -m tank_project.main calibration"



################################################################################
PATH: ./scripts/run_calibration.py
################################################################################
#!/usr/bin/env python3
"""
Calibration Wizard Runner

Interactive calibration process to set up the arena:
1. Safe zone definition
2. Geometric calibration (projected corners)
3. Metric calibration (physical marker)
4. Obstacle mapping

Saves results to config/arena.yaml

Usage:
    python3 run_calibration.py
    
Prerequisites:
    - RealSense camera connected
    - Projector showing Pygame window with projected ArUco corners
"""

import sys
import yaml
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from perception.camera.realsense_stream import RealSenseStream
from perception.calibration.calibration_wizard import CalibrationWizard


def main():
    print("[CALIB_RUNNER] ========== Calibration Wizard ==========")
    
    # Load configuration
    print("[CALIB_RUNNER] Loading configuration...")
    config_dir = Path(__file__).parent.parent / 'config'
    
    with open(config_dir / 'camera.yaml') as f:
        camera_config = yaml.safe_load(f)
    
    with open(config_dir / 'projector.yaml') as f:
        projector_config = yaml.safe_load(f)
    
    # Initialize camera with config
    print("[CALIB_RUNNER] Initializing camera...")
    camera = RealSenseStream(
        width=camera_config['realsense']['width'],
        height=camera_config['realsense']['height'],
        fps=camera_config['realsense']['fps']
    )
    camera.start()
    
    # Run wizard with projector config
    proj = projector_config['projector']
    disp = projector_config['display']
    calib = projector_config['calibration']
    
    wizard = CalibrationWizard(
        camera, 
        projector_width=proj['width'],
        projector_height=proj['height'],
        margin_px=proj['margin_px'],
        monitor_offset_x=disp['monitor_offset_x'],
        monitor_offset_y=disp['monitor_offset_y'],
        borderless=disp['borderless'],
        hide_cursor=disp['hide_cursor'],
        marker_size_m=calib['marker_size_m']
    )
    
    try:
        print("[CALIB_RUNNER] Running calibration wizard...")
        results = wizard.run()
        print("[CALIB_RUNNER] Calibration wizard completed!")
        
        # Save to config
        config_path = Path(__file__).parent.parent / 'config' / 'arena.yaml'
        
        print("[CALIB_RUNNER] Saving calibration to {}".format(config_path))
        
        # Convert numpy types to Python native types for clean YAML
        import numpy as np
        
        def numpy_to_python(obj):
            """Recursively convert numpy types to Python native types."""
            if isinstance(obj, dict):
                return {k: numpy_to_python(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [numpy_to_python(item) for item in obj]
            elif isinstance(obj, np.ndarray):
                return numpy_to_python(obj.tolist())
            elif isinstance(obj, (np.floating, np.float64, np.float32)):
                return float(obj)
            elif isinstance(obj, (np.integer, np.int64, np.int32)):
                return int(obj)
            else:
                return obj
        
        clean_results = numpy_to_python(results)
        
        with open(config_path, 'w') as f:
            yaml.dump(clean_results, f, default_flow_style=False)
        
        print("[CALIB_RUNNER] Calibration saved successfully!")
        print("[CALIB_RUNNER] You can now run the game with: python3 run_game.py")
        
    except Exception as e:
        print("[CALIB_RUNNER] ERROR: {}".format(e))
        
    finally:
        camera.stop()
        print("[CALIB_RUNNER] Done")


if __name__ == '__main__':
    main()



################################################################################
PATH: ./scripts/run_game.py
################################################################################
#!/usr/bin/env python3
"""
Main Game Loop

Runs the tank arena game at 30 FPS:
1. Vision and tracking (ArUco detection, Kalman filtering)
2. World update (occupancy grid, robot poses)
3. Game engine tick (shots, hits, timers)
4. AI decision (behavior tree, path planning)
5. Control (trajectory following, ROS commands)
6. Visualization (Pygame rendering)

Usage:
    python3 run_game.py
    
Prerequisites:
    - Calibration completed (config/arena.yaml exists)
    - ROS bridge running
    - RealSense camera connected
    - Projector configured
"""

import sys
import time
import yaml
import pygame
import numpy as np
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from perception.camera.realsense_stream import RealSenseStream
from perception.camera.aruco_detector import ArucoDetector
from perception.camera.kalman_filter import KalmanFilter
from perception.camera.homography import apply_homography_single
from core.world.world_model import WorldModel
from core.world.coordinate_frames import TransformManager
from core.game.game_engine import GameEngine
from core.ia.strategy import AIStrategy
from core.control.trajectory_follower import TrajectoryFollower
from core.control.ros_bridge_client import ROSBridgeClient
from visualization.pygame_renderer import PygameRenderer


def load_config():
    """Load all configuration files."""
    config_dir = Path(__file__).parent.parent / 'config'
    
    configs = {}
    for config_file in ['arena', 'camera', 'game', 'ia', 'robot']:
        with open(config_dir / '{}.yaml'.format(config_file)) as f:
            configs[config_file] = yaml.safe_load(f)
    
    return configs


def setup_transform_manager(configs):
    """
    Setup coordinate transform from calibration data.
    
    Returns:
        TransformManager with H_C2W set, or None if not calibrated
    """
    transform_mgr = TransformManager()
    
    # Load H_C2W from calibration if available
    transform_data = configs['arena'].get('transform', {})
    scale = transform_data.get('scale_m_per_av', 1.0)
    h_c2w = transform_data.get('H_C2W')
    
    if h_c2w is not None:
        transform_mgr.H_C2W = np.array(h_c2w)
        print("[MAIN] Loaded H_C2W from calibration")
    else:
        # Fallback: use simple scaling (assumes camera aligned with arena)
        transform_mgr.set_av_to_world_scale(scale)
        print("[MAIN] WARNING: No H_C2W found, using fallback scaling")
    
    return transform_mgr


def camera_to_world(detection_center, transform_mgr, fallback_scale=1.0):
    """
    Transform ArUco detection from camera pixels to world meters.
    
    Args:
        detection_center: (u, v) pixel coordinates
        transform_mgr: TransformManager instance
        fallback_scale: Scale to use if no homography
        
    Returns:
        (x, y) in meters
    """
    u, v = detection_center
    
    if transform_mgr.H_C2W is not None:
        return transform_mgr.camera_to_world(u, v)
    else:
        # Fallback: simple scaling from center
        x = u * fallback_scale / 1000.0
        y = v * fallback_scale / 1000.0
        return (x, y)


def get_robot_pose_in_world(detection_data, transform_mgr, fallback_scale=1.0):
    """
    Calcule (x, y, theta) complet dans le référentiel Monde.
    
    CRITIQUE: L'orientation pixel doit être transformée via homographie
    sinon le robot croira avancer vers l'Est alors qu'il va vers le Nord.
    
    Args:
        detection_data: Dict avec 'center' et 'orientation' (pixels)
        transform_mgr: TransformManager instance
        fallback_scale: Fallback si pas d'homographie
        
    Returns:
        (x, y, theta) en mètres et radians (référentiel monde)
    """
    # 1. Centre du robot (Pixels)
    u, v = detection_data['center']
    
    # 2. Point "Devant" le robot (Pixels) - utilise l'orientation pixel
    theta_pix = detection_data['orientation']
    dist_px = 20  # 20 pixels devant
    u_front = u + dist_px * np.cos(theta_pix)
    v_front = v + dist_px * np.sin(theta_pix)
    
    # 3. Projection dans le monde (Mètres)
    x, y = camera_to_world((u, v), transform_mgr, fallback_scale)
    x_front, y_front = camera_to_world((u_front, v_front), transform_mgr, fallback_scale)
    
    # 4. Calcul du vrai angle Monde
    theta_world = np.arctan2(y_front - y, x_front - x)
    
    return x, y, theta_world


def main():
    print("[MAIN] ========== Tank Arena Game ==========")
    
    # Load configuration
    print("[MAIN] Loading configuration...")
    configs = load_config()
    
    # Setup coordinate transforms
    transform_mgr = setup_transform_manager(configs)
    
    # Initialize subsystems
    print("[MAIN] Initializing subsystems...")
    
    # 1. Vision
    camera = RealSenseStream(
        width=configs['camera']['realsense']['width'],
        height=configs['camera']['realsense']['height'],
        fps=configs['camera']['realsense']['fps']
    )
    camera.start()
    
    aruco = ArucoDetector()
    kalman_ai = KalmanFilter()
    kalman_human = KalmanFilter()
    
    # 2. World - avec config robot
    robot_config = configs['arena'].get('robot', {})
    world = WorldModel(
        arena_width_m=configs['arena']['arena']['width_m'],
        arena_height_m=configs['arena']['arena']['height_m'],
        grid_resolution_m=configs['arena']['grid']['resolution_m'],
        robot_radius_m=robot_config.get('radius_m', 0.09),
        inflation_margin_m=robot_config.get('inflation_margin_m', 0.05)
    )
    
    # CRITIQUE: Générer la costmap AVANT de donner la grille à l'IA
    # Sinon le robot frôlera les murs
    world.generate_costmap()
    
    # 3. Game
    game_engine = GameEngine(configs['game'])
    
    # 4. IA (utilise maintenant la costmap gonflée)
    ai_strategy = AIStrategy(configs['ia'])
    ai_strategy.set_planner(world.grid)
    
    # 5. Control - get velocity limits from correct config path
    velocity_limits = configs['robot'].get('velocity_limits', {})
    max_linear_vel = velocity_limits.get('max_linear_mps', 0.22)
    max_angular_vel = velocity_limits.get('max_angular_radps', 2.84)
    
    controller = TrajectoryFollower(configs['robot'].get('control', {}))
    ros_bridge = ROSBridgeClient(
        host=configs['robot']['ros_bridge']['host'],
        port=configs['robot']['ros_bridge']['port']
    )
    ros_bridge.connect()
    
    # 6. Visualization
    display_config = configs['arena'].get('display', {})
    renderer = PygameRenderer(
        width=configs['arena']['projector']['width'],
        height=configs['arena']['projector']['height'],
        margin=configs['arena']['projector']['margin_px'],
        fullscreen=display_config.get('fullscreen', False),
        display_index=display_config.get('display_index', 0)
    )
    renderer.set_arena_dimensions(
        configs['arena']['arena']['width_m'],
        configs['arena']['arena']['height_m']
    )
    
    print("[MAIN] All subsystems initialized")
    print("[MAIN] Starting game loop at 30 FPS...")
    
    # Initialize Input (Joystick/Keyboard)
    joystick = None
    if pygame.joystick.get_count() > 0:
        joystick = pygame.joystick.Joystick(0)
        joystick.init()
        print("[MAIN] Joystick detected: {}".format(joystick.get_name()))
    
    # Game loop
    dt = 1.0 / configs['game']['match']['tick_rate_fps']
    running = True
    
    # Fallback scale from config
    fallback_scale = configs['arena'].get('transform', {}).get('scale_m_per_av', 1.0)
    
    # Initial game state - used to check if playing before game_engine.tick() is called
    game_state = {'status': 'ready'}
    tick_counter = 0
    
    print("[LOG] === GAME LOOP STARTING ===")
    print(f"[LOG] Initial game_state: {game_state}")
    print(f"[LOG] Renderer state: {renderer.match_state}")
    
    try:
        while running:
            tick_start = time.time()
            tick_counter += 1
            
            # --- INPUT HANDLING ---
            human_input = {
                'v': 0.0,
                'omega': 0.0,
                'fire_request': False,
                'start_game': False
            }
            
            # Process Pygame Events
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    running = False
                    print("[LOG] QUIT event received")
                elif event.type == pygame.VIDEORESIZE:
                    # Handle window resize
                    renderer.handle_resize(event.w, event.h)
                elif event.type == pygame.KEYDOWN:
                    print(f"[LOG] KEYDOWN: {pygame.key.name(event.key)}")
                    if event.key == pygame.K_ESCAPE:
                        if renderer.fullscreen:
                            renderer.toggle_fullscreen()
                        else:
                            running = False
                    elif event.key == pygame.K_F11:
                        renderer.toggle_fullscreen()
                    elif event.key == pygame.K_SPACE:
                        human_input['fire_request'] = True
                        if renderer.match_state == renderer.STATE_WAITING:
                            human_input['start_game'] = True
                            print("[LOG] SPACE pressed -> start_game = True")
                    elif event.key == pygame.K_RETURN:
                        human_input['start_game'] = True
                        print("[LOG] RETURN pressed -> start_game = True")
                    elif event.key == pygame.K_d:
                        renderer.show_debug_path = not renderer.show_debug_path
                        print(f"[LOG] Debug path: {'ON' if renderer.show_debug_path else 'OFF'}")
            
            # Sync renderer state with game engine start
            if human_input['start_game']:
                print(f"[LOG] start_game=True, renderer.match_state={renderer.match_state}")
                if renderer.match_state == renderer.STATE_WAITING:
                    renderer.start_match(countdown_seconds=3.0)
                    print("[LOG] >>> MATCH COUNTDOWN STARTED <<<")
                         
            # Joystick Input (PS4/Xbox)
            if joystick:
                v = -joystick.get_axis(1) * max_linear_vel
                omega = -joystick.get_axis(3) * max_angular_vel
                
                if abs(v) > 0.05:
                    human_input['v'] = v
                if abs(omega) > 0.05:
                    human_input['omega'] = omega
                
                if joystick.get_button(5) or joystick.get_button(0):
                    human_input['fire_request'] = True
                if joystick.get_button(9):
                    human_input['start_game'] = True

            # Keyboard fallback for movement (Arrow keys)
            keys = pygame.key.get_pressed()
            if keys[pygame.K_UP]:
                human_input['v'] = max_linear_vel
            if keys[pygame.K_DOWN]:
                human_input['v'] = -max_linear_vel
            if keys[pygame.K_LEFT]:
                human_input['omega'] = max_angular_vel
            if keys[pygame.K_RIGHT]:
                human_input['omega'] = -max_angular_vel

            
            # 1. Vision
            color_frame, _ = camera.get_frames()
            if color_frame is None:
                continue
                
            detections = aruco.detect(color_frame)
            
            # 2. Update robot poses with proper coordinate transform (ANGLE CORRIGÉ)
            if 4 in detections:  # AI robot
                x, y, theta = get_robot_pose_in_world(detections[4], transform_mgr, fallback_scale)
                
                # FIX: Initialiser Kalman à la première détection (évite téléportation depuis 0,0)
                if np.all(kalman_ai.state == 0):
                    kalman_ai.state = np.array([x, y, 0, 0, theta, 0], dtype=float)
                
                kalman_ai.predict()
                kalman_ai.update((x, y, theta))
                filtered_pose = kalman_ai.get_pose()
                world.update_robot_pose(4, filtered_pose)
                 
            if 5 in detections:  # Human robot
                x, y, theta = get_robot_pose_in_world(detections[5], transform_mgr, fallback_scale)
                
                # FIX: Initialiser Kalman à la première détection (évite téléportation depuis 0,0)
                if np.all(kalman_human.state == 0):
                    kalman_human.state = np.array([x, y, 0, 0, theta, 0], dtype=float)
                
                kalman_human.predict()
                kalman_human.update((x, y, theta))
                filtered_pose = kalman_human.get_pose()
                world.update_robot_pose(5, filtered_pose)
            
            # 3. Update world occupancy
            world.update_occupancy()
            
            # 4. Game tick FIRST - to get current game status
            ai_decision = {'state': 'IDLE', 'fire_request': False, 'has_los': False, 'target_position': None}
            game_state = game_engine.tick(world, ai_decision, human_input)
            
            # Log state changes
            if tick_counter % 30 == 0:  # Every second
                print(f"[LOG] Tick {tick_counter}: game_state.status={game_state.get('status')}, renderer={renderer.match_state}")
                print(f"[LOG]   Robot4: {world.get_robot_pose(4)}, Robot5: {world.get_robot_pose(5)}")
            
            # 5. AI Decision (only if game is playing)
            if game_state.get('status', 'ready') == 'playing':
                if tick_counter % 30 == 0:
                    print("[LOG] >>> GAME IS PLAYING - AI DECIDING <<<")
                
                # Construct AI Context
                ai_context = world.get_state_dict()
                ai_context['ai_pose'] = world.get_robot_pose(4)
                ai_context['human_pose'] = world.get_robot_pose(5)
                ai_context['raycast_sys'] = game_engine.raycast
                ai_context['game_time'] = time.time()
                
                ai_decision = ai_strategy.decide(ai_context)
                
                if tick_counter % 30 == 0:
                    print(f"[LOG]   AI decision: state={ai_decision.get('state')}, target={ai_decision.get('target_position')}")
                    print(f"[LOG]   AI path length: {len(ai_strategy.current_path)}")
            
            # 6. Control (only if game is playing)
            if game_state.get('status', 'ready') == 'playing':
                # AI Control
                if ai_decision.get('target_position'):
                    path = ai_strategy.current_path
                    if path:
                        v_ai, omega_ai = controller.compute_control(world.get_robot_pose(4), path)
                        ros_bridge.send_velocity_command(4, v_ai, omega_ai)
                        renderer.set_ai_path(path)
                        
                        if tick_counter % 30 == 0:
                            print(f"[LOG]   Sending cmd: v={v_ai:.3f}, omega={omega_ai:.3f}")
                    else:
                        ros_bridge.send_velocity_command(4, 0, 0)
                        if tick_counter % 30 == 0:
                            print("[LOG]   No path - sending STOP")
                else:
                    ros_bridge.send_velocity_command(4, 0, 0)
                    if tick_counter % 30 == 0:
                        print("[LOG]   No target_position - sending STOP")
                     
                # Human Control (for RC mode)
                if human_input['v'] != 0 or human_input['omega'] != 0:
                    ros_bridge.send_velocity_command(5, human_input['v'], human_input['omega'])
            else:
                # Not playing - ensure robots are stopped
                ros_bridge.send_velocity_command(4, 0, 0)
                ros_bridge.send_velocity_command(5, 0, 0)

            
            # 7. Render
            renderer.render_frame(world.get_state_dict(), game_state)
            
            # Maintain frame rate
            elapsed = time.time() - tick_start
            if elapsed < dt:
                time.sleep(dt - elapsed)
                
    except KeyboardInterrupt:
        print("\n[MAIN] Shutting down...")
        
    finally:
        # Cleanup
        camera.stop()
        ros_bridge.disconnect()
        renderer.cleanup()
        print("[MAIN] Goodbye!")


if __name__ == '__main__':
    main()



################################################################################
PATH: ./scripts/show_grid.py
################################################################################
#!/usr/bin/env python3
"""
Check Homography & Grid - Outil de Validation Visuelle

Cet outil permet de vérifier la correspondance entre le Monde Réel et le Monde Virtuel.

Fonctionnalités :
1. Affiche la grille (WorldModel) et les obstacles.
2. Détecte les ArUco en live et projette leur position (Homographie).
3. Permet de vérifier si le robot "voit" correctement les murs.

Commandes :
    [D]   : Basculer l'affichage de l'Inflation (Costmap / Grille brute)
    [ESC] : Quitter
"""

import sys
import time
import yaml
import pygame
import numpy as np
import cv2
from pathlib import Path

# Ajout du chemin racine pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from perception.camera.realsense_stream import RealSenseStream
from perception.camera.aruco_detector import ArucoDetector
from core.world.world_model import WorldModel
from core.world.coordinate_frames import TransformManager

# --- COULEURS ---
C_BG = (10, 10, 15)
C_GRID = (40, 40, 50)
C_OBSTACLE = (200, 50, 50)       # Rouge (Mur physique)
C_INFLATION = (200, 100, 0)      # Orange (Zone de danger)
C_MARKER = (0, 255, 255)         # Cyan (Position calculée du marqueur)
C_TEXT = (255, 255, 255)

def main():
    print("[CHECK] Démarrage de la vérification Homographie & Grille...")

    # 1. Chargement des Configs
    config_dir = Path(__file__).parent.parent / 'config'
    with open(config_dir / 'arena.yaml') as f: arena_cfg = yaml.safe_load(f)
    with open(config_dir / 'camera.yaml') as f: camera_cfg = yaml.safe_load(f)
    with open(config_dir / 'robot.yaml') as f: robot_cfg = yaml.safe_load(f)

    # 2. Initialisation Vision (Caméra + ArUco + Transform)
    cam_cfg = camera_cfg.get('realsense', {})
    camera = RealSenseStream(
        width=cam_cfg.get('width', 848), 
        height=cam_cfg.get('height', 480), 
        fps=cam_cfg.get('fps', 60)
    ) 
    camera.start()
    aruco = ArucoDetector()
    
    transform_mgr = TransformManager()
    if 'transform' in arena_cfg and 'H_C2W' in arena_cfg['transform']:
        transform_mgr.H_C2W = np.array(arena_cfg['transform']['H_C2W'])
        print("[CHECK] Matrice d'homographie chargée.")
    else:
        print("[CHECK] ERREUR CRITIQUE : Pas de calibration (H_C2W) dans arena.yaml !")
        print("        Veuillez lancer 'python3 scripts/run_calibration.py' d'abord.")
        return

    # 3. Initialisation WorldModel (Grille + Inflation)
    # Config robot depuis les deux fichiers
    robot_radius = robot_cfg.get('physical', {}).get('robot_radius_m', 0.09)
    inflation_margin = arena_cfg.get('robot', {}).get('inflation_margin_m', 0.05)
    
    print("[CHECK] Génération de la Costmap...")
    world = WorldModel(
        arena_width_m=arena_cfg['arena']['width_m'],
        arena_height_m=arena_cfg['arena']['height_m'],
        grid_resolution_m=arena_cfg['grid']['resolution_m'],
        robot_radius_m=robot_radius,
        inflation_margin_m=inflation_margin
    )
    # Important : On génère l'inflation pour le test visuel
    world.generate_costmap()

    # 4. Setup Affichage Pygame
    pygame.init()
    
    # On définit une fenêtre d'affichage confortable (ex: 80% de la hauteur écran)
    info = pygame.display.Info()
    win_h = int(info.current_h * 0.8)
    # Calcul largeur pour respecter le ratio de l'arène
    aspect_ratio = world.arena_width / world.arena_height
    win_w = int(win_h * aspect_ratio)
    
    screen = pygame.display.set_mode((win_w, win_h))
    pygame.display.set_caption("Check Homography - [D] Toggle Inflation")
    font = pygame.font.SysFont("Consolas", 18)

    # Facteur d'échelle Affichage (Pixels écran / Mètres monde)
    scale_px = win_w / world.arena_width

    def to_screen(x_m, y_m):
        """Convertit mètres (monde) -> pixels (fenêtre debug)."""
        # Pygame Y est inversé (0 en haut)
        sx = int(x_m * scale_px)
        sy = int(win_h - (y_m * scale_px))
        return sx, sy

    # --- BOUCLE PRINCIPALE ---
    running = True
    show_inflation = False  # Par défaut, on voit les murs bruts

    print("[CHECK] Fenêtre ouverte. Appuyez sur [D] pour changer de vue, [ESC] pour quitter.")

    try:
        while running:
            # A. Gestion Événements
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    running = False
                elif event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_ESCAPE:
                        running = False
                    elif event.key == pygame.K_d:
                        show_inflation = not show_inflation
                        print(f"[CHECK] Mode Inflation : {'ACTIVÉ' if show_inflation else 'DÉSACTIVÉ'}")

            # B. Acquisition Vision
            color_frame, _ = camera.get_frames()
            detections = {}
            if color_frame is not None:
                detections = aruco.detect(color_frame)

            # C. Dessin - Fond & Grille
            screen.fill(C_BG)
            
            # Dessiner la grille fine (tous les 10cm)
            for x in np.arange(0, world.arena_width, 0.1):
                p1 = to_screen(x, 0)
                p2 = to_screen(x, world.arena_height)
                pygame.draw.line(screen, C_GRID, p1, p2, 1)
            for y in np.arange(0, world.arena_height, 0.1):
                p1 = to_screen(0, y)
                p2 = to_screen(world.arena_width, y)
                pygame.draw.line(screen, C_GRID, p1, p2, 1)

            # D. Dessin - Obstacles & Inflation
            # On récupère la bonne grille
            grid_data = world.grid.costmap if (show_inflation and hasattr(world.grid, 'costmap')) else world.grid.grid
            rows, cols = grid_data.shape
            
            # Pour optimiser, on ne dessine que les cases occupées
            cell_w_px = world.grid.resolution * scale_px
            
            for r in range(rows):
                for c in range(cols):
                    if grid_data[r, c] > 0.5:  # Occupé
                        # Centre de la case en mètres
                        xm, ym = world.grid.grid_to_world(r, c)
                        sx, sy = to_screen(xm, ym)
                        
                        # Rectangle centré
                        rect = pygame.Rect(
                            sx - cell_w_px/2, sy - cell_w_px/2, 
                            cell_w_px + 1, cell_w_px + 1  # +1 pour éviter les lignes noires
                        )
                        color = C_INFLATION if show_inflation else C_OBSTACLE
                        pygame.draw.rect(screen, color, rect)

            # E. Dessin - Marqueurs ArUco (Check Homographie)
            for marker_id, data in detections.items():
                # 1. Calcul position Monde via Homographie
                u, v = data['center']
                xm, ym = transform_mgr.camera_to_world(u, v)
                
                # 2. Conversion vers écran debug
                sx, sy = to_screen(xm, ym)
                
                # 3. Dessiner la cible (Le point central)
                # Cercle
                pygame.draw.circle(screen, C_MARKER, (sx, sy), 8, 2)
                # Croix
                pygame.draw.line(screen, C_MARKER, (sx-10, sy), (sx+10, sy), 2)
                pygame.draw.line(screen, C_MARKER, (sx, sy-10), (sx, sy+10), 2)
                
                # 4. Afficher infos texte
                text_pos = f"ID {marker_id}: ({xm:.2f}m, {ym:.2f}m)"
                label = font.render(text_pos, True, C_TEXT)
                screen.blit(label, (sx + 15, sy - 10))

            # F. UI Overlay
            mode_str = "COSTMAP (GONFLÉE)" if show_inflation else "OBSTACLES BRUTS"
            info_str = f"[D] Changer Vue : {mode_str}"
            ui_label = font.render(info_str, True, (0, 255, 0))
            pygame.draw.rect(screen, (0, 0, 0), (10, 10, ui_label.get_width()+20, 30))
            screen.blit(ui_label, (20, 15))

            pygame.display.flip()

    except KeyboardInterrupt:
        pass
    finally:
        camera.stop()
        pygame.quit()
        print("[CHECK] Fin.")

if __name__ == '__main__':
    main()



################################################################################
PATH: ./visualization/colors.py
################################################################################
"""
Colors - Palette Couleurs

Définit toutes les couleurs utilisées dans le projet.
"""

# Couleurs de base
WHITE = (255, 255, 255)
BLACK = (0, 0, 0)
RED = (255, 50, 50)
GREEN = (50, 255, 50)
BLUE = (50, 150, 255)
YELLOW = (255, 255, 50)
ORANGE = (255, 165, 0)
PURPLE = (160, 32, 240)
CYAN = (0, 255, 255)
MAGENTA = (255, 0, 255)

# Nuances de gris
GRAY_LIGHT = (200, 200, 200)
GRAY = (150, 150, 150)
GRAY_DARK = (100, 100, 100)

# Couleurs robots
ROBOT_AI_COLOR = BLUE
ROBOT_HUMAN_COLOR = RED

# Couleurs interface
HUD_TEXT_COLOR = BLACK
HUD_BG_COLOR = (255, 255, 255, 180)  # Semi-transparent
TIMER_COLOR = BLACK
SCORE_AI_COLOR = BLUE
SCORE_HUMAN_COLOR = RED

# Couleurs visualisation
ARENA_BORDER_COLOR = BLACK
ARENA_BG_COLOR = WHITE
OBSTACLE_COLOR = GRAY_DARK
GRID_LINE_COLOR = (220, 220, 220)

# Couleurs tir
SHOT_AI_COLOR = (100, 200, 255)
SHOT_HUMAN_COLOR = (255, 100, 100)
HIT_FLASH_COLOR = (255, 255, 0, 200)

# Couleurs lock-on
LOCKON_COLOR = RED
LOCKON_PULSE_COLOR = (255, 100, 100)

# Couleurs debug
PATH_COLOR = (0, 255, 0, 100)
WAYPOINT_COLOR = GREEN
LOS_LINE_COLOR = YELLOW



################################################################################
PATH: ./visualization/debug_draw.py
################################################################################
"""
Debug Draw - Visualisation Debug

Affiche éléments de debug (paths, LOS, grille, etc.).

Logs: [DEBUG_DRAW] prefix
"""

import pygame
import numpy as np
from typing import List, Tuple, Optional
from .colors import *


class DebugDraw:
    """
    Gère affichage éléments debug.
    """
    
    def __init__(self, surface: pygame.Surface, projector_mapping):
        """
        Initialize debug drawer.
        
        Args:
            surface: Surface Pygame
            projector_mapping: ProjectorMapping instance
        """
        self.surface = surface
        self.mapping = projector_mapping
        self.font = pygame.font.SysFont('Courier', 16)
        
    def draw_path(self, waypoints: List[Tuple[float, float]], 
                 color: Tuple = PATH_COLOR):
        """
        Dessine chemin planifié.
        
        Args:
            waypoints: Liste (x, y) en mètres
            color: Couleur ligne
        """
        if len(waypoints) < 2:
            return
        
        # Convertir en pixels
        points_px = [self.mapping.world_to_projector(x, y) 
                    for x, y in waypoints]
        
        # Dessiner lignes
        pygame.draw.lines(self.surface, color, False, points_px, 3)
        
        # Dessiner waypoints
        for px in points_px:
            pygame.draw.circle(self.surface, WAYPOINT_COLOR, px, 5)
            
    def draw_line_of_sight(self,
                          start: Tuple[float, float],
                          end: Tuple[float, float],
                          blocked: bool = False):
        """
        Dessine ligne de vue.
        
        Args:
            start: Point départ (x, y) mètres
            end: Point arrivée (x, y) mètres
            blocked: True si bloquée
        """
        p1 = self.mapping.world_to_projector(*start)
        p2 = self.mapping.world_to_projector(*end)
        
        color = RED if blocked else LOS_LINE_COLOR
        pygame.draw.line(self.surface, color, p1, p2, 2)
        
    def draw_grid(self, grid_resolution: float = 0.5):
        """
        Dessine grille métrique.
        
        Args:
            grid_resolution: Espacement grille (mètres)
        """
        # Lignes verticales
        x = 0.0
        while x <= self.mapping.arena_width_m:
            p1 = self.mapping.world_to_projector(x, 0)
            p2 = self.mapping.world_to_projector(x, self.mapping.arena_height_m)
            pygame.draw.line(self.surface, GRID_LINE_COLOR, p1, p2, 1)
            x += grid_resolution
        
        # Lignes horizontales
        y = 0.0
        while y <= self.mapping.arena_height_m:
            p1 = self.mapping.world_to_projector(0, y)
            p2 = self.mapping.world_to_projector(self.mapping.arena_width_m, y)
            pygame.draw.line(self.surface, GRID_LINE_COLOR, p1, p2, 1)
            y += grid_resolution
            
    def draw_occupancy_grid(self, grid):
        """
        Dessine grille occupation.
        
        Args:
            grid: OccupancyGrid instance
        """
        # Simplification: dessiner cellules occupées
        for row in range(grid.n_rows):
            for col in range(grid.n_cols):
                if grid.grid[row, col] > 0.5:
                    # Convertir cell → monde → pixels
                    x_m, y_m = grid.grid_to_world(row, col)
                    px, py = self.mapping.world_to_projector(x_m, y_m)
                    
                    cell_size = self.mapping.scale_length(grid.resolution)
                    rect = pygame.Rect(px, py, cell_size, cell_size)
                    
                    alpha = int(grid.grid[row, col] * 150)
                    s = pygame.Surface((cell_size, cell_size))
                    s.set_alpha(alpha)
                    s.fill(OBSTACLE_COLOR)
                    self.surface.blit(s, rect)
                    
    def draw_robot_info(self, robot_id: int,
                       pose: Tuple[float, float, float],
                       velocity: Optional[Tuple[float, float, float]] = None):
        """
        Affiche infos robot au-dessus.
        
        Args:
            robot_id: ID robot
            pose: (x, y, theta)
            velocity: (vx, vy, omega) optionnel
        """
        x, y, theta = pose
        px, py = self.mapping.world_to_projector(x, y)
        
        # Texte infos
        info_lines = [f"R{robot_id}"]
        info_lines.append(f"({x:.2f}, {y:.2f})")
        
        if velocity:
            vx, vy, omega = velocity
            v_norm = np.sqrt(vx**2 + vy**2)
            info_lines.append(f"v={v_norm:.2f}m/s")
        
        # Dessiner texte
        y_offset = -40
        for line in info_lines:
            text = self.font.render(line, True, BLACK)
            text_rect = text.get_rect(center=(px, py + y_offset))
            
            # Fond
            bg = text_rect.inflate(10, 4)
            s = pygame.Surface((bg.width, bg.height))
            s.set_alpha(180)
            s.fill(WHITE)
            self.surface.blit(s, bg)
            
            self.surface.blit(text, text_rect)
            y_offset += 20



################################################################################
PATH: ./visualization/__init__.py
################################################################################



################################################################################
PATH: ./visualization/projector_overlay.py
################################################################################
"""
Projector Overlay - Gestion Affichage Projecteur

Gère affichage superposé sur projecteur (éléments calibration, debug).

Logs: [OVERLAY] prefix
"""

import pygame
import numpy as np
from typing import Tuple, Optional
from .colors import *


class ProjectorOverlay:
    """
    Gère éléments overlay projetés.
    """
    
    def __init__(self, surface: pygame.Surface):
        """
        Initialize overlay.
        
        Args:
            surface: Surface Pygame où dessiner
        """
        self.surface = surface
        self.font = pygame.font.SysFont('Arial', 32)
        self.font_small = pygame.font.SysFont('Arial', 20)
        
    def draw_calibration_markers(self,
                                positions: list,
                                marker_ids: list,
                                size: int = 100):
        """
        Dessine marqueurs ArUco virtuels pour calibration.
        
        Args:
            positions: Liste (x, y) positions pixels
            marker_ids: Liste IDs marqueurs (0-3)
            size: Taille marqueurs pixels
        """
        for pos, marker_id in zip(positions, marker_ids):
            # Dessiner carré blanc avec bordure noire
            rect = pygame.Rect(pos[0] - size//2, pos[1] - size//2, size, size)
            pygame.draw.rect(self.surface, WHITE, rect)
            pygame.draw.rect(self.surface, BLACK, rect, 3)
            
            # Dessiner ID au centre
            text = self.font.render(str(marker_id), True, BLACK)
            text_rect = text.get_rect(center=pos)
            self.surface.blit(text, text_rect)
    
    def draw_crosshair(self, pos: Tuple[int, int], 
                      size: int = 20,
                      color: Tuple = RED):
        """
        Dessine réticule.
        
        Args:
            pos: Position (x, y)
            size: Taille
            color: Couleur
        """
        x, y = pos
        pygame.draw.line(self.surface, color, 
                        (x - size, y), (x + size, y), 2)
        pygame.draw.line(self.surface, color,
                        (x, y - size), (x, y + size), 2)
        pygame.draw.circle(self.surface, color, pos, size, 2)
    
    def draw_message(self, message: str,
                    position: Tuple[int, int],
                    color: Tuple = BLACK,
                    font_size: str = 'normal'):
        """
        Affiche message texte.
        
        Args:
            message: Texte à afficher
            position: (x, y) position
            color: Couleur texte
            font_size: 'normal' ou 'small'
        """
        font = self.font if font_size == 'normal' else self.font_small
        text = font.render(message, True, color)
        self.surface.blit(text, position)
    
    def draw_instruction(self, instruction: str):
        """
        Affiche instruction centrée en haut.
        
        Args:
            instruction: Texte instruction
        """
        text = self.font.render(instruction, True, BLACK)
        text_rect = text.get_rect(center=(self.surface.get_width() // 2, 100))
        
        # Fond semi-transparent
        bg_rect = text_rect.inflate(40, 20)
        s = pygame.Surface((bg_rect.width, bg_rect.height))
        s.set_alpha(200)
        s.fill(WHITE)
        self.surface.blit(s, bg_rect)
        
        # Texte
        self.surface.blit(text, text_rect)



################################################################################
PATH: ./visualization/pygame_renderer.py
################################################################################
"""
Pygame Renderer - Modern Gaming Visualization Engine

Premium rendering with:
- Strategic HUD placement (corners, never obscures ArUcos in center)
- Match start/end screens with animations
- Debug path visualization (toggle with 'D' key)
- Dynamic scoring with visual feedback
- Modern design (gradients, shadows, premium fonts)

Logs: [VIS] prefix
"""

import pygame
import numpy as np
import os
import math
from typing import Dict, Tuple, Optional, List
from datetime import datetime


class PygameRenderer:
    """
    Modern gaming renderer with tactical HUD design.
    
    Features:
    - Corner-based HUD (doesn't obstruct center ArUcos)
    - Match states (waiting, countdown, playing, finished)
    - Debug path visualization (toggle D)
    - Score animations and visual effects
    """
    
    # Match states
    STATE_WAITING = "waiting"
    STATE_COUNTDOWN = "countdown"
    STATE_PLAYING = "playing"
    STATE_FINISHED = "finished"
    
    def __init__(self, width: int = 1024, height: int = 768, margin: int = 50, 
                 fullscreen: bool = True, display_index: int = 0):
        """
        Initialize renderer with modern gaming UI.
        
        Args:
            width: Projector width
            height: Projector height
            margin: Safe zone margin
            fullscreen: If True, fullscreen mode
            display_index: 0 = primary, 1 = projector
        """
        # Set display position BEFORE pygame.init()
        if display_index == 1:
            os.environ['SDL_VIDEO_WINDOW_POS'] = f'{width},0'
        elif display_index == 0:
            os.environ['SDL_VIDEO_WINDOW_POS'] = '0,0'
        
        pygame.init()
        
        self.base_width = width
        self.base_height = height
        self.width = width
        self.height = height
        self.margin = margin
        self.fullscreen = fullscreen
        
        self.draw_width = width - 2 * margin
        self.draw_height = height - 2 * margin
        
        # Create display - always start in resizable windowed mode for stability
        self._create_display()
        
        pygame.display.set_caption("Tank Arena - Tactical Combat")
        
        # Fonts - Premium hierarchy
        pygame.font.init()
        self.font_title = pygame.font.SysFont('Arial Black', 72, bold=True)
        self.font_large = pygame.font.SysFont('Arial Black', 48, bold=True)
        self.font_medium = pygame.font.SysFont('Arial', 36, bold=True)
        self.font_small = pygame.font.SysFont('Arial', 24)
        self.font_tiny = pygame.font.SysFont('Courier', 16)
        
        # Premium color palette
        self.BG_DARK = (20, 20, 25)
        self.BG_LIGHT = (240, 240, 245)
        
        self.AI_PRIMARY = (50, 150, 255)      # Blue
        self.AI_GLOW = (100, 200, 255)
        self.AI_DARK = (20, 80, 150)
        
        self.HUMAN_PRIMARY = (255, 80, 80)    # Red
        self.HUMAN_GLOW = (255, 150, 150)
        self.HUMAN_DARK = (180, 30, 30)
        
        self.ACCENT_GOLD = (255, 200, 50)
        self.ACCENT_GREEN = (50, 255, 150)
        self.ACCENT_ORANGE = (255, 150, 50)
        
        self.TEXT_DARK = (30, 30, 35)
        self.TEXT_LIGHT = (250, 250, 255)
        self.TEXT_GRAY = (150, 150, 160)
        
        self.OVERLAY_DARK = (0, 0, 0, 180)    # Semi-transparent
        self.OVERLAY_LIGHT = (255, 255, 255, 200)
        
        # Arena dimensions
        self.arena_width_m = 3.0
        self.arena_height_m = 2.0
        self.scale = None
        
        # Match state
        self.match_state = self.STATE_WAITING
        self.countdown_start = None
        self.match_start_time = None
        self.winner = None
        
        # Debug visualization
        self.show_debug_path = False
        self.ai_path = []  # List of (x, y) waypoints
        
        # Visual effects
        self.score_flash_ai = 0
        self.score_flash_human = 0
        self.last_ai_score = 0
        
    def _create_display(self):
        """Create or recreate the display with current settings."""
        try:
            if self.fullscreen:
                # Try fullscreen without HWSURFACE (more compatible)
                self.screen = pygame.display.set_mode(
                    (self.width, self.height),
                    pygame.FULLSCREEN
                )
            else:
                # Windowed + resizable
                self.screen = pygame.display.set_mode(
                    (self.width, self.height),
                    pygame.RESIZABLE
                )
            print(f"[VIS] Display created: {self.width}x{self.height} fullscreen={self.fullscreen}")
        except pygame.error as e:
            print(f"[VIS] Display error: {e}, falling back to windowed")
            self.fullscreen = False
            self.screen = pygame.display.set_mode(
                (self.base_width, self.base_height),
                pygame.RESIZABLE
            )
    
    def toggle_fullscreen(self):
        """Toggle between fullscreen and windowed mode."""
        self.fullscreen = not self.fullscreen
        if not self.fullscreen:
            # Restore base dimensions when exiting fullscreen
            self.width = self.base_width
            self.height = self.base_height
        else:
            # Get current display info for fullscreen
            info = pygame.display.Info()
            self.width = info.current_w
            self.height = info.current_h
        
        self._update_dimensions()
        self._create_display()
    
    def _update_dimensions(self):
        """Update drawing dimensions after resize."""
        self.draw_width = self.width - 2 * self.margin
        self.draw_height = self.height - 2 * self.margin
        
        # Recalculate scale
        if self.arena_width_m > 0 and self.arena_height_m > 0:
            scale_x = self.draw_width / self.arena_width_m
            scale_y = self.draw_height / self.arena_height_m
            self.scale = min(scale_x, scale_y)
    
    def handle_resize(self, new_width: int, new_height: int):
        """Handle window resize event."""
        self.width = new_width
        self.height = new_height
        self._update_dimensions()
        self.screen = pygame.display.set_mode((new_width, new_height), pygame.RESIZABLE)
        print(f"[VIS] Resized to {new_width}x{new_height}")
        self.last_human_score = 0
        
    def set_arena_dimensions(self, width_m: float, height_m: float):
        """Set arena dimensions for coordinate conversion."""
        self.arena_width_m = width_m
        self.arena_height_m = height_m
        
        scale_x = self.draw_width / width_m
        scale_y = self.draw_height / height_m
        self.scale = min(scale_x, scale_y)
        
    def world_to_screen(self, x_m: float, y_m: float) -> Tuple[int, int]:
        """Convert world coordinates to screen pixels."""
        if self.scale is None:
            self.scale = 100
        
        px = self.margin + int(x_m * self.scale)
        py = self.margin + int((self.arena_height_m - y_m) * self.scale)
        
        return (px, py)
    
    def handle_keypress(self, event: pygame.event.Event):
        """
        Handle keyboard input.
        
        Args:
            event: Pygame key event
        """
        if event.type == pygame.KEYDOWN:
            if event.key == pygame.K_d:
                # Toggle debug path visualization
                self.show_debug_path = not self.show_debug_path
                print("[VIS] Debug path: {}".format('ON' if self.show_debug_path else 'OFF'))
            elif event.key == pygame.K_F11:
                # Toggle fullscreen
                self.toggle_fullscreen()
            elif event.key == pygame.K_ESCAPE and self.fullscreen:
                # Exit fullscreen on ESC
                self.toggle_fullscreen()
    
    def start_match(self, countdown_seconds: float = 3.0):
        """Start match countdown."""
        self.match_state = self.STATE_COUNTDOWN
        self.countdown_start = pygame.time.get_ticks() / 1000.0
        self.countdown_duration = countdown_seconds
        
    def begin_playing(self):
        """Begin actual gameplay."""
        self.match_state = self.STATE_PLAYING
        self.match_start_time = pygame.time.get_ticks() / 1000.0
        
    def end_match(self, winner: str):
        """
        End match and show results.
        
        Args:
            winner: "AI" or "HUMAN"
        """
        self.match_state = self.STATE_FINISHED
        self.winner = winner
        
    def set_ai_path(self, waypoints: List[Tuple[float, float]]):
        """
        Set AI path for debug visualization.
        
        Args:
            waypoints: List of (x, y) positions in meters
        """
        self.ai_path = waypoints
    
    def render_frame(self, world_state: Dict, game_state: Dict):
        """
        Render complete frame based on match state.
        
        Args:
            world_state: Robot poses, obstacles, etc.
            game_state: Scores, time, status
        """
        # Update score flash effects
        ai_score = game_state.get('robot_4_hits_inflicted', 0)
        human_score = game_state.get('robot_5_hits_inflicted', 0)
        
        if ai_score > self.last_ai_score:
            self.score_flash_ai = 30  # Flash for 30 frames
            self.last_ai_score = ai_score
        
        if human_score > self.last_human_score:
            self.score_flash_human = 30
            self.last_human_score = human_score
        
        self.score_flash_ai = max(0, self.score_flash_ai - 1)
        self.score_flash_human = max(0, self.score_flash_human - 1)
        
        # Render based on state
        if self.match_state == self.STATE_WAITING:
            self._render_waiting_screen()
        elif self.match_state == self.STATE_COUNTDOWN:
            self._render_countdown(world_state, game_state)
        elif self.match_state == self.STATE_PLAYING:
            self._render_gameplay(world_state, game_state)
        elif self.match_state == self.STATE_FINISHED:
            self._render_end_screen(game_state)
        
        # Update display
        pygame.display.flip()
    
    def _render_waiting_screen(self):
        """Render waiting for match start screen."""
        self.screen.fill(self.BG_DARK)
        
        # Title
        title = self.font_title.render("TANK ARENA", True, self.ACCENT_GOLD)
        title_rect = title.get_rect(center=(self.width // 2, self.height // 2 - 100))
        self.screen.blit(title, title_rect)
        
        # Subtitle
        subtitle = self.font_medium.render("Tactical Robot Combat", True, self.TEXT_LIGHT)
        subtitle_rect = subtitle.get_rect(center=(self.width // 2, self.height // 2))
        self.screen.blit(subtitle, subtitle_rect)
        
        # Instruction
        pulse = int(128 + 127 * math.sin(pygame.time.get_ticks() / 500))
        instruction = self.font_small.render("Press START to begin", True, (pulse, pulse, pulse))
        instr_rect = instruction.get_rect(center=(self.width // 2, self.height // 2 + 100))
        self.screen.blit(instruction, instr_rect)
    
    def _render_countdown(self, world_state: Dict, game_state: Dict):
        """Render countdown before match."""
        # Draw arena first (faded)
        self._render_gameplay(world_state, game_state, alpha=100)
        
        # Overlay countdown
        overlay = pygame.Surface((self.width, self.height), pygame.SRCALPHA)
        overlay.fill((0, 0, 0, 150))
        self.screen.blit(overlay, (0, 0))
        
        # Calculate countdown
        elapsed = pygame.time.get_ticks() / 1000.0 - self.countdown_start
        remaining = max(0, self.countdown_duration - elapsed)
        
        if remaining > 0:
            count_num = int(remaining) + 1
            scale = 1.0 + 0.3 * (1.0 - (remaining % 1.0))
            
            count_text = self.font_title.render(str(count_num), True, self.ACCENT_GOLD)
            count_text = pygame.transform.scale(
                count_text,
                (int(count_text.get_width() * scale), int(count_text.get_height() * scale))
            )
            count_rect = count_text.get_rect(center=(self.width // 2, self.height // 2))
            self.screen.blit(count_text, count_rect)
        else:
            # Switch to playing
            self.begin_playing()
    
    def _render_gameplay(self, world_state: Dict, game_state: Dict, alpha: int = 255):
        """Render main gameplay view."""
        # Background
        self.screen.fill(self.BG_LIGHT)
        
        # Arena border
        self._draw_arena_border()
        
        # Debug: AI path (if enabled)
        if self.show_debug_path and self.ai_path:
            self._draw_ai_path()
        
        # Obstacles
        self._draw_obstacles(world_state.get('occupancy_grid'))
        
        # Robots
        robot_4_pose = world_state.get('robot_4_pose', (0, 0, 0))
        robot_5_pose = world_state.get('robot_5_pose', (0, 0, 0))
        
        self._draw_robot(robot_4_pose, self.AI_PRIMARY, "AI", self.AI_GLOW)
        self._draw_robot(robot_5_pose, self.HUMAN_PRIMARY, "HUMAN", self.HUMAN_GLOW)
        
        # Lock-on indicator
        if game_state.get('ai_has_los', False):
            self._draw_lock_on(robot_5_pose[:2])
        
        # HUD - STRATEGIC PLACEMENT (corners only!)
        self._draw_modern_hud(game_state)
        
        # Debug info (bottom right corner)
        if self.show_debug_path:
            self._draw_debug_info(game_state)
    
    def _draw_arena_border(self):
        """Draw arena boundary with modern style."""
        rect = pygame.Rect(self.margin, self.margin, self.draw_width, self.draw_height)
        
        # Outer glow
        for i in range(3):
            glow_rect = rect.inflate(i * 4, i * 4)
            alpha = 40 - i * 10
            s = pygame.Surface((glow_rect.width, glow_rect.height), pygame.SRCALPHA)
            pygame.draw.rect(s, (*self.TEXT_GRAY, alpha), s.get_rect(), 2)
            self.screen.blit(s, glow_rect.topleft)
        
        # Main border
        pygame.draw.rect(self.screen, self.TEXT_DARK, rect, 4)
    
    def _draw_obstacles(self, grid):
        """Draw obstacles from occupancy grid."""
        if grid is None:
            return
            
        # Iterate through occupied cells
        # Note: inefficient for very large grids, optimization would be to use static surface
        rows, cols = grid.grid.shape
        
        # Color for obstacles
        obs_color = (60, 60, 70)
        
        for r in range(rows):
            for c in range(cols):
                if grid.grid[r, c] > 0.5:
                    # Convert grid cell to world m -> screen px
                    # Grid (r, c) -> World (x, y) center
                    x_m, y_m = grid.grid_to_world(r, c)
                    
                    # Convert to screen
                    px, py = self.world_to_screen(x_m, y_m)
                    
                    # Size of cell in pixels
                    size = int(grid.resolution * self.scale) + 1  # +1 to avoid gaps
                    
                    # Draw rect
                    rect = pygame.Rect(px - size//2, py - size//2, size, size)
                    pygame.draw.rect(self.screen, obs_color, rect)
    
    def _draw_robot(self, pose: Tuple[float, float, float], color: Tuple, 
                   label: str, glow_color: Tuple):
        """Draw robot with modern visual effects."""
        x, y, theta = pose
        px, py = self.world_to_screen(x, y)
        
        radius_px = int(0.09 * self.scale) if self.scale else 30
        
        # Glow effect
        for i in range(3):
            glow_radius = radius_px + (3 - i) * 8
            alpha = 30 + i * 10
            s = pygame.Surface((glow_radius * 2, glow_radius * 2), pygame.SRCALPHA)
            pygame.draw.circle(s, (*glow_color, alpha), (glow_radius, glow_radius), glow_radius)
            self.screen.blit(s, (px - glow_radius, py - glow_radius))
        
        # Main body
        pygame.draw.circle(self.screen, color, (px, py), radius_px)
        pygame.draw.circle(self.screen, self.TEXT_DARK, (px, py), radius_px, 3)
        
        # Orientation indicator (cannon)
        line_len = radius_px * 1.8
        end_x = px + int(line_len * np.cos(theta))
        end_y = py - int(line_len * np.sin(theta))
        
        # Cannon shadow
        pygame.draw.line(self.screen, (0, 0, 0, 100), 
                        (px + 2, py + 2), (end_x + 2, end_y + 2), 5)
        # Cannon
        pygame.draw.line(self.screen, self.TEXT_DARK, (px, py), (end_x, end_y), 5)
        pygame.draw.circle(self.screen, self.ACCENT_GOLD, (end_x, end_y), 6)
    
    def _draw_ai_path(self):
        """Draw AI pathfinding visualization (debug mode)."""
        if len(self.ai_path) < 2:
            return
        
        # Convert waypoints to screen coords
        points = [self.world_to_screen(x, y) for x, y in self.ai_path]
        
        # Draw path line
        if len(points) >= 2:
            pygame.draw.lines(self.screen, self.AI_PRIMARY, False, points, 3)
        
        # Draw waypoint markers
        for i, (px, py) in enumerate(points):
            pygame.draw.circle(self.screen, self.AI_PRIMARY, (px, py), 4)
            if i == len(points) - 1:
                # Goal marker
                pygame.draw.circle(self.screen, self.ACCENT_GREEN, (px, py), 8, 2)
    
    def _draw_lock_on(self, target_pos: Tuple[float, float]):
        """Draw lock-on indicator with pulse effect."""
        px, py = self.world_to_screen(*target_pos)
        
        t = pygame.time.get_ticks() / 1000.0
        radius = 25 + int(8 * math.sin(t * 4))
        
        # Crosshair
        pygame.draw.circle(self.screen, self.HUMAN_PRIMARY, (px, py), radius, 3)
        pygame.draw.line(self.screen, self.HUMAN_PRIMARY, 
                        (px - radius - 10, py), (px - radius - 5, py), 3)
        pygame.draw.line(self.screen, self.HUMAN_PRIMARY, 
                        (px + radius + 5, py), (px + radius + 10, py), 3)
        pygame.draw.line(self.screen, self.HUMAN_PRIMARY, 
                        (px, py - radius - 10), (px, py - radius - 5), 3)
        pygame.draw.line(self.screen, self.HUMAN_PRIMARY, 
                        (px, py + radius + 5), (px, py + radius + 10), 3)
        
        # "LOCKED" text
        locked_text = self.font_tiny.render("LOCKED", True, self.HUMAN_PRIMARY)
        self.screen.blit(locked_text, (px - 25, py + radius + 15))
    
    def _draw_modern_hud(self, game_state: Dict):
        """
        Draw HUD in CORNERS only (never obscures center ArUcos).
        
        Layout:
        - Top-left: AI score
        - Top-right: Human score
        - Top-center: Timer (small, minimal)
        - Bottom-left: Match info
        - Bottom-right: Debug toggle hint
        """
        # --- TOP-LEFT: AI SCORE ---
        ai_score = game_state.get('robot_4_hits_inflicted', 0)
        ai_health = game_state.get('robot_4_hits_received', 0)
        
        flash_alpha = self.score_flash_ai * 8 if self.score_flash_ai > 0 else 0
        ai_bg_color = (
            min(255, self.AI_PRIMARY[0] + flash_alpha),
            min(255, self.AI_PRIMARY[1] + flash_alpha),
            min(255, self.AI_PRIMARY[2] + flash_alpha)
        )
        
        self._draw_score_corner(30, 30, "AI ROBOT", ai_score, ai_health, 
                               ai_bg_color, self.AI_DARK, "left")
        
        # --- TOP-RIGHT: HUMAN SCORE ---
        human_score = game_state.get('robot_5_hits_inflicted', 0)
        human_health = game_state.get('robot_5_hits_received', 0)
        
        flash_alpha = self.score_flash_human * 8 if self.score_flash_human > 0 else 0
        human_bg_color = (
            min(255, self.HUMAN_PRIMARY[0] + flash_alpha),
            min(255, self.HUMAN_PRIMARY[1] + flash_alpha),
            min(255, self.HUMAN_PRIMARY[2] + flash_alpha)
        )
        
        self._draw_score_corner(self.width - 30, 30, "HUMAN", human_score, human_health,
                               human_bg_color, self.HUMAN_DARK, "right")
        
        # --- TOP-CENTER: TIMER (minimal) ---
        time_remaining = game_state.get('time_remaining_s', 0)
        mins = int(time_remaining // 60)
        secs = int(time_remaining % 60)
        
        timer_text = f"{mins:02d}:{secs:02d}"
        timer_surface = self.font_medium.render(timer_text, True, self.TEXT_DARK)
        timer_rect = timer_surface.get_rect(midtop=(self.width // 2, 30))
        
        # Timer background
        bg_rect = timer_rect.inflate(40, 20)
        pygame.draw.rect(self.screen, self.TEXT_LIGHT, bg_rect, border_radius=10)
        pygame.draw.rect(self.screen, self.TEXT_GRAY, bg_rect, 2, border_radius=10)
        
        self.screen.blit(timer_surface, timer_rect)
        
        # --- BOTTOM-LEFT: Match info ---
        status = game_state.get('ai_state', 'IDLE')
        info_text = f"AI: {status}"
        info_surface = self.font_small.render(info_text, True, self.TEXT_GRAY)
        self.screen.blit(info_surface, (30, self.height - 60))
        
        # --- BOTTOM-RIGHT: Debug hint ---
        if self.show_debug_path:
            debug_text = "[D] Hide Path"
            debug_color = self.ACCENT_GREEN
        else:
            debug_text = "[D] Show AI Path"
            debug_color = self.TEXT_GRAY
        
        debug_surface = self.font_small.render(debug_text, True, debug_color)
        debug_rect = debug_surface.get_rect(bottomright=(self.width - 30, self.height - 30))
        self.screen.blit(debug_surface, debug_rect)
    
    def _draw_score_corner(self, x: int, y: int, label: str, score: int, health: int,
                          bg_color: Tuple, border_color: Tuple, align: str):
        """Draw score panel in corner."""
        # Panel dimensions
        panel_width = 240
        panel_height = 120
        
        # Calculate position based on alignment
        if align == "right":
            panel_x = x - panel_width
        else:
            panel_x = x
        
        panel_rect = pygame.Rect(panel_x, y, panel_width, panel_height)
        
        # Background with transparency
        s = pygame.Surface((panel_width, panel_height), pygame.SRCALPHA)
        pygame.draw.rect(s, (*bg_color, 200), s.get_rect(), border_radius=15)
        pygame.draw.rect(s, border_color, s.get_rect(), 3, border_radius=15)
        self.screen.blit(s, panel_rect.topleft)
        
        # Label
        label_surface = self.font_small.render(label, True, self.TEXT_LIGHT)
        self.screen.blit(label_surface, (panel_x + 15, y + 10))
        
        # Score (large)
        score_surface = self.font_large.render(str(score), True, self.TEXT_LIGHT)
        self.screen.blit(score_surface, (panel_x + 15, y + 40))
        
        # Hits taken
        health_text = f"Hits: {health}"
        health_surface = self.font_tiny.render(health_text, True, self.TEXT_LIGHT)
        self.screen.blit(health_surface, (panel_x + 15, y + 95))
    
    def _draw_debug_info(self, game_state: Dict):
        """Draw debug information (bottom-right, small)."""
        debug_lines = [
            f"Path waypoints: {len(self.ai_path)}",
            f"AI LOS: {game_state.get('ai_has_los', False)}",
            f"Fire request: {game_state.get('ai_fire_request', False)}"
        ]
        
        y_offset = self.height - 120
        for line in debug_lines:
            text = self.font_tiny.render(line, True, self.TEXT_GRAY)
            self.screen.blit(text, (self.width - 300, y_offset))
            y_offset += 20
    
    def _render_end_screen(self, game_state: Dict):
        """Render match end screen with results."""
        self.screen.fill(self.BG_DARK)
        
        # Winner announcement
        if self.winner == "AI":
            winner_text = "AI VICTORY"
            winner_color = self.AI_GLOW
        else:
            winner_text = "HUMAN VICTORY"
            winner_color = self.HUMAN_GLOW
        
        winner_surface = self.font_title.render(winner_text, True, winner_color)
        winner_rect = winner_surface.get_rect(center=(self.width // 2, self.height // 2 - 100))
        self.screen.blit(winner_surface, winner_rect)
        
        # Final scores
        ai_score = game_state.get('robot_4_hits_inflicted', 0)
        human_score = game_state.get('robot_5_hits_inflicted', 0)
        
        scores_text = f"{ai_score}  -  {human_score}"
        scores_surface = self.font_large.render(scores_text, True, self.TEXT_LIGHT)
        scores_rect = scores_surface.get_rect(center=(self.width // 2, self.height // 2))
        self.screen.blit(scores_surface, scores_rect)
        
        # Labels
        ai_label = self.font_medium.render("AI", True, self.AI_PRIMARY)
        human_label = self.font_medium.render("HUMAN", True, self.HUMAN_PRIMARY)
        
        self.screen.blit(ai_label, (self.width // 2 - 150, self.height // 2 + 60))
        self.screen.blit(human_label, (self.width // 2 + 50, self.height // 2 + 60))
        
        # Restart hint
        restart_text = "Press R to restart"
        restart_surface = self.font_small.render(restart_text, True, self.TEXT_GRAY)
        restart_rect = restart_surface.get_rect(center=(self.width // 2, self.height // 2 + 150))
        self.screen.blit(restart_surface, restart_rect)
    
    def cleanup(self):
        """Cleanup Pygame resources."""
        pygame.quit()



################################################################################
PATH: ./visualization/ui_hud.py
################################################################################
"""
UI HUD - Heads-Up Display

Gère affichage HUD (scores, timer, status).

Logs: [HUD] prefix
"""

import pygame
import time
from typing import Dict, Optional
from .colors import *


class UI_HUD:
    """
    Gère HUD du jeu.
    """
    
    def __init__(self, surface: pygame.Surface):
        """
        Initialize HUD.
        
        Args:
            surface: Surface Pygame
        """
        self.surface = surface
        self.width = surface.get_width()
        self.height = surface.get_height()
        
        # Fonts
        self.font_large = pygame.font.SysFont('Arial', 64, bold=True)
        self.font_medium = pygame.font.SysFont('Arial', 40)
        self.font_small = pygame.font.SysFont('Arial', 28)
        
    def draw_timer(self, time_remaining: float):
        """
        Dessine chronomètre.
        
        Args:
            time_remaining: Temps restant (secondes)
        """
        minutes = int(time_remaining // 60)
        seconds = int(time_remaining % 60)
        
        time_text = f"{minutes:02d}:{seconds:02d}"
        text = self.font_large.render(time_text, True, TIMER_COLOR)
        text_rect = text.get_rect(center=(self.width // 2, 60))
        
        # Fond
        bg_rect = text_rect.inflate(40, 20)
        pygame.draw.rect(self.surface, WHITE, bg_rect)
        pygame.draw.rect(self.surface, BLACK, bg_rect, 3)
        
        self.surface.blit(text, text_rect)
        
    def draw_scores(self, ai_hits: int, human_hits: int):
        """
        Dessine scores.
        
        Args:
            ai_hits: Hits IA
            human_hits: Hits humain
        """
        # Score IA (gauche)
        ai_text = f"IA: {ai_hits}"
        ai_surface = self.font_medium.render(ai_text, True, SCORE_AI_COLOR)
        
        bg_ai = pygame.Rect(40, 40, 150, 60)
        pygame.draw.rect(self.surface, WHITE, bg_ai)
        pygame.draw.rect(self.surface, SCORE_AI_COLOR, bg_ai, 3)
        self.surface.blit(ai_surface, (50, 50))
        
        # Score Humain (droite)
        human_text = f"HUMAIN: {human_hits}"
        human_surface = self.font_medium.render(human_text, True, SCORE_HUMAN_COLOR)
        
        bg_human = pygame.Rect(self.width - 250, 40, 210, 60)
        pygame.draw.rect(self.surface, WHITE, bg_human)
        pygame.draw.rect(self.surface, SCORE_HUMAN_COLOR, bg_human, 3)
        self.surface.blit(human_surface, (self.width - 240, 50))
        
    def draw_cooldown(self, robot_id: int, cooldown_remaining: float):
        """
        Dessine barre cooldown tir.
        
        Args:
            robot_id: 4 (IA) ou 5 (Humain)
            cooldown_remaining: Temps restant (secondes)
        """
        if cooldown_remaining <= 0:
            return
        
        # Position selon robot
        if robot_id == 4:  # IA
            x, y = 40, 120
            color = SCORE_AI_COLOR
        else:  # Humain
            x, y = self.width - 250, 120
            color = SCORE_HUMAN_COLOR
        
        # Barre fond
        bar_width = 200
        bar_height = 20
        pygame.draw.rect(self.surface, GRAY_LIGHT, (x, y, bar_width, bar_height))
        
        # Barre remplissage
        fill_width = int(bar_width * (cooldown_remaining / 5.0))  # Max 5s
        pygame.draw.rect(self.surface, color, (x, y, fill_width, bar_height))
        
        # Bordure
        pygame.draw.rect(self.surface, BLACK, (x, y, bar_width, bar_height), 2)
        
    def draw_game_over(self, winner: str):
        """
        Affiche écran fin de partie.
        
        Args:
            winner: "AI", "HUMAN", ou "DRAW"
        """
        # Fond semi-transparent
        overlay = pygame.Surface((self.width, self.height))
        overlay.set_alpha(180)
        overlay.fill(BLACK)
        self.surface.blit(overlay, (0, 0))
        
        # Texte vainqueur
        if winner == "DRAW":
            text = "ÉGALITÉ!"
            color = YELLOW
        elif winner == "AI":
            text = "VICTOIRE IA!"
            color = SCORE_AI_COLOR
        else:
            text = "VICTOIRE HUMAIN!"
            color = SCORE_HUMAN_COLOR
        
        winner_surface = self.font_large.render(text, True, color)
        winner_rect = winner_surface.get_rect(center=(self.width // 2, self.height // 2))
        self.surface.blit(winner_surface, winner_rect)
        
    def draw_status_message(self, message: str, 
                          position: Optional[tuple] = None):
        """
        Affiche message status.
        
        Args:
            message: Message à afficher
            position: Position (x,y) ou None pour centré bas
        """
        text = self.font_small.render(message, True, BLACK)
        
        if position is None:
            position = (self.width // 2 - text.get_width() // 2, 
                       self.height - 100)
        
        # Fond
        bg_rect = text.get_rect(topleft=position).inflate(20, 10)
        pygame.draw.rect(self.surface, WHITE, bg_rect)
        pygame.draw.rect(self.surface, BLACK, bg_rect, 2)
        
        self.surface.blit(text, position)



