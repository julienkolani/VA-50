################################################################################
PATH: ./component_test/ai_behavior_viewer.py
################################################################################
#!/usr/bin/env python3
import sys
import os
import yaml
import pygame
import numpy as np
import cv2
import time
from pathlib import Path

# Ajout du chemin racine pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from perception.camera.realsense_stream import RealSenseStream
from perception.camera.aruco_detector import ArucoDetector
from core.world.world_model import WorldModel
from core.world.unified_transform import load_calibration

# --- CONFIGURATION VISUELLE ---
C_BG = (5, 5, 15)
C_RETRAIT = (255, 50, 50)   # Rouge
C_ATTAQUE = (0, 255, 100)   # Vert
C_POURSUITE = (0, 150, 255) # Bleu
C_CYAN = (0, 255, 255)      # IA
C_WHITE = (255, 255, 255)

# --- PARAMÈTRES IA ---
ROBOT_AI_ID = 5
ROBOT_ENEMY_ID = 4
SEUIL_MENACE_DEG = 15.0     # Angle max pour considérer que l'ennemi nous vise
SEUIL_LOCK_TIR_DEG = 5.0    # Précision requise pour l'état de tir

class TankBehaviorTree:
    """Arbre de comportement tactique en Français."""
    def __init__(self, world_model):
        self.world = world_model
        self.etat = "INITIALISATION"

    def check_ligne_de_vue(self, p1, p2):
        """Vérifie si le trajet est libre d'obstacles."""
        steps = 20
        for i in range(1, steps):
            tx = p1[0] + (p2[0] - p1[0]) * (i / steps)
            ty = p1[1] + (p2[1] - p1[1]) * (i / steps)
            # Utilise la grille d'occupation pour valider la position
            if not self.world.is_position_valid(tx, ty):
                return False
        return True

    def est_vise_par_ennemi(self, ai_pose, en_pose):
        """Détermine si l'ennemi pointe son canon vers l'IA avec une ligne de vue claire."""
        # Vecteur Ennemi -> IA
        dx, dy = ai_pose[0] - en_pose[0], ai_pose[1] - en_pose[1]
        angle_vers_ia = np.arctan2(dy, dx)
        
        # Erreur entre l'orientation de l'ennemi et la position de l'IA
        erreur_angle = (en_pose[2] - angle_vers_ia + np.pi) % (2 * np.pi) - np.pi
        erreur_deg = abs(np.degrees(erreur_angle))
        
        # Condition : L'ennemi regarde vers nous ET pas d'obstacle entre nous
        if erreur_deg < SEUIL_MENACE_DEG:
            return self.check_ligne_de_vue(en_pose[:2], ai_pose[:2])
        return False

    def update(self, ai_pose, en_pose):
        """Décide de l'état selon la hiérarchie des priorités."""
        if ai_pose is None or en_pose is None:
            self.etat = "RECHERCHE DE CIBLES"
            return self.etat

        # 1. PRIORITÉ : SURVIE (Seulement si visé directement)
        if self.est_vise_par_ennemi(ai_pose, en_pose):
            if self.etat != "RETRAIT (MENACE DIRECTE)":
                print(f"[BT] ÉTAT : RETRAIT - L'ennemi a une ligne de tir claire !")
            self.etat = "RETRAIT (MENACE DIRECTE)"
            return self.etat

        # 2. PRIORITÉ : ATTAQUE (Si ligne de vue sur l'ennemi)
        if self.check_ligne_de_vue(ai_pose[:2], en_pose[:2]):
            dx, dy = en_pose[0] - ai_pose[0], en_pose[1] - ai_pose[1]
            angle_vers_en = np.arctan2(dy, dx)
            erreur_ia = (ai_pose[2] - angle_vers_en + np.pi) % (2 * np.pi) - np.pi
            
            if abs(np.degrees(erreur_ia)) < SEUIL_LOCK_TIR_DEG:
                self.etat = "ATTAQUE (VERROUILLÉ)"
            else:
                self.etat = "ATTAQUE (ALIGNEMENT)"
            return self.etat

        # 3. PRIORITÉ : POURSUITE (Par défaut)
        if self.etat != "POURSUITE":
            print(f"[BT] ÉTAT : POURSUITE - Ennemi hors de vue ou caché.")
        self.etat = "POURSUITE"
        return self.etat

def get_robot_pose_in_world(detection_data, transform_mgr):
    u, v = detection_data['center']
    theta_pix = detection_data['orientation']
    u_f = u + 20 * np.cos(theta_pix)
    v_f = v + 20 * np.sin(theta_pix)
    x, y = transform_mgr.camera_to_world(u, v)
    xf, yf = transform_mgr.camera_to_world(u_f, v_f)
    # Protection contre les sorties de grille
    x = np.clip(x, 0.01, transform_mgr.arena_width_m - 0.01)
    y = np.clip(y, 0.01, transform_mgr.arena_height_m - 0.01)
    return x, y, np.arctan2(yf - y, xf - x)

def main():
    config_dir = Path(__file__).parent.parent / 'config'
    transform_mgr = load_calibration(str(config_dir))
    proj_conf = {}
    if (config_dir / 'projector.yaml').exists():
        with open(config_dir / 'projector.yaml') as f: proj_conf = yaml.safe_load(f)

    # Hack SDL pour projecteur
    off_x = proj_conf.get('display', {}).get('monitor_offset_x', 1920)
    os.environ['SDL_VIDEO_WINDOW_POS'] = f"{off_x},0"

    pygame.init()
    screen = pygame.display.set_mode((1024, 768), pygame.NOFRAME)
    font_etat = pygame.font.SysFont("Impact", 50)
    font_debug = pygame.font.SysFont("Consolas", 18)

    camera = RealSenseStream(width=1280, height=720, fps=30)
    camera.start()
    aruco = ArucoDetector()
    
    # WorldModel pour l'accès aux obstacles
    world = WorldModel(transform_mgr.arena_width_m, transform_mgr.arena_height_m)
    world.generate_costmap() #
    
    brain = TankBehaviorTree(world)

    try:
        while True:
            screen.fill(C_BG)
            for event in pygame.event.get():
                if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE: return

            color_frame, _ = camera.get_frames()
            if color_frame is None: continue
            detections = aruco.detect(color_frame)

            ai_pose = en_pose = None
            if ROBOT_AI_ID in detections:
                ai_pose = get_robot_pose_in_world(detections[ROBOT_AI_ID], transform_mgr)
            if ROBOT_ENEMY_ID in detections:
                en_pose = get_robot_pose_in_world(detections[ROBOT_ENEMY_ID], transform_mgr)

            # Mise à jour de l'intelligence
            mode = brain.update(ai_pose, en_pose)

            # --- RENDU VISUEL ---
            if ai_pose and en_pose:
                ai_px = transform_mgr.world_to_projector(ai_pose[0], ai_pose[1])
                en_px = transform_mgr.world_to_projector(en_pose[0], en_pose[1])
                
                # Couleur dynamique
                color = C_POURSUITE
                if "RETRAIT" in mode: color = C_RETRAIT
                if "ATTAQUE" in mode: color = C_ATTAQUE

                # Dessin du laser de visée
                pygame.draw.line(screen, color, ai_px, en_px, 2)
                
                # Robots
                pygame.draw.circle(screen, C_CYAN, ai_px, 15, 2)
                pygame.draw.circle(screen, C_RETRAIT, en_px, 15, 2)

            # Affichage de l'état central
            txt_surf = font_etat.render(f"MODE IA : {mode}", True, C_WHITE)
            screen.blit(txt_surf, (screen.get_width()//2 - txt_surf.get_width()//2, 50))

            # Debug logs en bas de l'écran
            if ai_pose and en_pose:
                dist = np.hypot(ai_pose[0]-en_pose[0], ai_pose[1]-en_pose[1])
                debug_txt = [
                    f"IA Pose: x={ai_pose[0]:.2f} y={ai_pose[1]:.2f} th={np.degrees(ai_pose[2]):.0f}°",
                    f"EN Pose: x={en_pose[0]:.2f} y={en_pose[1]:.2f} th={np.degrees(en_pose[2]):.0f}°",
                    f"Distance: {dist:.2f}m | Menace: {'OUI' if brain.est_vise_par_ennemi(ai_pose, en_pose) else 'NON'}"
                ]
                y_y = 650
                for line in debug_txt:
                    s = font_debug.render(line, True, C_WHITE)
                    screen.blit(s, (30, y_y))
                    y_y += 25

            pygame.display.flip()
            pygame.time.Clock().tick(30)

    except KeyboardInterrupt: pass
    finally:
        camera.stop()
        pygame.quit()

if __name__ == '__main__': main()

################################################################################
PATH: ./component_test/bt_latest.py
################################################################################
#!/usr/bin/env python3
import sys
import os
import yaml
import pygame
import numpy as np
import cv2
import time
from pathlib import Path
from enum import Enum

# Ajout du chemin racine pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from perception.camera.realsense_stream import RealSenseStream
from perception.camera.aruco_detector import ArucoDetector
from core.world.world_model import WorldModel
from core.world.unified_transform import load_calibration

# --- CONFIGURATION VISUELLE ---
C_BG = (5, 5, 15)
C_RETRAIT = (255, 50, 50)   # Rouge
C_ATTAQUE = (0, 255, 100)   # Vert
C_POURSUITE = (0, 150, 255) # Bleu
C_CYAN = (0, 255, 255)      # IA
C_WHITE = (255, 255, 255)

# --- PARAMÈTRES IA (par défaut, surchargeables via YAML) ---
ROBOT_AI_ID = 5
ROBOT_ENEMY_ID = 4
SEUIL_MENACE_DEG = 15.0     # Angle max pour considérer que l'ennemi nous vise
SEUIL_LOCK_TIR_DEG = 5.0    # Précision requise pour l'état de tir

# --- CONFIG "STABILITÉ D'ÉTATS" (NOUVEAU) ---
# But: empêcher les oscillations rapides d'état (debounce/dwell time)
DEFAULT_BT_CONFIG = {
    "min_state_duration_s": 0.40,  # durée minimale avant d'autoriser un changement d'état
    "min_state_duration_by_state_s": {
        # Durées par état (optionnel) : surcharge le global si présent
        "ATTAQUE (ALIGNEMENT)": 0.35,
        "ATTAQUE (VERROUILLÉ)": 0.25,
        "POURSUITE": 0.50,
        "RECHERCHE DE CIBLES": 0.20,
        "INITIALISATION": 0.10,
    },
    # Les états critiques peuvent casser le verrou temporel
    "force_switch_states": [
        "RETRAIT (MENACE DIRECTE)"
    ],
    # Logger les transitions
    "log_transitions": True
}

# --- NOEUDS DE L'ARBRE DE COMPORTEMENT ---
class BTStatus(Enum):
    SUCCESS = 0
    FAILURE = 1
    RUNNING = 2

class BTNode:
    def __init__(self, name):
        self.name = name

    def tick(self, ai_pose, en_pose, world):
        raise NotImplementedError

class SequenceNode(BTNode):
    def __init__(self, name, children):
        super().__init__(name)
        self.children = children

    def tick(self, ai_pose, en_pose, world):
        for child in self.children:
            status = child.tick(ai_pose, en_pose, world)
            if status != BTStatus.SUCCESS:
                return status
        return BTStatus.SUCCESS

class SelectorNode(BTNode):
    def __init__(self, name, children):
        super().__init__(name)
        self.children = children

    def tick(self, ai_pose, en_pose, world):
        for child in self.children:
            status = child.tick(ai_pose, en_pose, world)
            if status != BTStatus.FAILURE:
                return status
        return BTStatus.FAILURE

class ConditionNode(BTNode):
    def __init__(self, name, condition_func):
        super().__init__(name)
        self.condition_func = condition_func

    def tick(self, ai_pose, en_pose, world):
        if self.condition_func(ai_pose, en_pose, world):
            return BTStatus.SUCCESS
        return BTStatus.FAILURE

class ActionNode(BTNode):
    def __init__(self, name, action_func):
        super().__init__(name)
        self.action_func = action_func

    def tick(self, ai_pose, en_pose, world):
        self.action_func(ai_pose, en_pose, world)
        return BTStatus.SUCCESS

# --- ARBRE DE COMPORTEMENT ---
class TankBehaviorTree:
    def __init__(self, world_model, config=None):
        self.world = world_model
        self.etat = "INITIALISATION"

        # Config stabilité (dwell time)
        cfg = dict(DEFAULT_BT_CONFIG)
        if isinstance(config, dict):
            # merge shallow + merge dictionnaire nested si fourni
            cfg.update(config)
            if "min_state_duration_by_state_s" in config:
                merged = dict(DEFAULT_BT_CONFIG.get("min_state_duration_by_state_s", {}))
                merged.update(config["min_state_duration_by_state_s"])
                cfg["min_state_duration_by_state_s"] = merged

        self.bt_cfg = cfg
        self._last_state_change_ts = time.time()
        self._pending_state = None  # optionnel (si tu veux tracer)
        self._build_tree()

    # --- utilitaires stabilité d'état ---
    def _min_duration_for_state(self, state_name: str) -> float:
        by_state = self.bt_cfg.get("min_state_duration_by_state_s", {}) or {}
        if state_name in by_state:
            return float(by_state[state_name])
        return float(self.bt_cfg.get("min_state_duration_s", 0.4))

    def _can_switch(self, new_state: str) -> bool:
        now = time.time()
        dt = now - self._last_state_change_ts
        required = self._min_duration_for_state(self.etat)
        return dt >= required

    def _set_state(self, new_state: str, reason: str = ""):
        """
        Setter unique d'état avec verrou temporel.
        - Force le switch si état critique (force_switch_states)
        - Sinon, respecte min_state_duration (debounce)
        """
        if new_state == self.etat:
            return

        force_states = set(self.bt_cfg.get("force_switch_states", []) or [])
        force = (new_state in force_states)

        if force or self._can_switch(new_state):
            prev = self.etat
            self.etat = new_state
            self._last_state_change_ts = time.time()
            if self.bt_cfg.get("log_transitions", True):
                extra = f" | {reason}" if reason else ""
                print(f"[BT] TRANSITION: {prev} -> {new_state}{extra}")
        else:
            # On refuse le changement pour éviter les oscillations
            self._pending_state = new_state
            # Log léger et non-spam (optionnel)
            # print(f"[BT] HOLD: {self.etat} (pending {new_state})")

    def check_ligne_de_vue(self, p1, p2):
        """Vérifie si le trajet est libre d'obstacles."""
        steps = 20
        for i in range(1, steps):
            tx = p1[0] + (p2[0] - p1[0]) * (i / steps)
            ty = p1[1] + (p2[1] - p1[1]) * (i / steps)
            if not self.world.is_position_valid(tx, ty):
                return False
        return True

    def est_vise_par_ennemi(self, ai_pose, en_pose):
        """Détermine si l'ennemi pointe son canon vers l'IA avec une ligne de vue claire."""
        if ai_pose is None or en_pose is None:
            return False
        dx, dy = ai_pose[0] - en_pose[0], ai_pose[1] - en_pose[1]
        angle_vers_ia = np.arctan2(dy, dx)
        erreur_angle = (en_pose[2] - angle_vers_ia + np.pi) % (2 * np.pi) - np.pi
        erreur_deg = abs(np.degrees(erreur_angle))
        if erreur_deg < SEUIL_MENACE_DEG:
            return self.check_ligne_de_vue(en_pose[:2], ai_pose[:2])
        return False

    def _build_tree(self):
        """Construit l'arbre de comportement."""
        # Actions (utilisent _set_state pour éviter les changements trop rapides)
        def set_retrait(ai_pose, en_pose, world):
            self._set_state("RETRAIT (MENACE DIRECTE)", reason="Menace directe")

        def set_attaque_lock(ai_pose, en_pose, world):
            self._set_state("ATTAQUE (VERROUILLÉ)", reason="LOS + verrouillage")

        def set_attaque_align(ai_pose, en_pose, world):
            self._set_state("ATTAQUE (ALIGNEMENT)", reason="LOS sans verrouillage")

        def set_poursuite(ai_pose, en_pose, world):
            self._set_state("POURSUITE", reason="Ennemi hors LOS")

        # Conditions
        def menace_directe(ai_pose, en_pose, world):
            return self.est_vise_par_ennemi(ai_pose, en_pose)

        def ligne_de_vue(ai_pose, en_pose, world):
            if ai_pose is None or en_pose is None:
                return False
            return self.check_ligne_de_vue(ai_pose[:2], en_pose[:2])

        def verrouille(ai_pose, en_pose, world):
            if not ligne_de_vue(ai_pose, en_pose, world):
                return False
            dx, dy = en_pose[0] - ai_pose[0], en_pose[1] - ai_pose[1]
            angle_vers_en = np.arctan2(dy, dx)
            erreur_ia = (ai_pose[2] - angle_vers_en + np.pi) % (2 * np.pi) - np.pi
            return abs(np.degrees(erreur_ia)) < SEUIL_LOCK_TIR_DEG

        # Construction de l'arbre
        self.root = SelectorNode("Root", [
            # Priorité 1 : Retrait si menace directe
            SequenceNode("Retrait", [
                ConditionNode("Menace directe ?", menace_directe),
                ActionNode("Passer en retrait", set_retrait),
            ]),
            # Priorité 2 : Attaque si ligne de vue
            SelectorNode("Attaque", [
                SequenceNode("Attaque verrouillée", [
                    ConditionNode("Ligne de vue ?", ligne_de_vue),
                    ConditionNode("Verrouillé ?", verrouille),
                    ActionNode("Attaque verrouillée", set_attaque_lock),
                ]),
                SequenceNode("Attaque alignement", [
                    ConditionNode("Ligne de vue ?", ligne_de_vue),
                    ActionNode("Attaque alignement", set_attaque_align),
                ]),
            ]),
            # Priorité 3 : Poursuite par défaut
            ActionNode("Poursuite", set_poursuite),
        ])

    def update(self, ai_pose, en_pose):
        """Met à jour l'état en exécutant l'arbre de comportement (avec dwell time)."""
        if ai_pose is None or en_pose is None:
            # Ici aussi on passe par _set_state pour éviter oscillation "SEARCH <-> POURSUITE"
            self._set_state("RECHERCHE DE CIBLES", reason="Pose manquante")
            return self.etat

        self.root.tick(ai_pose, en_pose, self.world)
        return self.etat

# --- FONCTIONS AUXILIAIRES ---
def get_robot_pose_in_world(detection_data, transform_mgr):
    u, v = detection_data['center']
    theta_pix = detection_data['orientation']
    u_f = u + 20 * np.cos(theta_pix)
    v_f = v + 20 * np.sin(theta_pix)
    x, y = transform_mgr.camera_to_world(u, v)
    xf, yf = transform_mgr.camera_to_world(u_f, v_f)
    x = np.clip(x, 0.01, transform_mgr.arena_width_m - 0.01)
    y = np.clip(y, 0.01, transform_mgr.arena_height_m - 0.01)
    return x, y, np.arctan2(yf - y, xf - x)

def _load_bt_config(config_dir: Path) -> dict:
    """
    Charge une config IA optionnelle depuis config/ia_bt.yaml (ou renvoie defaults).
    Format attendu (exemple):
      bt:
        min_state_duration_s: 0.4
        min_state_duration_by_state_s:
          "POURSUITE": 0.6
        force_switch_states:
          - "RETRAIT (MENACE DIRECTE)"
        log_transitions: true
      params:
        seuil_menace_deg: 15.0
        seuil_lock_tir_deg: 5.0
    """
    path = config_dir / "ia_bt.yaml"
    if not path.exists():
        return {}

    try:
        with open(path, "r") as f:
            data = yaml.safe_load(f) or {}
            return data
    except Exception as e:
        print(f"[BT] WARNING: impossible de lire ia_bt.yaml: {e}")
        return {}

def main():
    global SEUIL_MENACE_DEG, SEUIL_LOCK_TIR_DEG, ROBOT_AI_ID, ROBOT_ENEMY_ID

    config_dir = Path(__file__).parent.parent / 'config'
    transform_mgr = load_calibration(str(config_dir))

    # Config projecteur
    proj_conf = {}
    if (config_dir / 'projector.yaml').exists():
        with open(config_dir / 'projector.yaml') as f:
            proj_conf = yaml.safe_load(f) or {}

    # Config IA BT (optionnelle)
    ia_conf = _load_bt_config(config_dir)
    bt_conf = ia_conf.get("bt", {}) if isinstance(ia_conf, dict) else {}
    params = ia_conf.get("params", {}) if isinstance(ia_conf, dict) else {}

    # Surcharge paramètres
    if "robot_ai_id" in params:
        ROBOT_AI_ID = int(params["robot_ai_id"])
    if "robot_enemy_id" in params:
        ROBOT_ENEMY_ID = int(params["robot_enemy_id"])
    if "seuil_menace_deg" in params:
        SEUIL_MENACE_DEG = float(params["seuil_menace_deg"])
    if "seuil_lock_tir_deg" in params:
        SEUIL_LOCK_TIR_DEG = float(params["seuil_lock_tir_deg"])

    # Hack SDL pour projecteur
    off_x = proj_conf.get('display', {}).get('monitor_offset_x', 1920)
    os.environ['SDL_VIDEO_WINDOW_POS'] = f"{off_x},0"

    pygame.init()
    screen = pygame.display.set_mode((1024, 768), pygame.NOFRAME)
    font_etat = pygame.font.SysFont("Impact", 50)
    font_debug = pygame.font.SysFont("Consolas", 18)

    camera = RealSenseStream(width=1280, height=720, fps=30)
    camera.start()
    aruco = ArucoDetector()

    world = WorldModel(transform_mgr.arena_width_m, transform_mgr.arena_height_m)
    world.generate_costmap()

    brain = TankBehaviorTree(world, config=bt_conf)

    try:
        while True:
            screen.fill(C_BG)
            for event in pygame.event.get():
                if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE:
                    return

            color_frame, _ = camera.get_frames()
            if color_frame is None:
                continue
            detections = aruco.detect(color_frame)

            ai_pose = en_pose = None
            if ROBOT_AI_ID in detections:
                ai_pose = get_robot_pose_in_world(detections[ROBOT_AI_ID], transform_mgr)
            if ROBOT_ENEMY_ID in detections:
                en_pose = get_robot_pose_in_world(detections[ROBOT_ENEMY_ID], transform_mgr)

            mode = brain.update(ai_pose, en_pose)

            if ai_pose and en_pose:
                ai_px = transform_mgr.world_to_projector(ai_pose[0], ai_pose[1])
                en_px = transform_mgr.world_to_projector(en_pose[0], en_pose[1])

                color = C_POURSUITE
                if "RETRAIT" in mode:
                    color = C_RETRAIT
                if "ATTAQUE" in mode:
                    color = C_ATTAQUE

                pygame.draw.line(screen, color, ai_px, en_px, 2)
                pygame.draw.circle(screen, C_CYAN, ai_px, 15, 2)
                pygame.draw.circle(screen, C_RETRAIT, en_px, 15, 2)

            txt_surf = font_etat.render(f"MODE IA : {mode}", True, C_WHITE)
            screen.blit(txt_surf, (screen.get_width()//2 - txt_surf.get_width()//2, 50))

            if ai_pose and en_pose:
                dist = np.hypot(ai_pose[0]-en_pose[0], ai_pose[1]-en_pose[1])
                menace = "OUI" if brain.est_vise_par_ennemi(ai_pose, en_pose) else "NON"
                debug_txt = [
                    f"IA Pose: x={ai_pose[0]:.2f} y={ai_pose[1]:.2f} th={np.degrees(ai_pose[2]):.0f}°",
                    f"EN Pose: x={en_pose[0]:.2f} y={en_pose[1]:.2f} th={np.degrees(en_pose[2]):.0f}°",
                    f"Distance: {dist:.2f}m | Menace: {menace}",
                    f"Seuil menace: {SEUIL_MENACE_DEG:.1f}° | Seuil lock: {SEUIL_LOCK_TIR_DEG:.1f}°",
                ]
                y_y = 650
                for line in debug_txt:
                    s = font_debug.render(line, True, C_WHITE)
                    screen.blit(s, (30, y_y))
                    y_y += 25

            pygame.display.flip()
            pygame.time.Clock().tick(30)

    except KeyboardInterrupt:
        pass
    finally:
        camera.stop()
        pygame.quit()

if __name__ == '__main__':
    main()


################################################################################
PATH: ./component_test/go_to_point.py
################################################################################
#!/usr/bin/env python3
"""
Test Composant : Go To Point

Test du système de navigation complet :
- Détection ArUco (Robot AI ID=4, Enemy ID=5)
- Planification de chemin A* vers l'ennemi avec distance de sécurité
- Suivi de trajectoire avec TrajectoryFollower
- Contrôle robot via ROS Bridge
- Visualisation pygame : caméra + grille + robots + path

Mode: Boucle ouverte - calcule le chemin, l'affiche, envoie les commandes, attend.

Commandes :
    [ESC] : Arrêt d'urgence et quitter
"""

import sys
import os
import yaml
import pygame
import numpy as np
import cv2
import time
from pathlib import Path

# Ajout du chemin racine pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from perception.camera.realsense_stream import RealSenseStream
from perception.camera.aruco_detector import ArucoDetector
from core.world.world_model import WorldModel
from core.world.unified_transform import UnifiedTransform, load_calibration
from core.ia.planners.a_star import AStarPlanner
from core.control.trajectory_follower import TrajectoryFollower
from core.control.ros_bridge_client import ROSBridgeClient

# Couleurs
C_BG = (0, 0, 0)                 # Noir
C_GRID = (50, 50, 50)            # Gris foncé
C_MARKER = (0, 255, 255)         # Cyan (Robots)
C_PATH = (0, 200, 100)           # Vert (Chemin)
C_WAYPOINT = (255, 255, 0)       # Jaune (Waypoint actuel)
C_TEXT = (255, 255, 255)         # Blanc

# IDs des robots
ROBOT_AI_ID = 5      # Robot qui se déplace (IA)
ROBOT_ENEMY_ID = 4   # Robot cible (fixe - adversaire)

# Distance de sécurité (en mètres)
SAFETY_DISTANCE = 0.20  # 20cm - distance de tir optimale (réduite)

# Seuil de replanification (cercle de tolérance)
REPLAN_THRESHOLD_M = 0.08  # 8cm - ne replanifie que si l'ennemi bouge > 8cm


def load_projector_config():
    """Charge la config projecteur pour la fenêtre."""
    config_dir = Path(__file__).parent.parent / 'config'
    path = config_dir / 'projector.yaml'
    if path.exists():
        with open(path) as f:
            return yaml.safe_load(f)
    return None


def load_robot_config():
    """Charge la config robot pour les limites de vitesse."""
    config_dir = Path(__file__).parent.parent / 'config'
    path = config_dir / 'robot.yaml'
    if path.exists():
        with open(path) as f:
            return yaml.safe_load(f)
    return None


def get_robot_pose_in_world(detection_data, transform_mgr):
    """
    Calcule (x, y, theta) complet dans le référentiel Monde.
    
    Args:
        detection_data: Dict avec 'center' et 'orientation' (pixels)
        transform_mgr: UnifiedTransform instance
        
    Returns:
        (x, y, theta) en mètres et radians
    """
    # 1. Centre du robot (Pixels)
    u, v = detection_data['center']
    
    # 2. Point "Devant" le robot (Pixels)
    # On utilise + sin car transform_mgr gère déjà la conversion d'axes
    theta_pix = detection_data['orientation']
    dist_px = 20  # 20 pixels devant
    u_front = u + dist_px * np.cos(theta_pix)
    v_front = v + dist_px * np.sin(theta_pix)  # + car transform_mgr gère la conversion
    
    # 3. Projection dans le monde (Mètres)
    x, y = transform_mgr.camera_to_world(u, v)
    x_front, y_front = transform_mgr.camera_to_world(u_front, v_front)
    
    # 4. Calcul du vrai angle Monde
    theta_world = np.arctan2(y_front - y, x_front - x)
    
    return x, y, theta_world


def main():
    print("[TEST] ========== Go To Point Test ==========")
    
    # 1. Chargement Calibration
    config_dir = Path(__file__).parent.parent / 'config'
    transform_mgr = load_calibration(str(config_dir))
    
    if not transform_mgr.is_calibrated():
        print("\n[ERREUR CRITIQUE] Aucune calibration trouvée !")
        print("Veuillez lancer : python -m perception.calibration.standalone_wizard")
        return

    print(f"[TEST] Calibration chargée : {transform_mgr.pixels_per_meter:.2f} px/m")
    
    # 2. Configuration Affichage
    proj_conf = load_projector_config()
    robot_conf = load_robot_config()
    
    if proj_conf:
        win_w = proj_conf['projector']['width']
        win_h = proj_conf['projector']['height']
        off_x = proj_conf['display'].get('monitor_offset_x', 0)
        off_y = proj_conf['display'].get('monitor_offset_y', 0)
    else:
        win_w = transform_mgr.proj_width
        win_h = transform_mgr.proj_height
        off_x, off_y = 1920, 0

    # Force la position de la fenêtre
    os.environ['SDL_VIDEO_WINDOW_POS'] = f"{off_x},{off_y}"
    
    pygame.init()
    screen = pygame.display.set_mode((win_w, win_h), pygame.NOFRAME | pygame.DOUBLEBUF)
    pygame.display.set_caption("Go To Point Test")
    font = pygame.font.SysFont("Consolas", 24, bold=True)

    # 3. Caméra et Vision
    print("[TEST] Démarrage caméra...")
    camera = RealSenseStream(width=1280, height=720, fps=30)
    camera.start()
    time.sleep(1.0)  # Warmup

    K, D = camera.get_intrinsics_matrix()
    aruco = ArucoDetector()

    # 4. World Model
    world = WorldModel(
        arena_width_m=transform_mgr.arena_width_m,
        arena_height_m=transform_mgr.arena_height_m,
        grid_resolution_m=0.02,
        robot_radius_m=0.09,
        inflation_margin_m=0.05
    )
    world.generate_costmap()
    
    # 5. Planificateur A*
    planner = AStarPlanner(world.grid, heuristic='euclidean')
    
    # 6. Contrôleur de Trajectoire
    if robot_conf:
        control_conf = robot_conf.get('control', {}).copy()
        control_conf.update(robot_conf.get('velocity_limits', {}))
    else:
        control_conf = {
            'lookahead_distance_m': 0.15,    # Réduit pour laisser Pure Pursuit guider plus longtemps
            'k_velocity': 0.15,              # Vitesse linéaire
            'k_theta': 1.2,                  # Non utilisé (remplacé par state machine)
            'waypoint_threshold_m': 0.05,    # Seuil de waypoint atteint
            'max_linear_mps': 0.20,
            'max_angular_radps': 1.0         # Réduit à 1.0 pour stabilité ArUco
        }
    
    controller = TrajectoryFollower(control_conf)
    
    # 7. ROS Bridge
    if robot_conf:
        ros_host = robot_conf['ros_bridge']['host']
        ros_port = robot_conf['ros_bridge']['port']
    else:
        ros_host = '127.0.0.1'
        ros_port = 8765
    
    ros_bridge = ROSBridgeClient(host=ros_host, port=ros_port)
    robot_connected = ros_bridge.connect(max_retries=2, retry_interval=1.0)
    
    if not robot_connected:
        print("[TEST] ATTENTION: Robot non connecté, visualisation seule")
    
    # Variables d'état
    clock = pygame.time.Clock()
    running = True
    
    # État du test
    state = "DETECTING"  # DETECTING -> PLANNING -> NAVIGATING -> REACHED
    ai_pose = None
    enemy_pose = None
    path = None
    current_waypoint_idx = 0
    tick_counter = 0
    last_log_time = time.time()
    last_enemy_pos_for_path = None  # Pour détecter si ennemi a bougé
    
    print("\n[TEST] ========================================")
    print("[TEST] Positionnez les robots:")
    print(f"  - Robot AI (ID {ROBOT_AI_ID})")
    print(f"  - Robot Enemy (ID {ROBOT_ENEMY_ID})")
    print("[TEST] Appuyez sur ESC pour arrêter")
    print("[TEST] ========================================\n")
    
    try:
        while running:
            tick_counter += 1
            current_time = time.time()
            
            # Events
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    running = False
                elif event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_ESCAPE:
                        print("\n[TEST] ========================================")
                        print("[TEST] ARRÊT D'URGENCE DEMANDÉ !")
                        if robot_connected:
                            print("[TEST] Envoi commande stop au robot...")
                            ros_bridge.send_velocity_command(ROBOT_AI_ID, 0.0, 0.0)
                            print("[TEST] Robot arrêté.")
                        print("[TEST] ========================================\n")
                        running = False

            # 1. Acquisition Image
            color_frame, _ = camera.get_frames()
            if color_frame is None:
                continue

            # 2. Correction Distorsion
            if K is not None and D is not None:
                color_frame = cv2.undistort(color_frame, K, D)

            # 3. Détection
            detections = aruco.detect(color_frame)

            # 4. Mise à jour des poses
            ai_detected = ROBOT_AI_ID in detections
            enemy_detected = ROBOT_ENEMY_ID in detections
            
            if ai_detected:
                x, y, theta = get_robot_pose_in_world(detections[ROBOT_AI_ID], transform_mgr)
                ai_pose = (x, y, theta)
            
            if enemy_detected:
                x, y, theta = get_robot_pose_in_world(detections[ROBOT_ENEMY_ID], transform_mgr)
                enemy_pose = (x, y, theta)
            
            # Log détection toutes les 2 secondes en mode DETECTING
            if state == "DETECTING" and (current_time - last_log_time) > 2.0:
                if ai_detected and enemy_detected:
                    print(f"[DETECT] [OK] AI et Enemy détectés")
                elif ai_detected:
                    print(f"[DETECT] [OK] AI détecté | [NON] Enemy manquant")
                elif enemy_detected:
                    print(f"[DETECT] [NON] AI manquant | [OK] Enemy détecté")
                else:
                    print(f"[DETECT] [NON] Aucun robot détecté")
                last_log_time = current_time

            # 5. Machine à états du test
            if state == "DETECTING":
                if ai_pose is not None and enemy_pose is not None:
                    print("\n[TEST] ========================================")
                    print("[TEST] [OK] ROBOTS DÉTECTÉS !")
                    print(f"[TEST]   AI Pose    : ({ai_pose[0]:.3f}, {ai_pose[1]:.3f}, θ={np.degrees(ai_pose[2]):.1f}°)")
                    print(f"[TEST]   Enemy Pose : ({enemy_pose[0]:.3f}, {enemy_pose[1]:.3f}, θ={np.degrees(enemy_pose[2]):.1f}°)")
                    
                    dist_initial = np.sqrt((ai_pose[0] - enemy_pose[0])**2 + (ai_pose[1] - enemy_pose[1])**2)
                    print(f"[TEST]   Distance   : {dist_initial:.3f}m")
                    print("[TEST] ========================================\n")
                    state = "PLANNING"
            
            elif state == "PLANNING":
                if ai_pose and enemy_pose:
                    # Mémoriser où était l'ennemi au moment du calcul
                    last_enemy_pos_for_path = np.array([enemy_pose[0], enemy_pose[1]])
                    
                    print("[PLAN] Calcul de la cible avec distance de sécurité...")
                    
                    # Calculer le point cible avec distance de sécurité
                    dx = ai_pose[0] - enemy_pose[0]
                    dy = ai_pose[1] - enemy_pose[1]
                    dist = np.sqrt(dx**2 + dy**2)
                    
                    # Direction de l'enemy vers l'AI
                    if dist > 0.01:
                        ux = dx / dist
                        uy = dy / dist
                    else:
                        ux, uy = 1.0, 0.0
                    
                    # Point à distance de sécurité de l'enemy
                    target_x = enemy_pose[0] + ux * SAFETY_DISTANCE
                    target_y = enemy_pose[1] + uy * SAFETY_DISTANCE
                    target_pos = (target_x, target_y)
                    
                    print(f"[PLAN] Position cible : ({target_x:.3f}, {target_y:.3f})")
                    print(f"[PLAN] Distance à parcourir : {dist - SAFETY_DISTANCE:.3f}m")
                    print(f"[PLAN] Lancement A* pathfinding...")
                    
                    # Planifier le chemin
                    start_time = time.time()
                    full_path = planner.plan(ai_pose[:2], target_pos)
                    planning_time = time.time() - start_time
                    
                    if full_path and len(full_path) > 0:
                        # Downsampling : garder 1 point sur 4 pour fluidité (tous les ~8-10cm)
                        path = full_path[::4] if len(full_path) > 4 else full_path
                        # S'assurer que la cible finale est bien dans le path
                        if path[-1] != target_pos:
                            path.append(target_pos)
                        
                        print(f"[PLAN] [OK] Chemin trouvé en {planning_time*1000:.1f}ms")
                        print(f"[PLAN] Nombre de waypoints : {len(full_path)} → {len(path)} (downsampled)")
                        print(f"[PLAN] Premier waypoint : ({path[0][0]:.3f}, {path[0][1]:.3f})")
                        print(f"[PLAN] Dernier waypoint : ({path[-1][0]:.3f}, {path[-1][1]:.3f})")
                        current_waypoint_idx = 0
                        print("\n[NAV] Début de la navigation...\n")
                        state = "NAVIGATING"
                    else:
                        print("[PLAN] [NON] ATTENTION: Impossible de planifier le chemin !")
                        print("[PLAN] Utilisation waypoint direct (fallback)...")
                        path = [target_pos]
                        current_waypoint_idx = 0
                        print("\n[NAV] Début de la navigation (mode direct)...\n")
                        state = "NAVIGATING"
            
            elif state == "NAVIGATING":
                if ai_pose and path and enemy_pose:
                    # ======== BOUCLE FERMÉE : Vérifier si l'ennemi a bougé ========
                    if last_enemy_pos_for_path is not None:
                        curr_enemy_pos = np.array([enemy_pose[0], enemy_pose[1]])
                        enemy_movement = np.linalg.norm(curr_enemy_pos - last_enemy_pos_for_path)
                        
                        if enemy_movement > REPLAN_THRESHOLD_M:
                            print(f"\n[LOOP] [ATTENTION] L'adversaire a bougé de {enemy_movement*100:.1f}cm > {REPLAN_THRESHOLD_M*100:.0f}cm")
                            print("[LOOP] → Replanification du chemin...")
                            state = "PLANNING"
                            continue
                    
                    # Vérifier si destination atteinte (tous waypoints passés)
                    if current_waypoint_idx >= len(path):
                        print("\n[NAV] ========================================")
                        print("[NAV] [OK] TOUS LES WAYPOINTS ATTEINTS !")
                        if robot_connected:
                            print("[NAV] Arrêt du robot...")
                            ros_bridge.send_velocity_command(ROBOT_AI_ID, 0.0, 0.0)
                        
                        # Calcul distance finale
                        if enemy_pose:
                            final_dist = np.sqrt((ai_pose[0] - enemy_pose[0])**2 + (ai_pose[1] - enemy_pose[1])**2)
                            print(f"[NAV] Distance finale à l'ennemi : {final_dist:.3f}m")
                            print(f"[NAV] Écart par rapport à la cible : {abs(final_dist - SAFETY_DISTANCE)*100:.1f}cm")
                        print("[NAV] ========================================\n")
                        state = "REACHED"
                    else:
                        # Calculer distance au waypoint actuel
                        wp = path[current_waypoint_idx]
                        dist_to_wp = np.sqrt((ai_pose[0] - wp[0])**2 + (ai_pose[1] - wp[1])**2)
                        
                        # Si waypoint atteint, avancer
                        if dist_to_wp < 0.05:  # 5cm
                            print(f"[NAV] [OK] Waypoint {current_waypoint_idx + 1}/{len(path)} atteint ({wp[0]:.3f}, {wp[1]:.3f})")
                            current_waypoint_idx += 1
                            if current_waypoint_idx >= len(path):
                                print("\n[NAV] ========================================")
                                print("[NAV] [OK] TOUS LES WAYPOINTS ATTEINTS !")
                                if robot_connected:
                                    print("[NAV] Arrêt du robot...")
                                    ros_bridge.send_velocity_command(ROBOT_AI_ID, 0.0, 0.0)
                                
                                # Calcul distance finale
                                if enemy_pose:
                                    final_dist = np.sqrt((ai_pose[0] - enemy_pose[0])**2 + (ai_pose[1] - enemy_pose[1])**2)
                                    print(f"[NAV] Distance finale à l'ennemi : {final_dist:.3f}m")
                                    print(f"[NAV] Écart par rapport à la cible : {abs(final_dist - SAFETY_DISTANCE)*100:.1f}cm")
                                print("[NAV] ========================================\n")
                                state = "REACHED"
                        else:
                            # Calculer commandes de contrôle (CRITIQUE: passer le chemin futur)
                            future_path = path[current_waypoint_idx:]
                            
                            # ===== LOGS DÉTAILLÉS =====
                            # Position actuelle
                            print(f"\n[NAV] ========== Navigation Step ==========")
                            print(f"[NAV] Pose actuelle : x={ai_pose[0]:.3f}m, y={ai_pose[1]:.3f}m, θ={np.degrees(ai_pose[2]):.1f}°")
                            
                            # Waypoint actuel
                            print(f"[NAV] Waypoint actuel [{current_waypoint_idx+1}/{len(path)}] : x={wp[0]:.3f}m, y={wp[1]:.3f}m")
                            print(f"[NAV] Distance au waypoint : {dist_to_wp:.3f}m")
                            
                            # Afficher les 3 prochains waypoints
                            print(f"[NAV] Chemin restant ({len(future_path)} points) :")
                            for i, wp_preview in enumerate(future_path[:min(3, len(future_path))]):
                                print(f"[NAV]   WP[{current_waypoint_idx+i+1}]: ({wp_preview[0]:.3f}, {wp_preview[1]:.3f})")
                            if len(future_path) > 3:
                                print(f"[NAV]   ... et {len(future_path)-3} autres points")
                            
                            # Distance au goal final
                            final_goal = path[-1]
                            dist_to_goal = np.sqrt((ai_pose[0] - final_goal[0])**2 + (ai_pose[1] - final_goal[1])**2)
                            print(f"[NAV] Distance au GOAL final : {dist_to_goal:.3f}m")
                            
                            # Commandes de contrôle
                            v_cmd, omega_cmd = controller.compute_control(ai_pose, future_path)
                            print(f"[NAV] Commande : v={v_cmd:.3f} m/s, ω={omega_cmd:.3f} rad/s")
                            print(f"[NAV] ==========================================\n")
                            
                            # CONDITION D'ARRÊT ABSOLU : Si très proche de la cible finale
                            if enemy_pose:
                                dist_to_enemy = np.sqrt((ai_pose[0] - enemy_pose[0])**2 + (ai_pose[1] - enemy_pose[1])**2)
                                if dist_to_enemy < 0.05:  # Moins de 5cm de l'ennemi
                                    v_cmd = 0.0
                                    omega_cmd = 0.0
                                    print(f"[NAV] [OK] Distance cible atteinte ({dist_to_enemy*100:.1f}cm), arrêt absolu")
                            
                            # Log périodique des commandes (toutes les 1 secondes)
                            if tick_counter % 30 == 0:
                                print(f"[NAV] WP {current_waypoint_idx + 1}/{len(path)} | Pose: ({ai_pose[0]:.3f}, {ai_pose[1]:.3f}) | Dist: {dist_to_wp:.3f}m | v={v_cmd:.3f} ω={omega_cmd:.3f}")
                            
                            # Envoyer au robot
                            if robot_connected:
                                ros_bridge.send_velocity_command(ROBOT_AI_ID, v_cmd, omega_cmd)
            
            elif state == "REACHED":
                # Arrêt confirmé
                if robot_connected:
                    ros_bridge.send_velocity_command(ROBOT_AI_ID, 0.0, 0.0)
                
                # ======== BOUCLE FERMÉE : Surveiller si l'ennemi s'éloigne ========
                if enemy_pose and last_enemy_pos_for_path is not None:
                    curr_enemy_pos = np.array([enemy_pose[0], enemy_pose[1]])
                    enemy_movement = np.linalg.norm(curr_enemy_pos - last_enemy_pos_for_path)
                    
                    if enemy_movement > REPLAN_THRESHOLD_M:
                        print(f"\n[LOOP] [ATTENTION] L'adversaire s'est éloigné de {enemy_movement*100:.1f}cm")
                        print("[LOOP] → Nouvelle planification...")
                        state = "PLANNING"

            # 6. Rendu Pygame
            screen.fill(C_BG)

            # Dessin de la grille du maillage A* (lignes rouges)
            # Utilise la résolution du WorldModel pour afficher les cellules exactes
            grid_step_m = world.grid.resolution  # 0.02m (2cm) - résolution du A*
            grid_color = (255, 0, 0)  # Rouge pour bien visualiser
            
            # Lignes verticales
            for x in np.arange(0, world.arena_width, grid_step_m):
                start_px = transform_mgr.world_to_projector(x, 0)
                end_px = transform_mgr.world_to_projector(x, world.arena_height)
                pygame.draw.line(screen, grid_color, start_px, end_px, 1)

            # Lignes horizontales
            for y in np.arange(0, world.arena_height, grid_step_m):
                start_px = transform_mgr.world_to_projector(0, y)
                end_px = transform_mgr.world_to_projector(world.arena_width, y)
                pygame.draw.line(screen, grid_color, start_px, end_px, 1)

            # Dessin du chemin
            if path and len(path) > 1:
                for i in range(len(path) - 1):
                    p1 = transform_mgr.world_to_projector(path[i][0], path[i][1])
                    p2 = transform_mgr.world_to_projector(path[i+1][0], path[i+1][1])
                    pygame.draw.line(screen, C_PATH, p1, p2, 3)
            
            # Dessin du waypoint actuel
            if path and state == "NAVIGATING" and current_waypoint_idx < len(path):
                wp = path[current_waypoint_idx]
                wp_px = transform_mgr.world_to_projector(wp[0], wp[1])
                pygame.draw.circle(screen, C_WAYPOINT, (int(wp_px[0]), int(wp_px[1])), 8, 3)

            # Dessin des robots détectés
            for mid, data in detections.items():
                if mid in [ROBOT_AI_ID, ROBOT_ENEMY_ID]:
                    u, v = data['center']
                    proj_x, proj_y = transform_mgr.camera_to_projector(u, v)
                    
                    center = (int(proj_x), int(proj_y))
                    pygame.draw.circle(screen, C_MARKER, center, 10, 2)
                    pygame.draw.line(screen, C_MARKER, (center[0]-15, center[1]), (center[0]+15, center[1]), 2)
                    pygame.draw.line(screen, C_MARKER, (center[0], center[1]-15), (center[0], center[1]+15), 2)
                    
                    label_text = "AI" if mid == ROBOT_AI_ID else "ENEMY"
                    lbl = font.render(f"{label_text}", True, C_TEXT)
                    screen.blit(lbl, (center[0]+15, center[1]-15))

            # UI Info
            status_text = f"État: {state}"
            if ai_pose and enemy_pose:
                dist = np.sqrt((ai_pose[0] - enemy_pose[0])**2 + (ai_pose[1] - enemy_pose[1])**2)
                status_text += f" | Distance: {dist:.2f}m"
            
            info_lines = [
                "[ESC] Arrêt",
                status_text,
                f"Safety Distance: {SAFETY_DISTANCE}m"
            ]
            
            y_offset = 20
            for line in info_lines:
                txt = font.render(line, True, (0, 255, 0))
                screen.blit(txt, (20, y_offset))
                y_offset += 30

            pygame.display.flip()
            clock.tick(30)  # 30 FPS

    except KeyboardInterrupt:
        print("[TEST] Interruption clavier")
    finally:
        print("\n[TEST] Nettoyage...")
        if robot_connected:
            ros_bridge.send_velocity_command(ROBOT_AI_ID, 0.0, 0.0)
            ros_bridge.disconnect()
        camera.stop()
        pygame.quit()
        
        # WAIT à la fin comme demandé
        print("[TEST] Test terminé. Appuyez sur Entrée pour fermer...")
        input()


if __name__ == '__main__':
    main()


################################################################################
PATH: ./component_test/point_and_tir.py
################################################################################
#!/usr/bin/env python3
import sys
import os
import yaml
import pygame
import numpy as np
import cv2
import time
from pathlib import Path

# Ajout du chemin racine pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from perception.camera.realsense_stream import RealSenseStream
from perception.camera.aruco_detector import ArucoDetector
from core.world.unified_transform import UnifiedTransform, load_calibration
from core.control.ros_bridge_client import ROSBridgeClient

# Configuration Couleurs & Design
C_BG = (5, 5, 15)
C_LASER = (255, 0, 0)
C_LOCK = (0, 255, 0)
C_HUD = (0, 255, 255)
C_GOLD = (255, 215, 0)

# Paramètres de Tir
ROBOT_AI_ID = 5
ROBOT_ENEMY_ID = 4
ALIGN_PRECISION_DEG = 3.0  # Seuil de tir
SHOOT_COOLDOWN = 1.5       # Temps entre deux tirs
ROTATION_GAIN = 1.5        # Force de la rotation sur place

class FloatingScore:
    """Animation +1 PT au centre de l'arène"""
    def __init__(self, pos_px):
        self.x, self.y = pos_px
        self.lifetime = 45 # frames
        self.alpha = 255
        self.font = pygame.font.SysFont("Impact", 42)

    def update(self):
        self.y -= 1.5
        self.alpha = max(0, self.alpha - 6)
        self.lifetime -= 1

    def draw(self, surface):
        s = self.font.render("+1 POINT", True, C_GOLD)
        s.set_alpha(self.alpha)
        surface.blit(s, (self.x - s.get_width()//2, self.y))

def get_robot_pose_in_world(detection_data, transform_mgr):
    u, v = detection_data['center']
    theta_pix = detection_data['orientation']
    dist_px = 20
    u_front = u + dist_px * np.cos(theta_pix)
    v_front = v + dist_px * np.sin(theta_pix)
    x, y = transform_mgr.camera_to_world(u, v)
    x_f, y_f = transform_mgr.camera_to_world(u_front, v_front)
    return x, y, np.arctan2(y_f - y, x_f - x)

def main():
    print("[SYSTEM] Démarrage du module Sniper...")
    
    # 1. Chargement Calibration & Config
    config_dir = Path(__file__).parent.parent / 'config'
    transform_mgr = load_calibration(str(config_dir))
    proj_conf = {}
    path_proj = config_dir / 'projector.yaml'
    if path_proj.exists():
        with open(path_proj) as f: proj_conf = yaml.safe_load(f)

    # 2. Hack SDL Position Fenêtre
    off_x = proj_conf.get('display', {}).get('monitor_offset_x', 1920)
    off_y = proj_conf.get('display', {}).get('monitor_offset_y', 0)
    os.environ['SDL_VIDEO_WINDOW_POS'] = f"{off_x},{off_y}"

    # 3. Initialisation Pygame
    pygame.init()
    win_w = proj_conf.get('projector', {}).get('width', 1024)
    win_h = proj_conf.get('projector', {}).get('height', 768)
    screen = pygame.display.set_mode((win_w, win_h), pygame.NOFRAME)
    font_hud = pygame.font.SysFont("Consolas", 24, bold=True)

    # 4. Vision & Hardware
    camera = RealSenseStream(width=1280, height=720, fps=30)
    camera.start()
    aruco = ArucoDetector()
    K, D = camera.get_intrinsics_matrix()
    
    ros_bridge = ROSBridgeClient(host='127.0.0.1', port=8765)
    robot_connected = ros_bridge.connect()

    # Variables de jeu
    score = 0
    last_shot_time = 0
    muzzle_flash_timer = 0
    floating_scores = []
    
    # Calcul du centre de l'arène en pixels (via homographie)
    center_world_x = transform_mgr.arena_width_m / 2
    center_world_y = transform_mgr.arena_height_m / 2
    arena_center_px = transform_mgr.world_to_projector(center_world_x, center_world_y)

    try:
        while True:
            current_time = time.time()
            screen.fill(C_BG)
            
            for event in pygame.event.get():
                if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE: return

            # Perception
            color_frame, _ = camera.get_frames()
            if color_frame is None: continue
            if K is not None: color_frame = cv2.undistort(color_frame, K, D)
            detections = aruco.detect(color_frame)

            ai_pose = enemy_pose = None
            if ROBOT_AI_ID in detections:
                ai_pose = get_robot_pose_in_world(detections[ROBOT_AI_ID], transform_mgr)
            if ROBOT_ENEMY_ID in detections:
                enemy_pose = get_robot_pose_in_world(detections[ROBOT_ENEMY_ID], transform_mgr)

            # --- LOGIQUE SNIPER ---
            is_locked = False
            error_deg = 0.0
            if ai_pose and enemy_pose:
                dx, dy = enemy_pose[0] - ai_pose[0], enemy_pose[1] - ai_pose[1]
                angle_to_enemy = np.arctan2(dy, dx)
                alpha = (angle_to_enemy - ai_pose[2] + np.pi) % (2 * np.pi) - np.pi
                error_deg = abs(np.degrees(alpha))

                # Commande de rotation pur
                w_cmd = np.clip(-(ROTATION_GAIN * alpha), -1.2, 1.2)

                if error_deg < ALIGN_PRECISION_DEG:
                    is_locked = True
                    w_cmd = 0 # Stabilisation
                    
                    if (current_time - last_shot_time) > SHOOT_COOLDOWN:
                        # --- FEU ! ---
                        score += 1
                        last_shot_time = current_time
                        muzzle_flash_timer = 6 # Durée du flash
                        floating_scores.append(FloatingScore(arena_center_px))
                        
                        if robot_connected:
                            # Utilisation de send_velocity_command pour simuler ou commande spécifique
                            print(f"[FIRE] Score: {score}")
                            try: ros_bridge.send_shoot_command(ROBOT_AI_ID)
                            except: pass

                if robot_connected:
                    ros_bridge.send_velocity_command(ROBOT_AI_ID, 0.0, w_cmd)

            # --- RENDU HUD & ANIMATIONS ---
            # 1. Grille de fond
            for x in np.arange(0, transform_mgr.arena_width_m, 0.1):
                p1 = transform_mgr.world_to_projector(x, 0)
                p2 = transform_mgr.world_to_projector(x, transform_mgr.arena_height_m)
                pygame.draw.line(screen, (20, 20, 40), p1, p2, 1)

            if ai_pose and enemy_pose:
                ai_px = transform_mgr.world_to_projector(ai_pose[0], ai_pose[1])
                en_px = transform_mgr.world_to_projector(enemy_pose[0], enemy_pose[1])
                color = C_LOCK if is_locked else C_LASER
                
                # Laser
                pygame.draw.line(screen, color, ai_px, en_px, 1 if not is_locked else 2)
                
                # Réticule Ennemi
                s = 40 + int(np.sin(time.time()*12)*5)
                pygame.draw.rect(screen, color, (en_px[0]-s//2, en_px[1]-s//2, s, s), 2)

                # Muzzle Flash (Animation de tir sur le robot AI)
                if muzzle_flash_timer > 0:
                    pygame.draw.circle(screen, (255, 255, 255), ai_px, 35)
                    pygame.draw.circle(screen, (255, 255, 0), ai_px, 20)
                    muzzle_flash_timer -= 1

            # 2. Floating Scores (Au centre de l'arène)
            for fs in floating_scores[:]:
                fs.update()
                fs.draw(screen)
                if fs.lifetime <= 0: floating_scores.remove(fs)

            # 3. HUD Infos
            hud_y = 30
            infos = [
                f"SCORE: {score:03d}",
                f"ERROR: {error_deg:.1f} DEG",
                f"STATUS: {'LOCKED' if is_locked else 'AIMING'}"
            ]
            for line in infos:
                img = font_hud.render(line, True, C_HUD)
                screen.blit(img, (30, hud_y))
                hud_y += 35

            pygame.display.flip()
            pygame.time.Clock().tick(30)

    except KeyboardInterrupt: pass
    finally:
        if robot_connected: ros_bridge.send_velocity_command(ROBOT_AI_ID, 0.0, 0.0)
        camera.stop()
        pygame.quit()

if __name__ == '__main__': main()

################################################################################
PATH: ./component_test/run_away.py
################################################################################
#!/usr/bin/env python3
import sys
import os
import yaml
import pygame
import numpy as np
import cv2
import time
from pathlib import Path

# Ajout du chemin racine pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from perception.camera.realsense_stream import RealSenseStream
from perception.camera.aruco_detector import ArucoDetector
from core.world.world_model import WorldModel
from core.world.unified_transform import UnifiedTransform, load_calibration
from core.ia.planners.a_star import AStarPlanner
from core.control.trajectory_follower import TrajectoryFollower
from core.control.ros_bridge_client import ROSBridgeClient

# --- CONFIGURATION DESIGN ---
C_BG = (5, 5, 15)
C_TACTICAL_GRID = (30, 30, 60)
C_SAFE_ZONE = (0, 255, 100)        # Vert (Refuge)
C_PATH = (0, 255, 150)             # Vert (Trajectoire)
C_DANGER = (255, 50, 50)           # Rouge (Ennemi)
C_CYAN = (0, 255, 255)             # IA
C_GOLD = (255, 215, 0)             # Animation
C_WHITE = (255, 255, 255)

# --- PARAMÈTRES TACTIQUES ---
ROBOT_AI_ID = 5
ROBOT_ENEMY_ID = 4
PANIC_DISTANCE = 0.60     # Rayon d'alerte (60cm)
REPLAN_THRESHOLD_M = 0.10  # Re-calculer si l'ennemi bouge de 10cm
SEGMENTS_X, SEGMENTS_Y = 5, 4 # Pour le choix de cible stratégique

class FloatingText:
    def __init__(self, pos_px, text):
        self.x, self.y = pos_px
        self.text = text
        self.lifetime = 45 
        self.alpha = 255
        self.font = pygame.font.SysFont("Impact", 52)

    def update(self):
        self.y -= 2
        self.alpha = max(0, self.alpha - 6)
        self.lifetime -= 1

    def draw(self, surface):
        s = self.font.render(self.text, True, C_GOLD)
        s.set_alpha(self.alpha)
        surface.blit(s, (self.x - s.get_width()//2, self.y))

def get_robot_pose_in_world(detection_data, transform_mgr):
    u, v = detection_data['center']
    theta_pix = detection_data['orientation']
    u_front = u + 20 * np.cos(theta_pix)
    v_front = v + 20 * np.sin(theta_pix)
    x, y = transform_mgr.camera_to_world(u, v)
    x_f, y_f = transform_mgr.camera_to_world(u_front, v_front)
    # Protection contre les bords de l'arène (évite les erreurs d'index négatif)
    x = np.clip(x, 0.01, transform_mgr.arena_width_m - 0.01)
    y = np.clip(y, 0.01, transform_mgr.arena_height_m - 0.01)
    return x, y, np.arctan2(y_f - y, x_f - x)

def main():
    print("[SYSTEM] Démarrage du module Tactical Flee (All Fixes Applied)...")
    
    # 1. Initialisation
    config_dir = Path(__file__).parent.parent / 'config'
    transform_mgr = load_calibration(str(config_dir))
    proj_conf = {}
    if (config_dir / 'projector.yaml').exists():
        with open(config_dir / 'projector.yaml') as f: proj_conf = yaml.safe_load(f)

    # Hack SDL pour projecteur
    off_x = proj_conf.get('display', {}).get('monitor_offset_x', 1920)
    os.environ['SDL_VIDEO_WINDOW_POS'] = f"{off_x},0"

    pygame.init()
    screen = pygame.display.set_mode((1024, 768), pygame.NOFRAME)
    font_hud = pygame.font.SysFont("Consolas", 20, bold=True)

    camera = RealSenseStream(width=1280, height=720, fps=30)
    camera.start()
    aruco = ArucoDetector()
    K, D = camera.get_intrinsics_matrix()

    # WorldModel avec grille fine de 2cm
    world = WorldModel(transform_mgr.arena_width_m, transform_mgr.arena_height_m)
    world.generate_costmap() 
    planner = AStarPlanner(world.grid)
    
    controller = TrajectoryFollower({
        'lookahead_distance_m': 0.15,
        'k_velocity': 0.7,
        'waypoint_threshold_m': 0.08,
        'max_linear_mps': 0.22,
        'max_angular_radps': 1.2
    })
    
    ros_bridge = ROSBridgeClient(host='127.0.0.1', port=8765)
    robot_connected = ros_bridge.connect()

    state = "SCANNING" 
    path = []
    current_waypoint_idx = 0
    flee_target = None
    last_enemy_pos_for_path = None
    floating_texts = []
    arena_center_px = transform_mgr.world_to_projector(world.arena_width/2, world.arena_height/2)

    try:
        while True:
            screen.fill(C_BG)
            current_time = time.time()
            for event in pygame.event.get():
                if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE: return

            color_frame, _ = camera.get_frames()
            if color_frame is None: continue
            if K is not None: color_frame = cv2.undistort(color_frame, K, D)
            detections = aruco.detect(color_frame)

            ai_pose = enemy_pose = None
            if ROBOT_AI_ID in detections: 
                ai_pose = get_robot_pose_in_world(detections[ROBOT_AI_ID], transform_mgr)
            if ROBOT_ENEMY_ID in detections: 
                enemy_pose = get_robot_pose_in_world(detections[ROBOT_ENEMY_ID], transform_mgr)

            # --- LOGIQUE TACTIQUE ---
            if ai_pose and enemy_pose:
                dist_enemy = np.hypot(ai_pose[0]-enemy_pose[0], ai_pose[1]-enemy_pose[1])

                if state == "FLEEING" and last_enemy_pos_for_path:
                    moved = np.hypot(enemy_pose[0]-last_enemy_pos_for_path[0], enemy_pose[1]-last_enemy_pos_for_path[1])
                    if moved > REPLAN_THRESHOLD_M: state = "SCANNING"

                if state == "SCANNING":
                    if dist_enemy < PANIC_DISTANCE:
                        # FIX: Utilisation du bon nom d'attribut 'robot_radius_m'
                        world.grid.update_dynamic_obstacles([enemy_pose], world.robot_radius_m)
                        last_enemy_pos_for_path = (enemy_pose[0], enemy_pose[1])
                        
                        max_d, best_zone = -1, None
                        sx, sy = world.arena_width / SEGMENTS_X, world.arena_height / SEGMENTS_Y
                        for ix in range(SEGMENTS_X):
                            for iy in range(SEGMENTS_Y):
                                zx, zy = (ix + 0.5) * sx, (iy + 0.5) * sy
                                # FIX: Utilisation de is_position_valid du WorldModel
                                if world.is_position_valid(zx, zy): 
                                    d = np.hypot(zx - enemy_pose[0], zy - enemy_pose[1])
                                    if d > max_d: max_d, best_zone = d, (zx, zy)
                        
                        if best_zone:
                            flee_target = best_zone
                            full_path = planner.plan(ai_pose[:2], flee_target)
                            if full_path:
                                path = full_path[::2] 
                                if path[-1] != flee_target: path.append(flee_target)
                                current_waypoint_idx = 0
                                state = "FLEEING"

                elif state == "FLEEING":
                    if current_waypoint_idx >= len(path):
                        floating_texts.append(FloatingText(arena_center_px, "+1 ESCAPE"))
                        state = "SCANNING"
                        if robot_connected: ros_bridge.send_velocity_command(ROBOT_AI_ID, 0, 0)
                        continue

                    wp = path[current_waypoint_idx]
                    if controller.is_waypoint_reached(ai_pose, wp):
                        current_waypoint_idx += 1
                        if current_waypoint_idx >= len(path):
                            floating_texts.append(FloatingText(arena_center_px, "+1 ESCAPE"))
                            state = "SCANNING"
                            if robot_connected: ros_bridge.send_velocity_command(ROBOT_AI_ID, 0, 0)
                            continue
                    
                    v, w = controller.compute_control(ai_pose, path[current_waypoint_idx:])
                    if robot_connected: ros_bridge.send_velocity_command(ROBOT_AI_ID, v, w)

            # --- RENDU VISUEL ---
            sx, sy = world.arena_width / SEGMENTS_X, world.arena_height / SEGMENTS_Y
            for ix in range(SEGMENTS_X):
                for iy in range(SEGMENTS_Y):
                    p1 = transform_mgr.world_to_projector(ix*sx, iy*sy)
                    p2 = transform_mgr.world_to_projector((ix+1)*sx, (iy+1)*sy)
                    pygame.draw.rect(screen, C_TACTICAL_GRID, (p1[0], p1[1], p2[0]-p1[0], p2[1]-p1[1]), 1)

            if path and state == "FLEEING":
                path_px = [transform_mgr.world_to_projector(p[0], p[1]) for p in path[current_waypoint_idx:]]
                if len(path_px) > 1:
                    pygame.draw.lines(screen, C_PATH, False, path_px, 3)

            if flee_target and state == "FLEEING":
                target_px = transform_mgr.world_to_projector(flee_target[0], flee_target[1])
                pygame.draw.circle(screen, C_SAFE_ZONE, target_px, 40, 2)

            if ai_pose: pygame.draw.circle(screen, C_CYAN, transform_mgr.world_to_projector(ai_pose[0], ai_pose[1]), 15, 2)
            if enemy_pose: pygame.draw.circle(screen, C_DANGER, transform_mgr.world_to_projector(enemy_pose[0], enemy_pose[1]), 15, 2)

            for ft in floating_texts[:]:
                ft.update()
                ft.draw(screen)
                if ft.lifetime <= 0: floating_texts.remove(ft)

            pygame.display.flip()
            pygame.time.Clock().tick(30)

    except KeyboardInterrupt: pass
    finally:
        if robot_connected: ros_bridge.send_velocity_command(ROBOT_AI_ID, 0, 0)
        camera.stop()
        pygame.quit()

if __name__ == '__main__': main()

################################################################################
PATH: ./config/arena.yaml
################################################################################
# =============================================================================
# Configuration Arène - Tank Arena
# =============================================================================
# Dimensions physiques de l'arène.
# Note: Les dimensions réelles sont calculées automatiquement par la calibration.
# Ces valeurs sont des fallback si la calibration n'est pas disponible.

arena:
  # Dimensions de l'arène en mètres (fallback si pas de calibration)
  # Ces valeurs seront remplacées par celles calculées par la calibration
  width_m: 2.50
  height_m: 1.80

# Grille d'occupation pour le pathfinding
grid:
  resolution_m: 0.02  # 2 cm par cellule

# Propriétés physiques du robot
robot:
  radius_m: 0.09              # Rayon physique Turtlebot Burger (~9cm)
  inflation_margin_m: 0.05    # Marge de sécurité supplémentaire

# Obstacles statiques (définis manuellement ou par calibration)
obstacles: []

# =============================================================================
# SECTION OBSOLÈTE - SUPPRIMÉE
# =============================================================================
# Les sections suivantes ont été supprimées car maintenant gérées par:
# - calibration.npz : Matrice H_CamToProj (homographie directe)
# - calibration_meta.json : Échelle (pixels_per_meter) et métadonnées
# - projector.yaml : Configuration du projecteur
#
# Anciennes clés supprimées:
# - display (déplacé vers projector.yaml)
# - projector (déplacé vers projector.yaml)  
# - transform.H_C2W (remplacé par H_CamToProj dans calibration.npz)
# - transform.scale (remplacé par pixels_per_meter dans calibration_meta.json)


################################################################################
PATH: ./config/calibration_meta.json
################################################################################
{
  "version": "2.1",
  "calibration_date": "2026-01-06T17:00:02.490167",
  "scale": {
    "pixels_per_meter": 671.7033081054688,
    "marker_size_m": 0.1
  },
  "projector": {
    "width": 1024,
    "height": 768,
    "margin": 50
  }
}

################################################################################
PATH: ./config/calibration.npz
################################################################################
PK-       !   H_CamToProj.npy                NUMPY v {'descr': '<f8', 'fortran_order': False, 'shape': (3, 3), }                                                          
04@^Sԕ?x6a`%>?g	@νp_n50?1'VB??PK-       ! 5!  camera_K.npy                NUMPY v {'descr': '<f4', 'fortran_order': False, 'shape': (3, 3), }                                                          
*eD    \Y"D    ieD_C          ?PK-       ! TS  camera_D.npy                NUMPY v {'descr': '<f4', 'fortran_order': False, 'shape': (5,), }                                                            
                    PK--       !                      H_CamToProj.npyPK--       ! 5!                 	  camera_K.npyPK--       ! TS                   camera_D.npyPK             

################################################################################
PATH: ./config/camera.yaml
################################################################################
# Configuration Caméra

realsense:
  # Résolution HD pour meilleure précision
  # L'homographie gère le changement d'échelle automatiquement
  width: 1280
  height: 720
  fps: 30 # IPS standard pour cette résolution
  enable_depth: false

aruco:
  dictionary: DICT_4X4_50 # Type de dictionnaire ArUco

  # IDs des marqueurs et leurs significations
  markers:
    projected_corners: [0, 1, 2, 3] # Coins virtuels pour calibration
    robot_ai: 4 # Marqueur robot IA
    robot_human: 5 # Marqueur robot Humain

  # Taille physique des marqueurs pour estimation d'échelle
  marker_size_m: 0.10 # Marqueurs de 10cm

kalman:
  # Covariance du bruit de processus (Modèle physique)
  # Augmenté pour permettre des changements de direction plus vifs
  process_noise:
    position: 0.1     # Était 0.01
    velocity: 1.0     # Était 0.1
    orientation: 0.1  # Était 0.01
    angular_velocity: 1.0 # Était 0.1

  # Covariance du bruit de mesure (Confiance capteur)
  # Diminué car la détection ArUco est très précise (low jitter)
  # Cela rapproche le comportement du "raw tracking" de show_grid
  measurement_noise:
    position: 0.001   # Était 0.05
    orientation: 0.01 # Était 0.1

  # Note: Le pas de temps (dt) est maintenant calculé dynamiquement
  # dans run_game.py au lieu d'être codé en dur ici.
  # Cela évite les glissements si le pipeline ralentit.


################################################################################
PATH: ./config/game.yaml
################################################################################
game:
  mode: "DEATHMATCH"
  duration: 180         # Durée de la partie en secondes (3 min)
  
  shooting:
    cooldown: 1.0       # Temps d'attente entre deux tirs (secondes)
    max_range: 10.0     # Portée maximale du tir (mètres)
    hit_radius: 0.15    # Rayon de la hitbox autour du robot (mètres) - UTILISÉ PAR L'ARBITRE

  mock:
    # Positions de départ pour le mode simulation
    ai_start_pos: [0.2, 0.5, 0.0]   # x%, y%, theta (relative to arena instructions in Manager)
    human_start_pos: [0.8, 0.5, 3.14] 


################################################################################
PATH: ./config/ia.yaml
################################################################################
# AI Configuration

behavior:
  # Threat assessment thresholds (adaptés à une arène de 2.5m x 1.8m)
  danger_distance_m: 0.3         # Ennemi trop proche (40cm)
  optimal_range_min_m: 0.4       # Portée min tir optimale (60cm)
  optimal_range_max_m: 1.8       # Portée max tir optimale (1.8m = largeur arène)

  # Behavior priorities
  survival_priority: true # Retreat overrides attack

  sniper:
    align_precision_deg: 3.0
    rotation_gain: 1.5

strategy:
  # Path planning
  heuristic: "euclidean" # 'euclidean', 'manhattan', 'diagonal'
  path_simplify: true # Apply Douglas-Peucker simplification
  simplify_epsilon_m: 0.05 # Simplification tolerance

  # Path smoothing
  smooth_path: true
  smooth_weight_data: 0.5
  smooth_weight_smooth: 0.3

  # Flanking behavior
  flank_angle_deg: 90 # Preferred flanking angle
  cover_preference: 0.8 # Preference for cover (0-1)

decision_rate:
  # How often to replan (in ticks)
  replan_interval: 20 # Replan every 20 ticks (~0.66s at 30fps)


################################################################################
PATH: ./config/projector.yaml
################################################################################
projector:
  width: 1024
  height: 768
  
display:
  monitor_offset_x: 1920
  borderless: true
  fullscreen: false

visuals:
  projectile_speed_px_s: 800.0  # Vitesse du projectile en pixels/seconde
  
  colors:
    background: [10, 10, 20]
    grid: [30, 30, 50]
    ai: [0, 150, 255]
    human: [255, 50, 0]
    hitbox: [255, 255, 0]
    
  hitbox:
    radius_m: 0.15 # Doit correspondre à game.yaml/shooting/hit_radius


################################################################################
PATH: ./config/robot.yaml
################################################################################
robot:
  radius_m: 0.09        # Rayon physique du robot
  inflation_margin_m: 0.05 # Marge de sécurité obstacle

  control:
    # --- PURE PURSUIT (Suivi de chemin) ---
    lookahead_distance: 0.15      # Distance de visée sur le chemin (m)
    k_v: 0.6                      # Gain proportionnel vitesse linéaire (v = k_v * sqrt(erreur))
    
    # --- LIMITES PHYSIQUES / SECURITÉ ---
    limits:
      max_linear_vel: 0.22        # Vitesse linéaire maximale (m/s)
      max_angular_vel: 1.2        # Vitesse angulaire maximale (rad/s)
      
    # --- ALIGNEMENT SUR PLACE (Rotation pure) ---
    alignment:
      threshold_angle: 45.0       # Erreur angulaire (deg) déclenchant la rotation sur place
      hysteresis_angle: 10.0      # Erreur angulaire (deg) pour sortir du mode rotation
      gain_omega: 1.2             # Gain proportionnel pour la rotation (omega = -gain * erreur)
      
    # --- APPROCHE FINALE (Arrivée au but) ---
    final_approach:
      activation_dist: 0.15       # Distance (m) ou nb de points restants pour activer l'approche fine
      gain_v: 0.4                 # Gain réduit pour l'approche douce
      gain_omega: 1.0             # Gain angulaire pour l'ajustement final
      
    # --- PRÉCISION ---
    epsilon:
      waypoint_threshold: 0.03    # Distance (m) pour considérer un waypoint comme atteint

  ros_bridge:
    host: "127.0.0.1"
    port: 8765


################################################################################
PATH: ./core/control/__init__.py
################################################################################


################################################################################
PATH: ./core/control/kinematics.py
################################################################################
"""
Cinématique - Modèle de mouvement robot

Cinématique de robot à entraînement différentiel pour Turtlebot Burger :
- Cinématique directe : (v, ω) -> (dx, dy, dθ)
- Cinématique inverse : contraintes de vitesse
- Modèle dynamique (simplifié)

Utilisé pour la simulation et la validation du contrôle.
"""

import numpy as np
from typing import Tuple


class DifferentialDriveKinematics:
    """
    Cinématique pour robot à entraînement différentiel (Turtlebot Burger).
    
    Paramètres du robot :
    - Empattement : distance entre les roues
    - Rayon des roues
    """
    
    def __init__(self, wheel_base: float = 0.16, wheel_radius: float = 0.033):
        """
        Initialise le modèle cinématique.
        
        Args:
            wheel_base: Distance entre les roues en mètres (Burger : 0.16m)
            wheel_radius: Rayon des roues en mètres (Burger : 0.033m)
        """
        self.wheel_base = wheel_base
        self.wheel_radius = wheel_radius
        
    def forward_kinematics(self, 
                          v: float, 
                          omega: float, 
                          current_pose: Tuple[float, float, float],
                          dt: float) -> Tuple[float, float, float]:
        """
        Calcule la nouvelle pose à partir des commandes de vitesse.
        
        Args:
            v: Vitesse linéaire en m/s
            omega: Vitesse angulaire en rad/s
            current_pose: (x, y, theta) pose actuelle
            dt: Pas de temps en secondes
            
        Returns:
            (x_new, y_new, theta_new) pose mise à jour
            
        Équations :
            dx = v * cos(θ) * dt
            dy = v * sin(θ) * dt
            dθ = ω * dt
        """
        x, y, theta = current_pose
        
        # Mise à jour de l'orientation en premier
        theta_new = theta + omega * dt
        
        # Theta moyen pour une intégration plus précise
        theta_avg = theta + 0.5 * omega * dt
        
        # Mise à jour de la position
        x_new = x + v * np.cos(theta_avg) * dt
        y_new = y + v * np.sin(theta_avg) * dt
        
        # Normalisation de theta entre [-pi, pi]
        theta_new = np.arctan2(np.sin(theta_new), np.cos(theta_new))
        
        return (x_new, y_new, theta_new)
    
    def validate_velocities(self, 
                           v: float, 
                           omega: float,
                           max_v: float = 0.22,
                           max_omega: float = 2.84) -> Tuple[float, float]:
        """
        Valide et limite les vitesses aux contraintes du robot.
        
        Args:
            v, omega: Vitesses désirées
            max_v: Vitesse linéaire maximum (Burger : 0.22 m/s)
            max_omega: Vitesse angulaire maximum (Burger : 2.84 rad/s)
            
        Returns:
            (v_clamped, omega_clamped)
        """
        v_clamped = np.clip(v, -max_v, max_v)
        omega_clamped = np.clip(omega, -max_omega, max_omega)
        
        return (v_clamped, omega_clamped)


################################################################################
PATH: ./core/control/ros_bridge_client.py
################################################################################
"""
Client Pont ROS - Communication avec le système ROS

Client WebSocket pour envoyer des commandes au pont ROS :
- Se connecte au serveur pont ROS via WebSocket
- Envoie des commandes de vitesse sur /cmd_vel
- Reçoit les acquittements
- Maintaint la santé de la connexion

Logs : [ROS] Commande envoyée : v=X, ω=Y
"""

import json
import time
import asyncio
import threading
from typing import Optional

try:
    import websockets
    from websockets.sync.client import connect as ws_connect
    HAS_WEBSOCKETS = True
except ImportError:
    HAS_WEBSOCKETS = False
    print("[ROS] Attention : websockets non installé, utilisation du repli")


class ROSBridgeClient:
    """
    Client pour communiquer avec le pont ROS via WebSocket.
    
    Envoie des commandes de vitesse au robot physique.
    """
    
    def __init__(self, host: str = 'localhost', port: int = 8765):
        """
        Initialise le client pont ROS.
        
        Args:
            host: Hôte du serveur pont ROS
            port: Port du serveur pont ROS (défaut 8765 pour WebSocket)
        """
        self.host = host
        self.port = port
        self.ws = None
        self.connected = False
        self.uri = f"ws://{host}:{port}"
        
    def connect(self, max_retries: int = 3, retry_interval: float = 2.0):
        """
        Établit la connexion WebSocket avec le pont ROS.
        
        Args:
            max_retries: Tentatives max
            retry_interval: Secondes entre les essais
        
        Logs :
            [ROS] Connecté au pont à host:port
            [ROS] Connexion échouée : erreur
        """
        if not HAS_WEBSOCKETS:
            print("[ROS] Bibliothèque WebSocket non disponible")
            return False
            
        attempt = 0
        while attempt < max_retries:
            attempt += 1
            try:
                # Disable keepalive pings (ping_interval=None) to avoid 1011 errors
                # if the bridge is slow or doesn't support pings correctly.
                self.ws = ws_connect(self.uri, open_timeout=5, ping_interval=None)
                self.connected = True
                
                # Lit le message de bienvenue
                try:
                    welcome = self.ws.recv(timeout=2)
                    print(f"[ROS] Connecté au pont à {self.host}:{self.port}")
                except:
                    print(f"[ROS] Connecté au pont à {self.host}:{self.port}")
                    
                return True
                
            except Exception as e:
                self.connected = False
                print(f"[ROS] Tentative de connexion {attempt} échouée : {e}")
                
                if attempt >= max_retries:
                    print("[ROS] Nb max d'essais atteint, exécution sans connexion ROS")
                    return False
                
                print(f"[ROS] Nouvel essai dans {retry_interval} secondes...")
                time.sleep(retry_interval)
        
        return False
    
    def disconnect(self):
        """Ferme la connexion WebSocket au pont ROS."""
        if self.ws:
            try:
                self.ws.close()
            except:
                pass
            self.connected = False
            print("[ROS] Déconnecté du pont")
    
    def send_velocity_command(self, 
                             robot_id: int, 
                             v: float, 
                             omega: float) -> bool:
        """
        Envoie une commande de vitesse au robot.
        
        Args:
            robot_id: 4 (IA) ou 5 (Humain)
            v: Vitesse linéaire en m/s
            omega: Vitesse angulaire en rad/s
            
        Returns:
            True si envoyé avec succès
            
        Format message (JSON) :
            {
                "type": "cmd_vel",
                "linear_x": v,
                "angular_z": omega,
                "timestamp": unix_time
            }
            
        Logs :
            [ROS] Robot4 cmd : v=0.15 m/s, ω=-0.30 rad/s
        """
        if not self.connected or not self.ws:
            # Essaie de reconnecter silencieusement
            if not self.connect(max_retries=1, retry_interval=0.5):
                return False
        
        # Format message correspondant aux attentes de safety_bridge.py
        message = {
            "type": "cmd_vel",
            "robot_id": robot_id,
            "linear_x": round(v, 4),
            "angular_z": round(omega, 4),
            "timestamp": time.time()
        }
        
        try:
            msg_json = json.dumps(message)
            self.ws.send(msg_json)
            
            # Log (moins verbeux - seulement toutes les 30 commandes ou significatives)
            if abs(v) > 0.01 or abs(omega) > 0.1:
                print(f"[ROS] Robot{robot_id} cmd : v={v:.2f} m/s, w={omega:.2f} rad/s")
            
            # Essaie de recevoir l'acquittement (non-bloquant)
            try:
                self.ws.recv(timeout=0.01)
            except:
                pass  # Ignore si pas de réponse
            
            return True
            
        except Exception as e:
            print(f"[ROS] Envoi échoué : {e}")
            self.connected = False
            return False
    
    def receive_feedback(self, timeout: float = 0.01) -> Optional[dict]:
        """
        Reçoit le retour du pont ROS (non-bloquant).
        
        Args:
            timeout: Timeout socket en secondes
            
        Returns:
            dict avec données, ou None
        """
        if not self.connected or not self.ws:
            return None
        
        try:
            data = self.ws.recv(timeout=timeout)
            if data:
                return json.loads(data)
        except:
            pass  # Pas de données disponibles
            
        return None
    
    def send_stop_command(self, robot_id: int):
        """
        Envoie un arrêt d'urgence au robot.
        
        Args:
            robot_id: Robot à arrêter
        """
        self.send_velocity_command(robot_id, 0.0, 0.0)


################################################################################
PATH: ./core/control/trajectory_follower.py
################################################################################
"""
Suiveur de Trajectoire - Contrôleur Pure Pursuit Optimisé

Implémente le suivi de trajectoire par courbure géométrique :
- Sélection du point lookahead sur le chemin
- Transformation monde → repère robot  
- Calcul de courbure : κ = 2*y_r / L_d²
- Commande angulaire : ω = v * κ
- Alignement sur LOOKAHEAD (pas sur goal final)

Prend un chemin (liste de waypoints) et la pose actuelle, sort (v, ω).
"""

import math
import numpy as np
from typing import Tuple, List


class TrajectoryFollower:
    """
    Contrôleur de suivi de trajectoire pour la navigation par points.
    
    Utilise l'algorithme Pure Pursuit avec alignement sur lookahead local.
    """
    
    def __init__(self, config):
        # État interne pour hystérésis d'alignement
        self._in_alignment_mode = False
        
        # Support pour ancienne (flat) et nouvelle (nested) structure
        # Nouvelle structure : config est le dict 'control' complet
        
        # 1. PARAMÈTRES DE BASE (Pure Pursuit)
        self.lookahead_distance = config.get('lookahead_distance', 0.15)
        self.k_v = config.get('k_v', 0.6)
        
        # 2. LIMITES
        limits = config.get('limits', {})
        # Retour aux clés plates si le dictionnaire 'limits' est manquant
        max_lin = limits.get('max_linear_vel', config.get('max_linear_vel', 0.22))
        max_ang = limits.get('max_angular_vel', config.get('max_angular_vel', 1.2))
        
        self.max_linear_vel = float(max_lin)
        self.max_angular_vel = float(max_ang)
        
        # 3. ALIGNEMENT (Rotation sur place)
        align = config.get('alignment', {})
        self.align_threshold = math.radians(align.get('threshold_angle', 45.0))
        self.align_hysteresis = math.radians(align.get('hysteresis_angle', 10.0))
        self.gain_omega_align = align.get('gain_omega', 1.2)
        
        # 4. APPROCHE FINALE (Douceur)
        final = config.get('final_approach', {})
        self.final_gain_v = final.get('gain_v', 0.4)
        self.final_gain_omega = final.get('gain_omega', 1.0)
        
        # 5. PRÉCISION
        eps = config.get('epsilon', {})
        self.waypoint_reached_threshold = eps.get('waypoint_threshold', config.get('waypoint_threshold', 0.07))
    
    def compute_control(self, current_pose: Tuple[float, float, float], waypoints: List[Tuple[float, float]]) -> Tuple[float, float]:
        """
        Calcule les commandes de contrôle (v, omega) basées sur l'état actuel.
        
        Implémente la Machine à États Hybride :
        1. Trouver le Point Lookahead.
        2. Calculer l'Erreur de Cap Locale (Alpha).
        3. Vérifier le Mode Alignement (Le Fix pour le bug 180°).
        4. Vérifier l'Approche Finale (Précision).
        5. Exécuter Pure Pursuit.
        """
        if not waypoints:
            return (0.0, 0.0)
        
        x, y, theta = current_pose
        
        # 1. Trouve le point cible (Lookahead)
        target_wp = self._get_lookahead_point(current_pose, waypoints)
        dx = target_wp[0] - x
        dy = target_wp[1] - y
        dist_to_target = math.sqrt(dx**2 + dy**2)
        
        # 2. Alpha calculé sur la CIBLE LOCALE (lookahead), pas le but final
        angle_to_target = math.atan2(dy, dx)
        alpha = (angle_to_target - theta + math.pi) % (2 * math.pi) - math.pi
        
        # 3. ÉTAT ALIGNEMENT (Hystérésis - Fix Historique pour le spin 180)
        # Si on tourne le dos à la cible : STOP et Rotation.
        if not self._in_alignment_mode and abs(alpha) > self.align_threshold:
            self._in_alignment_mode = True
        elif self._in_alignment_mode and abs(alpha) < self.align_hysteresis:
            self._in_alignment_mode = False
        
        if self._in_alignment_mode:
            v = 0.0 # Stop mouvement linéaire
            omega = -(self.gain_omega_align * alpha)  # Pivoter pour faire face
            omega_sat = np.clip(omega, -self.max_angular_vel, self.max_angular_vel)
            # Log pour débuguer l'hystérésis
            print(f"[CTRL] ALIGNEMENT | alpha={math.degrees(alpha):.1f}° → ω={omega_sat:.3f}")
            return (v, omega_sat)
        
        # 4. Approche finale (seulement s'il reste <= 2 points)
        if dist_to_target < self.lookahead_distance and len(waypoints) <= 2:
            v = self.final_gain_v * dist_to_target
            omega = -(self.final_gain_omega * alpha)
            v_sat = np.clip(v, 0, self.max_linear_vel)
            omega_sat = np.clip(omega, -self.max_angular_vel, self.max_angular_vel)
            print(f"[CTRL] FINAL_APPROACH | dist={dist_to_target:.3f}m alpha={math.degrees(alpha):.1f}° → v={v_sat:.3f} ω={omega_sat:.3f}")
            return (v_sat, omega_sat)
        
        # 5. Pure Pursuit classique
        v, omega = self._pure_pursuit(current_pose, target_wp)
        print(f"[CTRL] PURE_PURSUIT | dist={dist_to_target:.3f}m → v={v:.3f} ω={omega:.3f}")
        return (v, omega)
    
    def _get_lookahead_point(self, pose, waypoints):
        """
        Trouve le waypoint à la distance de visée.
        
        Cherche le point le plus loin qui est à la distance lookahead
        et devant le robot (x_r > 0).
        
        Args:
            pose: Pose actuelle du robot
            waypoints: Liste de waypoints
            
        Returns:
            (x, y) waypoint cible
        """
        x, y, theta = pose
        
        # On cherche le point le plus loin qui est à la distance lookahead
        for wp in reversed(waypoints):
            dist = math.sqrt((wp[0] - x)**2 + (wp[1] - y)**2)
            if dist >= self.lookahead_distance:
                # Vérifier s'il est devant (x_r > 0)
                dx, dy = wp[0] - x, wp[1] - y
                x_r = math.cos(theta) * dx + math.sin(theta) * dy
                if x_r > 0:
                    return wp
        
        # Sinon retourner le dernier waypoint
        return waypoints[-1]
    
    def _pure_pursuit(self, pose, target):
        """
        Contrôleur Pure Pursuit géométrique.
        
        Args:
            pose: (x, y, theta) - robot pose in world frame
            target: (x_t, y_t) - target waypoint in world frame
            
        Returns:
            (v, omega) - linear and angular velocities
        """
        x, y, theta = pose
        dx, dy = target[0] - x, target[1] - y
        
        # Transformation repère robot
        x_r = math.cos(theta) * dx + math.sin(theta) * dy
        y_r = -math.sin(theta) * dx + math.cos(theta) * dy
        
        L_d2 = x_r**2 + y_r**2
        if L_d2 < 0.001:
            return (0.0, 0.0)
        
        # Courbure
        kappa = (2.0 * y_r) / L_d2
        
        # Vitesse avec modulation en courbe
        v = self.k_v * math.sqrt(L_d2)
        v = v / (1.0 + 0.5 * abs(kappa))  # Ralentir en courbe
        
        # Commande angulaire (signe inversé)
        omega = -(v * kappa)
        
        # Saturation
        v = np.clip(v, 0, self.max_linear_vel)
        omega = np.clip(omega, -self.max_angular_vel, self.max_angular_vel)
        
        return (v, omega)
    
    def is_waypoint_reached(self, pose, waypoint):
        """
        Vérifie si le waypoint actuel est atteint.
        
        Args:
            pose: (x, y, theta)
            waypoint: (x, y)
            
        Returns:
            True si dans le seuil
        """
        dist = math.sqrt((waypoint[0] - pose[0])**2 + (waypoint[1] - pose[1])**2)
        return dist < self.waypoint_reached_threshold


################################################################################
PATH: ./core/game/game_manager.py
################################################################################
#!/usr/bin/env python3
import sys
import time
import yaml
import numpy as np
import pygame
import math
import random
from pathlib import Path

# Add project root needed for imports if run directly
ROOT_DIR = Path(__file__).parent.parent.parent
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

# Conditional Import for Mock support
try:
    import cv2
    from perception.camera.realsense_stream import RealSenseStream
    from perception.camera.aruco_detector import ArucoDetector
    PERCEPTION_AVAILABLE = True
except ImportError:
    print("[MANAGER] Warning: Perception modules (cv2/realsense) not found. Forcing MOCK mode.")
    PERCEPTION_AVAILABLE = False

from core.world.unified_transform import load_calibration
from core.ia.strategy import AIStrategy
from core.world.world_model import WorldModel
from renderer.game_renderer import GameRenderer
from core.control.ros_bridge_client import ROSBridgeClient

class GameManager:
    def __init__(self, mock=False):
        self.root_dir = ROOT_DIR
        self.config_dir = self.root_dir / 'config'
        
        # 1. Load Calibration & Configs
        self.tm = load_calibration(str(self.config_dir))
        self.configs = self._load_configs()
        
        # Game Configs
        game_cfg = self.configs.get('game', {})
        self.game_duration = game_cfg.get('duration', 180)
        
        shot_cfg = game_cfg.get('shooting', {})
        self.SHOT_COOLDOWN = shot_cfg.get('cooldown', 1.0)
        self.MAX_SHOT_DIST = shot_cfg.get('max_range', 10.0)
        self.HIT_RADIUS = shot_cfg.get('hit_radius', 0.25)
        
        # Mock Configs
        mock_cfg = game_cfg.get('mock', {})
        # Note: Config stores relative pos [x%, y%, theta]
        mock_ai = mock_cfg.get('ai_start_pos', [0.2, 0.5, 0.0])
        mock_hu = mock_cfg.get('human_start_pos', [0.8, 0.5, 3.14])
        
        self.mock_ai_pos = [
            self.tm.arena_width_m * mock_ai[0],
            self.tm.arena_height_m * mock_ai[1],
            mock_ai[2]
        ]
        self.mock_hu_pos = [
            self.tm.arena_width_m * mock_hu[0],
            self.tm.arena_height_m * mock_hu[1],
            mock_hu[2]
        ]
        
        # Mock override
        self.mock_mode = mock or (not PERCEPTION_AVAILABLE)
        
        # 2. Setup Components
        self.camera = None
        self.aruco = None
        self._setup_perception()
        
        self.world_model = None
        self._setup_world()
        
        self.ai = None
        self._setup_ai()
        
        self.renderer = None
        self._setup_renderer()
        
        self.ros_bridge = None
        self._setup_control()
        
        # 3. Game State
        self.status = "WAITING" 
        self.scores = {"ai": 0, "human": 0}
        self.start_time = 0
        self.last_shot_time = {"ai": 0, "human": 0}
        self.running = True

    # ... (existing methods _load_configs, _setup_control, _setup_perception, _setup_world, _setup_ai, _setup_renderer) ...

    def _attempt_fire(self, shooter_name, shooter_pose, target_pose):
        now = time.time()
        if now - self.last_shot_time.get(shooter_name, 0) < self.SHOT_COOLDOWN:
            return

        self.last_shot_time[shooter_name] = now
        
        # 1. Physics: Raycasting
        sx, sy, sth = shooter_pose
        cos_th = math.cos(sth)
        sin_th = math.sin(sth)
        
        # Default: Max Range
        dist_final = self.MAX_SHOT_DIST
        
        # Check Wall Intersection (Box Arena)
        candidates = []
        if abs(cos_th) > 0.001:
            t1 = (0 - sx) / cos_th; t2 = (self.arena_w - sx) / cos_th
            if t1 > 0: candidates.append(t1)
            if t2 > 0: candidates.append(t2)
        if abs(sin_th) > 0.001:
            t3 = (0 - sy) / sin_th; t4 = (self.arena_h - sy) / sin_th
            if t3 > 0: candidates.append(t3)
            if t4 > 0: candidates.append(t4)
            
        if candidates:
            dist_wall = min(candidates)
            dist_final = min(dist_wall, dist_final)
        
        end_x = sx + dist_final * cos_th
        end_y = sy + dist_final * sin_th
        
        # 2. Check Hit on Target (Circle Intersection)
        hit_confirmed = False
        # HIT_RADIUS is now loaded from config in __init__
        
        if target_pose:
            tx, ty, _ = target_pose
            # Vector Shooter->Target
            idx = tx - sx
            idy = ty - sy
            # Projection scalar
            proj = (idx * cos_th + idy * sin_th)
            
            # If target is in front and within range
            if 0 < proj < dist_final:
                # Closest point on line
                cpx = sx + proj * cos_th
                cpy = sy + proj * sin_th
                
                dist_sq = (cpx - tx)**2 + (cpy - ty)**2
                if dist_sq < (self.HIT_RADIUS**2):
                    hit_confirmed = True
                    # Snap shot end to impact point roughly
                    end_x, end_y = cpx, cpy

    def _load_configs(self):
        cfgs = {}
        for name in ['game', 'arena', 'robot']:
            p = self.config_dir / f"{name}.yaml"
            if p.exists():
                with open(p) as f: cfgs[name] = yaml.safe_load(f)
        return cfgs

    def _setup_control(self):
        if self.mock_mode: return
        self.ros_bridge = ROSBridgeClient(host='127.0.0.1', port=8765)
        if self.ros_bridge.connect():
            print("[MANAGER] ROS Bridge Connected.")
        else:
            print("[MANAGER] WARNING: Failed to connect to ROS Bridge.")

    def _setup_perception(self):
        if self.mock_mode:
            print("[MANAGER] Mock Mode: Perception disabled.")
            return

        if not PERCEPTION_AVAILABLE:
             print("[MANAGER] ERROR: Perception modules not installed. Cannot run in Real mode.")
             print("           Install requirements or run with --mock.")
             sys.exit(1)

        try:
            self.camera = RealSenseStream()
            self.camera.start()
            # ArUco Config (ID 4=AI, ID 5=Human for Tank Arena)
            self.aruco = ArucoDetector(dictionary_type=cv2.aruco.DICT_4X4_50)
            print("[MANAGER] Perception initialized (Realsense + ArUco).")
        except Exception as e:
            print(f"[MANAGER] CRITICAL ERROR: Failed to initialize Perception.")
            print(f"Details: {e}")
            print("Check camera connection. To run simulation, use --mock.")
            # Do NOT switch to mock automatically. User expects real data.
            raise e

    def _setup_world(self):
        # We assume simple rectangular arena for raycasting
        self.arena_w = self.tm.arena_width_m
        self.arena_h = self.tm.arena_height_m
        
        # Load World Model for AI usage
        # Note: WorldModel constructor arguments might need tuning based on config
        self.world_model = WorldModel(self.arena_w, self.arena_h)

    def _setup_ai(self):
        # Simple AI config
        ai_conf = self.configs.get('ia', {})
        self.ai = AIStrategy(ai_conf)
        self.ai.set_world_model(self.world_model)

    def _setup_renderer(self):
        self.renderer = GameRenderer(str(self.config_dir))

    # --- MAIN LOOP ---
    def run(self):
        print("[MANAGER] Game Manager Running. Controls: SPACE=Start/Stop AI, F=User Fire")
        
        try:
            while self.running and self.renderer.running:
                # 1. Input / System Events
                if not self.renderer.process_events():
                    self.running = False
                    break
                
                # 2. Perception (Always Active)
                ai_pose, human_pose = self._update_perception()
                
                if ai_pose: self.world_model.update_robot_pose(4, ai_pose)
                if human_pose: self.world_model.update_robot_pose(5, human_pose)
                
                # 3. Game Logic Inputs
                self._handle_input(human_pose)
                
                # 4. AI & Game Loop
                if self.status == "RUNNING":
                    # Time Check (Simple Duration)
                    if time.time() - self.start_time > self.game_duration:
                        self.status = "TIME UP!" # Will be rendered by overlay
                        print("[MANAGER] TIME UP!")
                    
                    # AI Decision (ONLY executed if RUNNING)
                    if ai_pose and human_pose:
                        world_state = {
                            'ai_pose': ai_pose,
                            'human_pose': human_pose,
                            'game_status': self.status
                        }
                        decision = self.ai.decide(world_state)
                        
                        if decision.get('fire_request'):
                            self._attempt_fire("ai", ai_pose, human_pose)
                        
                        # Logging Strategy
                        v_cmd = decision.get('v', 0)
                        w_cmd = decision.get('w', 0)
                        state_ai = decision.get('state', 'UNKNOWN')
                        print(f"[STRATEGY] State: {state_ai} | v={v_cmd:.2f} w={w_cmd:.2f}")

                        # Note: Velocity commands would be sent to ROS bridge here
                        if not self.mock_mode and self.ros_bridge:
                             self.ros_bridge.send_velocity_command(4, v_cmd, w_cmd)
                else:
                    # AI Stopped (No Tick) - Send Stop command once if needed
                    # Ideally we send stop when transitioning to WAITING
                    pass

                # 5. Render
                render_state = {
                    "status": self.status,
                    "scores": self.scores,
                    "entities": {},
                    "events": []
                }
                if ai_pose: render_state["entities"]["ai"] = {"pos": ai_pose}
                if human_pose: render_state["entities"]["human"] = {"pos": human_pose}
                
                self.renderer.update(render_state)
                
        except KeyboardInterrupt:
            print("[MANAGER] Interrupted.")
        except Exception as e:
            print(f"[MANAGER] Crash: {e}")
            import traceback
            traceback.print_exc()
        finally:
            if self.camera: self.camera.stop()
            if self.renderer: self.renderer.close()
            if self.ros_bridge: 
                self.ros_bridge.send_stop_command(4)
                self.ros_bridge.disconnect()

class GameManager:
    """
    Orchestrateur Central du système Tank Arena.
    --------------------------------------------
    Rôle :
    - Fusionne la Perception (Caméra/ArUco) pour mettre à jour le Modèle du Monde.
    - Exécute la Logique de Jeu (Règles, Scores, Cooldowns).
    - Interroge la Stratégie IA pour les décisions.
    - Envoie les commandes de contrôle aux robots physiques via le Pont ROS.
    - Pilote la Visualisation via GameRenderer.
    
    Architecture :
    - Basée sur une boucle : La méthode run() est le cœur du système (cible 60 Hz).
    - Pilotée par état : Utilise 'status' (WAITING, RUNNING) pour contrôler le flux.
    - Configuration : Entièrement pilotée par les fichiers YAML dans /config.
    """
    def __init__(self, mock=False):
        self.root_dir = ROOT_DIR
        self.config_dir = self.root_dir / 'config'
        
        # 1. Load Calibration & Configs
        self.tm = load_calibration(str(self.config_dir))
        self.configs = self._load_configs()
        
        # Game Configs
        game_cfg = self.configs.get('game', {})
        self.game_duration = game_cfg.get('duration', 180)
        
        shot_cfg = game_cfg.get('shooting', {})
        self.SHOT_COOLDOWN = shot_cfg.get('cooldown', 1.0)
        self.MAX_SHOT_DIST = shot_cfg.get('max_range', 10.0)
        self.HIT_RADIUS = shot_cfg.get('hit_radius', 0.25)
        
        # Mock Configs
        mock_cfg = game_cfg.get('mock', {})
        # Note: Config stores relative pos [x%, y%, theta]
        mock_ai = mock_cfg.get('ai_start_pos', [0.2, 0.5, 0.0])
        mock_hu = mock_cfg.get('human_start_pos', [0.8, 0.5, 3.14])
        
        self.mock_ai_pos = [
            self.tm.arena_width_m * mock_ai[0],
            self.tm.arena_height_m * mock_ai[1],
            mock_ai[2]
        ]
        self.mock_hu_pos = [
            self.tm.arena_width_m * mock_hu[0],
            self.tm.arena_height_m * mock_hu[1],
            mock_hu[2]
        ]
        
        # Mock override
        self.mock_mode = mock or (not PERCEPTION_AVAILABLE)
        
        # 2. Setup Components
        self.camera = None
        self.aruco = None
        self._setup_perception()
        
        self.world_model = None
        self._setup_world()
        
        self.ai = None
        self._setup_ai()
        
        self.renderer = None
        self._setup_renderer()
        
        self.ros_bridge = None
        self._setup_control()
        
        # 3. Game State
        self.status = "WAITING" 
        self.scores = {"ai": 0, "human": 0}
        self.start_time = 0
        self.last_shot_time = {"ai": 0, "human": 0}
        self.last_shot_time = {"ai": 0, "human": 0}
        self.last_shot_time = {"ai": 0, "human": 0}
        self.running = True

    def _setup_perception(self):
        if self.mock_mode: return
        print("[MANAGER] Initialisation Perception...")
        try:
            self.camera = RealSenseStream(width=1280, height=720, fps=30)
            self.camera.start()
            self.aruco = ArucoDetector() # Calibration chargée implicitement ou par défaut
        except Exception as e:
            print(f"[MANAGER] Erreur Perception: {e}")
            print("[MANAGER] Passage en MODE MOCK.")
            self.mock_mode = True

    def _setup_world(self):
        print("[MANAGER] Initialisation World Model...")
        self.world_model = WorldModel(
            arena_width_m=self.tm.arena_width_m,
            arena_height_m=self.tm.arena_height_m
        )
        self.world_model.generate_costmap()

    def _setup_ai(self):
        # AI Strategy needs Config
        ia_cfg = self.configs.get('ia', {})
        self.ai = AIStrategy(ia_cfg)
        self.ai.set_world_model(self.world_model)

    def _setup_renderer(self):
        print("[MANAGER] Initialisation Renderer...")
        proj_cfg = self.configs.get('projector', {})
        self.renderer = GameRenderer(
             transform_manager=self.tm,
             config=proj_cfg 
        )

    def _setup_control(self):
        if self.mock_mode: return
        print("[MANAGER] Initialisation ROS Bridge...")
        robot_cfg = self.configs.get('robot', {})
        bridge_cfg = robot_cfg.get('ros_bridge', {})
        
        host = bridge_cfg.get('host', '127.0.0.1')
        port = bridge_cfg.get('port', 8765)
        
        self.ros_bridge = ROSBridgeClient(host, port)
        if self.ros_bridge.connect():
             print("[MANAGER] ROS Bridge Connecté.")
        else:
             print("[MANAGER] ATTENTION: Échec connexion ROS Bridge.")

    def _load_configs(self):
        """Loads all YAML configurations from config/ directory."""
        configs = {}
        for name in ['game', 'robot', 'projector', 'ia']:
            path = self.config_dir / f"{name}.yaml"
            if path.exists():
                with open(path, 'r') as f:
                    configs[name] = yaml.safe_load(f)
            else:
                print(f"[MANAGER] Warning: Config {name}.yaml not found.")
                configs[name] = {}
        return configs

    def _update_perception(self):
        """
        Récupère les frames, détecte les marqueurs ArUco et met à jour les Poses des Robots.
        
        Retourne :
            (ai_pose, human_pose) sous forme [x, y, theta]
        """
        if self.mock_mode:
            return self._mock_perception()
            
        # Real Perception
        color_frame, _ = self.camera.get_frames()
        if color_frame is None: return None, None
        
        detections = self.aruco.detect(color_frame)
        
        ai_pose = None
        human_pose = None
        
        for marker_id, data in detections.items():
            # Extraction données standard
            u, v = data['center']
            th_pix = data['orientation']
            
            # --- Transformation de Coordonnées (Caméra -> Monde) ---
            # Utilisation de UnifiedTransform (Homographie)
            wx, wy = self.tm.camera_to_world(u, v)
            
            # --- Calcul d'Orientation ---
            # Projette un point 20px devant dans l'espace image pour déterminer l'orientation monde.
            # Cette méthode est robuste à la distorsion et correspond à la logique legacy BaseTask.
            u_f = u + 20 * math.cos(th_pix)
            v_f = v + 20 * math.sin(th_pix)
            xf, yf = self.tm.camera_to_world(u_f, v_f)
            
            theta = math.atan2(yf - wy, xf - wx)
            
            # --- MAPPAGE ID (CORRECTIF HISTORIQUE) ---
            # Problème : Dans l'arène physique, les marqueurs étaient inversés par rapport au code.
            #            Le code supposait ID 4 = IA et ID 5 = Humain.
            #            Cependant, le robot Utilisateur avait l'ID 4, causant un tir de l'IA quand l'User appuyait.
            # Fix :      Inversion forcée des IDs ici pour correspondre à la Réalité.
            #            ID 4 -> HUMAIN (Joueur)
            #            ID 5 -> IA (Adversaire)
            
            if marker_id == 4:
                human_pose = [wx, wy, theta]
            elif marker_id == 5:
                ai_pose = [wx, wy, theta]
                    
        return ai_pose, human_pose

    def _mock_perception(self):
        """Simulate robot movements for testing logic."""
        # Simple random walk or static
        # AI
        self.mock_ai_pos[2] += 0.01 # Spin
        
        # Human - Mouse control? Or simple bounce
        # Just static + noise
        noise = (random.random() - 0.5) * 0.02
        self.mock_hu_pos[0] += noise
        
        # Clamp to arena
        self.mock_hu_pos[0] = max(0, min(self.arena_w, self.mock_hu_pos[0]))
        
        return self.mock_ai_pos, self.mock_hu_pos

    def _handle_input(self, human_pose):
        keys = pygame.key.get_pressed()
        
        # Debounce logic
        current_time = time.time()
        if hasattr(self, 'last_key_time') and (current_time - self.last_key_time < 0.2):
            return

        if keys[pygame.K_SPACE]:
            if self.status == "RUNNING":
                print("[MANAGER] STOP (AI Halted)")
                self.status = "WAITING"
            elif self.status == "WAITING" or self.status == "GAME OVER":
                self.status = "RUNNING"
                self.start_time = time.time()
                # self.scores = {"ai": 0, "human": 0} # Optional: reset scores on restart
                print("[MANAGER] START (AI Active)")
            self.last_key_time = current_time
        
        # User Fire (Always possible or only when running? User said "F l'user a tire")
        # Let's assume User can fire anytime for fun, or restrict to RUNNING. User said "pour l'instant on va se contenter quand on le lance que l'user tire" implies RUNNING.
        if keys[pygame.K_f] and self.status == "RUNNING":
             if human_pose:
                 # Target for Human is AI
                 ai_pos_target = self.world_model.get_robot_pose(4)
                 self._attempt_fire("human", human_pose, ai_pos_target)
                 self.last_key_time = current_time

    def _attempt_fire(self, shooter_name, shooter_pose, target_pose):
        now = time.time()
        if now - self.last_shot_time.get(shooter_name, 0) < self.SHOT_COOLDOWN:
            return

        self.last_shot_time[shooter_name] = now
        
        # 1. Physics: Raycasting
        sx, sy, sth = shooter_pose
        cos_th = math.cos(sth)
        sin_th = math.sin(sth)
        
        # Default: Max Range
        dist_final = self.MAX_SHOT_DIST
        
        # Check Wall Intersection (Box Arena)
        candidates = []
        if abs(cos_th) > 0.001:
            t1 = (0 - sx) / cos_th; t2 = (self.world_model.arena_width - sx) / cos_th
            if t1 > 0: candidates.append(t1)
            if t2 > 0: candidates.append(t2)
        if abs(sin_th) > 0.001:
            t3 = (0 - sy) / sin_th; t4 = (self.world_model.arena_height - sy) / sin_th
            if t3 > 0: candidates.append(t3)
            if t4 > 0: candidates.append(t4)
            
        if candidates:
            dist_wall = min(candidates)
            dist_final = min(dist_wall, dist_final)
        
        end_x = sx + dist_final * cos_th
        end_y = sy + dist_final * sin_th
        
        # 2. Check Hit on Target (Circle Intersection)
        hit_confirmed = False
        HIT_RADIUS = 0.20 # Updated to match yaml
        
        if target_pose:
            tx, ty, _ = target_pose
            # Vector Shooter->Target
            idx = tx - sx
            idy = ty - sy
            # Projection scalar
            proj = (idx * cos_th + idy * sin_th)
            
            # If target is in front and within range
            if 0 < proj < dist_final:
                # Closest point on line
                cpx = sx + proj * cos_th
                cpy = sy + proj * sin_th
                
                dist_sq = (cpx - tx)**2 + (cpy - ty)**2
                if dist_sq < (HIT_RADIUS**2):
                    hit_confirmed = True
                    # Snap shot end to impact point roughly
                    end_x, end_y = cpx, cpy

        # 3. Resolve
        is_ai = (shooter_name == "ai")
        if hit_confirmed:
            self.scores[shooter_name] += 1
            self.renderer.trigger_hit([end_x, end_y])
            print(f"[MANAGER] HIT! {shooter_name} scored.")
            
        self.renderer.trigger_shot([sx, sy], [end_x, end_y], is_ai)

    def run(self):
        print("[MANAGER] Démarrage de la boucle principale (60Hz)...")
        clock = pygame.time.Clock()
        
        try:
            while self.running:
                dt = clock.tick(60) / 1000.0
                
                # 1. Perception
                ai_pose, human_pose = self._update_perception()
                
                # DEBUG: Print perception status periodically
                if int(time.time() * 10) % 20 == 0: 
                     print(f"[DEBUG] Pose AI: {ai_pose}, Human: {human_pose}")

                # 2. Input
                self._handle_input(human_pose)
                for event in pygame.event.get():
                    if event.type == pygame.QUIT:
                        self.running = False
                
                # 3. Logic & AI
                if self.status == "RUNNING":
                    # Time Check
                    elapsed = time.time() - self.start_time
                    if elapsed > self.game_duration:
                        self.status = "GAME OVER"
                        print("[MANAGER] GAME OVER - Temps écoulé")
                    
                    # AI Decision
                    if ai_pose:
                        # Update World Model
                        if human_pose:
                             self.world_model.update_robot_pose(4, human_pose)
                        self.world_model.update_robot_pose(5, ai_pose)
                        
                        # Strategy Decision
                        state = {
                            'ai_pose': ai_pose,
                            'human_pose': human_pose,
                            'game_status': self.status
                        }
                        cmd = self.ai.decide(state)
                        
                        # Send Control
                        v = cmd.get('v', 0.0)
                        w = cmd.get('w', 0.0)
                        fire_req = cmd.get('fire_request', False)
                        
                        if int(time.time() * 10) % 20 == 0:
                            print(f"[DEBUG] CMD -> V={v:.2f}, W={w:.2f}, FIRE={fire_req}")

                        if self.ros_bridge:
                            self.ros_bridge.send_velocity_command(5, v, w)
                            
                        # Handle AI Fire
                        if fire_req and human_pose:
                            self._attempt_fire("ai", ai_pose, human_pose)

                else:
                    # Safety Stop
                    if self.ros_bridge:
                        self.ros_bridge.send_velocity_command(5, 0.0, 0.0)
                
                # 4. Render
                entities = {}
                if ai_pose:
                    entities['ai'] = {'pos': ai_pose}
                if human_pose:
                    entities['human'] = {'pos': human_pose}
                    
                state_dataset = {
                    'status': self.status,
                    'scores': self.scores,
                    'time_left': max(0, self.game_duration - (time.time() - self.start_time)) if self.status == "RUNNING" else 0,
                    'entities': entities
                }
                self.renderer.render(state_dataset)
                
        except KeyboardInterrupt:
            print("\n[MANAGER] Arrêt manuel (Ctrl+C)")
        finally:
            self.close()

    def close(self):
        print("[MANAGER] Fermeture des composants...")
        if self.camera: self.camera.stop()
        if self.ros_bridge: 
            self.ros_bridge.send_stop_command(5)
            self.ros_bridge.disconnect()
        pygame.quit()
        print("[MANAGER] Bye!")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--mock', action='store_true', help="Run in Mock Mode (no hardware)")
    args = parser.parse_args()
    
    gm = GameManager(mock=args.mock)
    gm.run()


################################################################################
PATH: ./core/game/__init__.py
################################################################################


################################################################################
PATH: ./core/game/raycast.py
################################################################################
"""
Raycast - Détection de Collision de Tir

Implémente la détection de collision par lancer de rayons pour les tirs laser :
- Lance un rayon depuis la position du tireur dans la direction theta
- Vérifie les intersections avec :
  * Obstacles statiques (murs, blocs)
  * Obstacles dynamiques (robots)
- Retourne le premier impact ou None

Utilise la grille d'occupation de core/world pour la détection d'obstacles.
Implémente DDA (Digital Differential Analyzer) pour une traversée de grille efficace.

Logs : [RAYCAST] Impact détecté / Manqué
"""

import numpy as np
from typing import Optional, Tuple


class Raycast:
    """
    Détection de collision efficace basée sur les rayons pour les tirs.
    
    Utilise l'algorithme DDA pour traverser la grille d'occupation et détecter les impacts.
    """
    
    def __init__(self, occupancy_grid):
        """
        Initialise le raycast avec la grille d'occupation du monde.
        
        Args:
            occupancy_grid: Instance OccupancyGrid de core/world
        """
        self.grid = occupancy_grid
    
    
    def cast_shot(self, start_pos, theta, max_range_m):
        """
        Lance un rayon de tir et détecte les collisions.
        
        Args:
            start_pos: (x, y) position du tireur en mètres
            theta: Direction du tir en radians
            max_range_m: Portée maximale du tir
            
        Returns:
            dict: {
                'hit': bool,
                'target': 'robot4' | 'robot5' | 'obstacle' | None,
                'impact_point': (x, y) en mètres ou None,
                'distance': float en mètres
            }
        """
        start_x, start_y = start_pos
        
        # Vecteur direction du rayon
        dx = np.cos(theta)
        dy = np.sin(theta)
        
        # Parcours de grille
        # FIX: Commencer 20cm devant pour sortir du robot
        current_dist = 0.20
        step_size = self.grid.resolution
        
        # Vérifie chaque point le long du rayon
        while current_dist <= max_range_m:
            curr_x = start_x + dx * current_dist
            curr_y = start_y + dy * current_dist
            
            # 1. Vérifie les limites de la carte
            if not (0 <= curr_x <= self.grid.width_m and 0 <= curr_y <= self.grid.height_m):
                break
                
            # 2. Vérifie les obstacles statiques utilisant la grille d'occupation
            grid_val = self.grid.get_value(curr_x, curr_y)
            if grid_val > 50:  # Threshold for occupied
                return {
                    'hit': True,
                    'target': 'obstacle',
                    'impact_point': (curr_x, curr_y),
                    'distance': current_dist
                }
            
            current_dist += step_size
            
        return {
            'hit': False,
            'target': None,
            'impact_point': None,
            'distance': max_range_m
        }

    def check_robot_collision(self, start_pos, theta, max_range_m, target_pos, target_radius_m=0.15):
        """
        Vérifie si le rayon touche un robot spécifique.
        
        Args:
            start_pos: (x,y) tireur
            theta: angle
            max_range_m: portée max
            target_pos: (x,y) centre cible
            target_radius_m: rayon de touche cible
        """
        # Vecteur du tireur à la cible
        sx, sy = start_pos
        tx, ty = target_pos
        
        val_x = tx - sx
        val_y = ty - sy
        
        # Projette le centre de la cible sur le rayon
        # Vecteur rayon : (cos, sin)
        ray_x, ray_y = np.cos(theta), np.sin(theta)
        
        # Produit scalaire
        t = val_x * ray_x + val_y * ray_y
        
        # Point le plus proche sur le rayon vers le centre de la cible
        closest_x = sx + t * ray_x
        closest_y = sy + t * ray_y
        
        # Vérification des distances
        if t < 0: return False # Cible derrière le tireur
        if t > max_range_m: return False # Cible hors de portée
        
        # Distance du point le plus proche au centre de la cible
        dist_sq = (closest_x - tx)**2 + (closest_y - ty)**2
        
        return dist_sq <= (target_radius_m**2)

    def _check_line_of_sight(self, pos1, pos2):
        """Vérifie si la ligne de vue (LOS) est dégagée entre deux points (version simple)."""
        x1, y1 = pos1
        x2, y2 = pos2
        
        dist = np.sqrt((x2-x1)**2 + (y2-y1)**2)
        if dist == 0: return True
        
        dx = (x2 - x1) / dist
        dy = (y2 - y1) / dist
        
        # Pas à pas dans la grille
        curr_dist = 0
        step = self.grid.resolution
        
        while curr_dist < dist:
            cx = x1 + dx * curr_dist
            cy = y1 + dy * curr_dist
            
            if self.grid.get_value(cx, cy) > 50:
                return False
                
            curr_dist += step
            
        return True



################################################################################
PATH: ./core/ia/behavior_tree.py
################################################################################
"""
Arbre de Comportement - Version Avancée (basée sur bt_latest.py)

Fonctionnalités :
1. Structure hiérarchique : Selector, Sequence, Condition, Action
2. Stabilité Temporelle (Dwell Time) : Évite les oscillations d'état
3. Priorités : Survie > Attaque > Poursuite
"""

import time
import numpy as np
from enum import Enum
from typing import Dict, List, Optional

# --- CONFIG DEFAULTS ---
# Ces valeurs peuvent être surchargées par la config YAML
DEFAULT_BT_CONFIG = {
    "min_state_duration_s": 0.40,
    "min_state_duration_by_state_s": {
        "ATTAQUE (ALIGNEMENT)": 0.35,
        "ATTAQUE (VERROUILLÉ)": 0.25,
        "POURSUITE": 0.50,
        "RECHERCHE DE CIBLES": 0.20,
        "INITIALISATION": 0.10,
    },
    "force_switch_states": [
        "RETRAIT (MENACE DIRECTE)"
    ],
    "log_transitions": True
}

# --- PARAMÈTRES IA PAR DÉFAUT ---
SEUIL_MENACE_DEG = 15.0     # Angle max pour considérer que l'ennemi nous vise
SEUIL_LOCK_TIR_DEG = 5.0    # Précision requise pour l'état de tir

class BTStatus(Enum):
    SUCCESS = 0
    FAILURE = 1
    RUNNING = 2

class BTNode:
    def __init__(self, name):
        self.name = name

    def tick(self, ai_pose, en_pose, world) -> BTStatus:
        raise NotImplementedError

class SequenceNode(BTNode):
    def __init__(self, name, children: List[BTNode]):
        super().__init__(name)
        self.children = children

    def tick(self, ai_pose, en_pose, world) -> BTStatus:
        for child in self.children:
            status = child.tick(ai_pose, en_pose, world)
            if status != BTStatus.SUCCESS:
                return status
        return BTStatus.SUCCESS

class SelectorNode(BTNode):
    def __init__(self, name, children: List[BTNode]):
        super().__init__(name)
        self.children = children

    def tick(self, ai_pose, en_pose, world) -> BTStatus:
        for child in self.children:
            status = child.tick(ai_pose, en_pose, world)
            if status != BTStatus.FAILURE:
                return status
        return BTStatus.FAILURE

class ConditionNode(BTNode):
    def __init__(self, name, condition_func):
        super().__init__(name)
        self.condition_func = condition_func

    def tick(self, ai_pose, en_pose, world) -> BTStatus:
        if self.condition_func(ai_pose, en_pose, world):
            return BTStatus.SUCCESS
        return BTStatus.FAILURE

class ActionNode(BTNode):
    def __init__(self, name, action_func):
        super().__init__(name)
        self.action_func = action_func

    def tick(self, ai_pose, en_pose, world) -> BTStatus:
        self.action_func(ai_pose, en_pose, world)
        return BTStatus.SUCCESS


class TankBehaviorTree:
    """
    Arbre de comportement principal du Tank.
    Intègre la logique de décision et la stabilité temporelle.
    """
    def __init__(self, world_model, config: Optional[Dict] = None):
        self.world = world_model
        self.etat = "INITIALISATION"
        self.panic_until = 0.0 # Timer pour le mode panique
        
        # Output de l'arbre pour la stratégie
        self.current_decision = {
            'state': "INITIALISATION",
            'fire_request': False,
            'target_orientation': None,
            'has_los': False,
            'target_position': None # Sera rempli par la stratégie/tactique
        }

        # Config stabilité (dwell time)
        cfg = dict(DEFAULT_BT_CONFIG)
        if isinstance(config, dict):
            cfg.update(config)
            if "min_state_duration_by_state_s" in config:
                merged = dict(DEFAULT_BT_CONFIG.get("min_state_duration_by_state_s", {}))
                merged.update(config["min_state_duration_by_state_s"])
                cfg["min_state_duration_by_state_s"] = merged

        self.bt_cfg = cfg
        self._last_state_change_ts = time.time()
        self._build_tree()
        
    def _min_duration_for_state(self, state_name: str) -> float:
        by_state = self.bt_cfg.get("min_state_duration_by_state_s", {}) or {}
        if state_name in by_state:
            return float(by_state[state_name])
        return float(self.bt_cfg.get("min_state_duration_s", 0.4))

    def _can_switch(self, new_state: str) -> bool:
        now = time.time()
        dt = now - self._last_state_change_ts
        required = self._min_duration_for_state(self.etat)
        return dt >= required

    def _set_state(self, new_state: str, context_updates: Dict, reason: str = ""):
        """
        Met à jour l'état avec verrou temporel et applique les mises à jour de contexte (décisions).
        """
        # Toujours mettre à jour les intentions (tir, orientation) même si l'état ne change pas
        self.current_decision.update(context_updates)
        self.current_decision['state'] = self.etat # Default to current

        if new_state == self.etat:
            return

        force_states = set(self.bt_cfg.get("force_switch_states", []) or [])
        force = (new_state in force_states)

        if force or self._can_switch(new_state):
            prev = self.etat
            self.etat = new_state
            self.current_decision['state'] = new_state
            self._last_state_change_ts = time.time()
            
            if self.bt_cfg.get("log_transitions", True):
                extra = f" | {reason}" if reason else ""
                print(f"[BT] TRANSITION: {prev} -> {new_state}{extra}")

    # --- MÉTHODES UTILITAIRES / PRÉDICATS ---

    def check_ligne_de_vue(self, p1, p2):
        """Vérifie si le trajet est libre d'obstacles."""
        steps = 20
        for i in range(1, steps):
            tx = p1[0] + (p2[0] - p1[0]) * (i / steps)
            ty = p1[1] + (p2[1] - p1[1]) * (i / steps)
            if not self.world.is_position_valid(tx, ty):
                return False
        return True

    def est_vise_par_ennemi(self, ai_pose, en_pose):
        if ai_pose is None or en_pose is None:
            return False
            
        # Vecteur Ennemi -> IA
        dx, dy = ai_pose[0] - en_pose[0], ai_pose[1] - en_pose[1]
        angle_vers_ia = np.arctan2(dy, dx)
        
        # Angle de regard de l'ennemi
        theta_ennemi = en_pose[2]
        
        erreur_angle = (theta_ennemi - angle_vers_ia + np.pi) % (2 * np.pi) - np.pi
        erreur_deg = abs(np.degrees(erreur_angle))
        
        # Si l'ennemi regarde vers nous ET qu'il n'y a pas d'obstacle
        if erreur_deg < SEUIL_MENACE_DEG:
            return self.check_ligne_de_vue(en_pose[:2], ai_pose[:2])
        return False

    def _build_tree(self):
        """Construit la structure de l'arbre."""
        
        # --- ACTIONS ---
        
        def set_retrait(ai_pose, en_pose, world):
            self._set_state("RETRAIT (MENACE DIRECTE)", {
                'fire_request': False,
                'target_orientation': None # La stratégie gérera la fuite
            }, reason="Menace directe")

        def set_attaque_lock(ai_pose, en_pose, world):
            self._set_state("ATTAQUE (VERROUILLÉ)", {
                'fire_request': True,
                'target_orientation': en_pose[:2], # Vise l'ennemi
                'has_los': True
            }, reason="Tir possible")

        def set_attaque_align(ai_pose, en_pose, world):
            self._set_state("ATTAQUE (ALIGNEMENT)", {
                'fire_request': False, # Pas encore
                'target_orientation': en_pose[:2], # Vise l'ennemi
                'has_los': True
            }, reason="Alignement en cours")

        def set_poursuite(ai_pose, en_pose, world):
            self._set_state("POURSUITE", {
                'fire_request': False,
                'target_orientation': None, # La stratégie gérera le pathfinding
                'has_los': False
            }, reason="Recherche cible")

        # --- CONDITIONS ---

        def menace_directe(ai_pose, en_pose, world):
            is_threat = self.est_vise_par_ennemi(ai_pose, en_pose)
            
            # Logic Panic (Hysteresis)
            if is_threat:
                self.panic_until = time.time() + 2.0 # 2s de Panique assurée
            
            if time.time() < self.panic_until:
                return True
                
            return False

        def ligne_de_vue(ai_pose, en_pose, world):
            if ai_pose is None or en_pose is None:
                return False
            return self.check_ligne_de_vue(ai_pose[:2], en_pose[:2])

        def verrouille(ai_pose, en_pose, world):
            # Vérifie si NOUS visons l'ennemi
            if not ligne_de_vue(ai_pose, en_pose, world):
                return False
            dx, dy = en_pose[0] - ai_pose[0], en_pose[1] - ai_pose[1]
            angle_vers_en = np.arctan2(dy, dx)
            erreur_ia = (ai_pose[2] - angle_vers_en + np.pi) % (2 * np.pi) - np.pi
            return abs(np.degrees(erreur_ia)) < SEUIL_LOCK_TIR_DEG

        # --- ARBRE ---
        
        self.root = SelectorNode("Root", [
            # 1. RETRAIT (Survie)
            SequenceNode("Retrait", [
                ConditionNode("Menace ?", menace_directe),
                ActionNode("Action Retrait", set_retrait),
            ]),
            
            # 2. ATTAQUE (Aggressif)
            SelectorNode("Attaque", [
                # Tir si verrouillé
                SequenceNode("Tir", [
                    ConditionNode("LOS ?", ligne_de_vue),
                    ConditionNode("Lock ?", verrouille),
                    ActionNode("Action Tir", set_attaque_lock),
                ]),
                # Alignement si vu maiz pas lock
                SequenceNode("Align", [
                    ConditionNode("LOS ?", ligne_de_vue),
                    ActionNode("Action Align", set_attaque_align),
                ]),
            ]),
            
            # 3. POURSUITE (Défaut)
            ActionNode("Action Poursuite", set_poursuite),
        ])

    def execute(self, context) -> Dict:
        """
        Exécute l'arbre et retourne les décisions.
        Format du context attendu : { 'ai_pose', 'human_pose', ... }
        """
        ai_pose = context.get('ai_pose')
        en_pose = context.get('human_pose')
        world = self.world # Use internal world ref for obstacles

        if ai_pose is None or en_pose is None:
            self._set_state("RECHERCHE DE CIBLES", {
                'fire_request': False,
                'target_position': None
            }, reason="Perte visuelle")
            return self.current_decision

        self.root.tick(ai_pose, en_pose, world)
        return self.current_decision


################################################################################
PATH: ./core/ia/decisions.py
################################################################################
"""
Décisions - Fonctions de Décision Tactique

Fournit les fonctions d'évaluation tactique utilisées par les conditions de l'arbre comportemental :
- L'ennemi est-il trop proche ? (évaluation de la menace)
- A-t-on une ligne de vue ? (vérification visibilité)
- Sommes-nous à portée de tir optimale ?
- Y a-t-il une couverture à proximité ?
- Devrions-nous nous replier ?

Toutes les fonctions prennent un dict context et retournent bool ou une valeur tactique.

Logs : [DECISION] Évaluation X : valeur Y
"""

import numpy as np
from typing import Dict, Tuple, Optional, List


def is_enemy_too_close(context: Dict, threshold_m: float = 0.8) -> bool:
    """
    Vérifie si l'ennemi est dangereusement proche.
    
    Args:
        context: État du monde avec poses robots
        threshold_m: Seuil de danger en mètres
        
    Returns:
        True si ennemi dans le seuil
    """
    ai_pos = context['ai_pose'][:2]
    human_pos = context['human_pose'][:2]
    distance = np.linalg.norm(np.array(ai_pos) - np.array(human_pos))
    
    return distance < threshold_m


def has_line_of_sight(context: Dict) -> bool:
    """
    Vérifie si l'IA a une ligne de vue dégagée vers l'ennemi.
    
    Utilise le raycast de core.world pour vérifier les obstacles.
    
    Args:
        context: État du monde avec poses robots et raycast
        
    Returns:
        True si une ligne de vue claire existe
        
    Logs :
        [DECISION] Verif LOS : CLAIR/BLOQUE
    """
    ai_pose = context.get('ai_pose')
    human_pose = context.get('human_pose')
    raycast = context.get('raycast_sys')
    
    if ai_pose is None or human_pose is None:
        return False
    
    if raycast is None:
        # Pas de système raycast disponible, suppose LOS dégagée
        print("[DECISION] Verif LOS : PAS DE SYSTEME RAYCAST")
        return True
    
    # Utilise la vérification LOS interne du raycast
    ai_pos = ai_pose[:2]
    human_pos = human_pose[:2]
    
    los_clear = raycast._check_line_of_sight(ai_pos, human_pos)
    
    status = "CLAIR" if los_clear else "BLOQUE"
    print("[DECISION] Verif LOS : {}".format(status))
    
    return los_clear


def is_optimal_firing_range(context: Dict, 
                            min_range: float = 1.2, 
                            max_range: float = 3.5) -> bool:
    """
    Vérifie si l'ennemi est à portée de tir optimale.
    
    Trop proche : risque de riposte
    Trop loin : la précision diminue
    
    Args:
        context: État du monde
        min_range: Distance minimale de sécurité
        max_range: Distance effective maximale
        
    Returns:
        True si dans la portée optimale
    """
    ai_pose = context.get('ai_pose')
    human_pose = context.get('human_pose')
    
    if ai_pose is None or human_pose is None:
        return False
    
    ai_pos = np.array(ai_pose[:2])
    human_pos = np.array(human_pose[:2])
    
    distance = np.linalg.norm(ai_pos - human_pos)
    
    in_range = min_range <= distance <= max_range
    
    print("[DECISION] Verif portée tir : distance={:.2f}m, optimal={}".format(
        distance, in_range))
    
    return in_range


def find_nearest_cover(context: Dict) -> Optional[Tuple[float, float]]:
    """
    Trouve la position de couverture la plus proche par rapport à l'ennemi.
    
    Couverture = obstacle qui bloque la ligne de vue vers l'ennemi.
    
    Args:
        context: État du monde avec grille d'occupation
        
    Returns:
        (x, y) position de la meilleure couverture, ou None
        
    Algorithme :
        1. Récupère toutes les cellules d'obstacle de la grille
        2. Pour chaque obstacle, vérifie s'il bloque la LOS vers l'ennemi
        3. Classe par :
           - Distance à l'IA (plus proche est mieux)
           - Efficacité de la couverture (bloque bien la LOS)
        4. Retourne la meilleure position
    """
    ai_pose = context.get('ai_pose')
    human_pose = context.get('human_pose')
    grid = context.get('occupancy_grid')
    
    if ai_pose is None or human_pose is None or grid is None:
        return None
    
    ai_pos = np.array(ai_pose[:2])
    human_pos = np.array(human_pose[:2])
    
    # Direction de l'ennemi vers l'IA
    direction = ai_pos - human_pos
    dist = np.linalg.norm(direction)
    if dist < 0.1:
        return None
    direction = direction / dist
    
    # Cherche des positions de couverture : déplacement perpendiculaire à la direction ennemie
    perpendicular = np.array([-direction[1], direction[0]])
    
    # Vérifie les positions à gauche et à droite de la position actuelle
    cover_distance = 0.5  # mètres de la position actuelle
    
    candidates = [
        ai_pos + perpendicular * cover_distance,
        ai_pos - perpendicular * cover_distance,
        ai_pos + direction * cover_distance,  # S'éloigne de l'ennemi
    ]
    
    # Trouve une position de couverture valide (dans les limites de l'arène)
    for candidate in candidates:
        x, y = candidate
        if 0 < x < grid.width_m and 0 < y < grid.height_m:
            if not grid.is_occupied(x, y):
                print("[DECISION] Couverture trouvée à ({:.2f}, {:.2f})".format(x, y))
                return (x, y)
    
    print("[DECISION] Aucune couverture trouvée")
    return None


def should_retreat(context: Dict) -> bool:
    """
    Décision de repli complète.
    
    Repli si :
    - Ennemi trop proche ET a la LOS
    - Santé faible (fonctionnalité future)
    - Encerclé
    
    Args:
        context: État du monde
        
    Returns:
        True si devrait se replier
    """
    too_close = is_enemy_too_close(context)
    los = has_line_of_sight(context)
    
    should_run = too_close and los
    
    if should_run:
        print("[DECISION] REPLI déclenché : ennemi trop proche avec LOS")
    
    return should_run


def calculate_flank_position(context: Dict) -> Optional[Tuple[float, float]]:
    """
    Calcule la position de contournement optimale.
    
    But : position qui :
    - Donne à l'IA une ligne de vue sur l'ennemi
    - N'est PAS dans la ligne de vue actuelle de l'ennemi
    - Utilise une couverture pour l'approche
    
    Args:
        context: État du monde
        
    Returns:
        (x, y) position cible de contournement
        
    Algorithme :
        1. Récupère position et orientation ennemi
        2. Trouve positions 90 deg gauche/droite de l'orientation ennemie
        3. Filtre par disponibilité couverture pendant l'approche
        4. Choisit la position valide la plus proche
    """
    ai_pose = context.get('ai_pose')
    human_pose = context.get('human_pose')
    grid = context.get('occupancy_grid')
    
    if ai_pose is None or human_pose is None:
        return None
    
    ai_pos = np.array(ai_pose[:2])
    human_pos = np.array(human_pose[:2])
    human_theta = human_pose[2] if len(human_pose) > 2 else 0.0
    
    # Direction du regard ennemi
    enemy_facing = np.array([np.cos(human_theta), np.sin(human_theta)])
    
    # Positions de flanc : 90 degrés par rapport au regard ennemi
    flank_distance = 1.5  # mètres de l'ennemi
    
    # Flanc gauche (perpendiculaire)
    left_perp = np.array([-enemy_facing[1], enemy_facing[0]])
    left_flank = human_pos + left_perp * flank_distance
    
    # Flanc droit
    right_perp = np.array([enemy_facing[1], -enemy_facing[0]])
    right_flank = human_pos + right_perp * flank_distance
    
    # Choisit le flanc le plus proche de l'IA
    dist_left = np.linalg.norm(ai_pos - left_flank)
    dist_right = np.linalg.norm(ai_pos - right_flank)
    
    if dist_left < dist_right:
        chosen_flank = left_flank
    else:
        chosen_flank = right_flank
    
    x, y = chosen_flank
    
    # Valide que la position est dans l'arène
    if grid is not None:
        if not (0 < x < grid.width_m and 0 < y < grid.height_m):
            print("[DECISION] Position contournement hors limites")
            return None
        if grid.is_occupied(x, y):
            print("[DECISION] Position contournement occupée")
            return None
    
    print("[DECISION] Position contournement : ({:.2f}, {:.2f})".format(x, y))
    return (x, y)


def get_distance_to_enemy(context: Dict) -> float:
    """
    Obtient la distance entre l'IA et l'ennemi.
    
    Args:
        context: État du monde
        
    Returns:
        Distance en mètres, ou l'infini si poses inconnues
    """
    ai_pose = context.get('ai_pose')
    human_pose = context.get('human_pose')
    
    if ai_pose is None or human_pose is None:
        return float('inf')
    
    ai_pos = np.array(ai_pose[:2])
    human_pos = np.array(human_pose[:2])
    
    return float(np.linalg.norm(ai_pos - human_pos))


def get_angle_to_enemy(context: Dict) -> float:
    """
    Obtient l'angle de l'IA vers l'ennemi.
    
    Args:
        context: État du monde
        
    Returns:
        Angle en radians, ou 0 si poses inconnues
    """
    ai_pose = context.get('ai_pose')
    human_pose = context.get('human_pose')
    
    if ai_pose is None or human_pose is None:
        return 0.0
    
    dx = human_pose[0] - ai_pose[0]
    dy = human_pose[1] - ai_pose[1]
    
    return float(np.arctan2(dy, dx))


def is_aimed_at_enemy(context: Dict, threshold_deg: float = 10.0) -> bool:
    """
    Vérifie si l'IA est alignée avec l'ennemi (angle < threshold).
    
    Args:
        context: État du monde
        threshold_deg: Seuil d'alignement en degrés
        
    Returns:
        True si aligné
    """
    ai_pose = context.get('ai_pose')
    human_pose = context.get('human_pose')
    
    if ai_pose is None or human_pose is None:
        return False
        
    target_angle = get_angle_to_enemy(context)
    current_angle = ai_pose[2]
    
    diff = target_angle - current_angle
    # Normalisation -pi à pi
    while diff > np.pi: diff -= 2*np.pi
    while diff < -np.pi: diff += 2*np.pi
    
    aligned = abs(diff) < np.radians(threshold_deg)
    
    # print(f"[DECISION] Alignement: Erreur={np.degrees(diff):.1f}°, Seuil={threshold_deg}°, Aligné={aligned}")
    return aligned


################################################################################
PATH: ./core/ia/__init__.py
################################################################################


################################################################################
PATH: ./core/ia/planners/a_star.py
################################################################################
"""
Algorithme de Planification A*

Implémente la recherche de chemin A* sur la grille d'occupation :
- Trouve le chemin le plus court sans collision
- Utilise des heuristiques configurables
- Gère les obstacles dynamiques (robots)
- Retourne une liste de waypoints en mètres

Le planificateur travaille sur la carte de coût (costmap) gonflée de core/world.

Logs : [ASTAR] Chemin trouvé : N waypoints, longueur : M mètres
"""

import numpy as np
import heapq
from typing import List, Tuple, Optional
from .heuristics import euclidean_distance, manhattan_distance, diagonal_distance


class AStarPlanner:
    """
    Recherche de chemin A* sur grille d'occupation 2D.
    
    Trouve le chemin optimal du départ au but en évitant les obstacles.
    """
    
    def __init__(self, occupancy_grid, heuristic='euclidean'):
        """
        Initialise le planificateur A*.
        
        Args:
            occupancy_grid: OccupancyGrid depuis core/world
            heuristic: 'euclidean', 'manhattan', ou 'diagonal'
        """
        self.grid = occupancy_grid
        self.heuristic_name = heuristic
        
    def _heuristic(self, cell1, cell2):
        """Calcule le coût heuristique."""
        if self.heuristic_name == 'manhattan':
            return manhattan_distance(cell1, cell2)
        elif self.heuristic_name == 'diagonal':
            return diagonal_distance(cell1, cell2)
        else:
            return euclidean_distance(cell1, cell2)

    
    def plan(self, start_m: Tuple[float, float], 
             goal_m: Tuple[float, float]) -> Optional[List[Tuple[float, float]]]:
        """
        Trouve un chemin du départ au but.
        """
        start_cell = self.grid.world_to_grid(*start_m)
        goal_cell = self.grid.world_to_grid(*goal_m)
        
        print(f"[ASTAR] Planification de {start_m} vers {goal_m}")
        print(f"[ASTAR]   Cellule départ : {start_cell}, Cellule but : {goal_cell}")
        print(f"[ASTAR]   Taille grille : {self.grid.grid.shape}")
        
        # Vérification des limites
        if not self.grid._is_valid_cell(*start_cell):
            print(f"[ASTAR]   ERREUR : Cellule départ {start_cell} HORS LIMITES")
            return None
        if not self.grid._is_valid_cell(*goal_cell):
            print(f"[ASTAR]   ERREUR : Cellule but {goal_cell} HORS LIMITES")
            return None
        
        # Note : On ne vérifie pas si le départ ou le but est occupé
        # Départ : le robot est déjà là
        # But : le robot ennemi est là - on VEUT aller là-bas !
            
        # Initialisation
        open_set = []
        heapq.heappush(open_set, (0, start_cell))
        
        came_from = {}
        g_score = {start_cell: 0}
        f_score = {start_cell: self._heuristic(start_cell, goal_cell)}
        
        closed_set = set()
        
        while open_set:
            current = heapq.heappop(open_set)[1]
            
            if current == goal_cell:
                path_cells = self._reconstruct_path(came_from, current)
                simplified = self._simplify_path(path_cells)
                result = [self.grid.grid_to_world(r, c) for r, c in simplified]
                print(f"[ASTAR]   SUCCES : Chemin trouvé avec {len(result)} waypoints")
                return result
            
            closed_set.add(current)
            
            for neighbor, cost in self._get_neighbors(current, start_cell):
                if neighbor in closed_set:
                    continue
                    
                tentative_g = g_score[current] + cost
                
                if neighbor not in g_score or tentative_g < g_score[neighbor]:
                    came_from[neighbor] = current
                    g_score[neighbor] = tentative_g
                    f = tentative_g + self._heuristic(neighbor, goal_cell)
                    f_score[neighbor] = f
                    heapq.heappush(open_set, (f, neighbor))
        
        print(f"[ASTAR]   ECHEC : Aucun chemin trouvé (exploré {len(closed_set)} cellules)")
        return None
        
    def _get_neighbors(self, cell, start_cell=None, ignore_radius=5):
        """Obtient les cellules voisines valides (8-connexité).
        
        Utilise la COSTMAP (gonflée) pour la vérification de collision, pas la grille brute.
        Si start_cell est fourni et qu'on est dans ignore_radius,
        on ignore les obstacles (pour s'échapper de l'empreinte du robot).
        """
        row, col = cell
        neighbors = []
        
        # Utilise la costmap si disponible (obstacles gonflés), sinon grille
        collision_grid = getattr(self.grid, 'costmap', self.grid.grid)
        
        for dr in [-1, 0, 1]:
            for dc in [-1, 0, 1]:
                if dr == 0 and dc == 0:
                    continue
                    
                r, c = row + dr, col + dc
                
                # Vérifie la validité
                if self.grid._is_valid_cell(r, c):
                    # Si près du départ, ignore les obstacles (empreinte du robot)
                    if start_cell is not None:
                        dist_to_start = abs(r - start_cell[0]) + abs(c - start_cell[1])
                        if dist_to_start <= ignore_radius:
                            cost = 1.414 if (dr != 0 and dc != 0) else 1.0
                            neighbors.append(((r, c), cost))
                            continue
                    
                    # Vérifie l'occupation sur la COSTMAP (gonflée, marges de sécurité)
                    if collision_grid[r, c] < 0.5:
                        cost = 1.414 if (dr != 0 and dc != 0) else 1.0
                        neighbors.append(((r, c), cost))
                        
        return neighbors
    
    def _reconstruct_path(self, came_from, current):
        """Reconstruit le chemin du but au départ."""
        total_path = [current]
        while current in came_from:
            current = came_from[current]
            total_path.append(current)
        return total_path[::-1] # Inverse
    
    def _simplify_path(self, path_cells, epsilon=1.5):
        """
        Simplification du chemin via algorithme Douglas-Peucker.
        
        Args:
            path_cells: Liste de cellules (row, col)
            epsilon: Tolérance de simplification (en cellules)
            
        Returns:
            Liste simplifiée de cellules
        """
        if len(path_cells) <= 2:
            return path_cells
        
        # Trouve le point le plus éloigné de la ligne start-end
        start = np.array(path_cells[0])
        end = np.array(path_cells[-1])
        
        max_dist = 0
        max_idx = 0
        
        line_vec = end - start
        line_len = np.linalg.norm(line_vec)
        
        if line_len == 0:
            return [path_cells[0], path_cells[-1]]
        
        line_unit = line_vec / line_len
        
        for i in range(1, len(path_cells) - 1):
            point = np.array(path_cells[i])
            vec_to_point = point - start
            
            # Distance perpendiculaire à la ligne
            proj_length = np.dot(vec_to_point, line_unit)
            proj_point = start + proj_length * line_unit
            dist = np.linalg.norm(point - proj_point)
            
            if dist > max_dist:
                max_dist = dist
                max_idx = i
        
        # Si le point le plus éloigné dépasse epsilon, récursion
        if max_dist > epsilon:
            left = self._simplify_path(path_cells[:max_idx + 1], epsilon)
            right = self._simplify_path(path_cells[max_idx:], epsilon)
            return left[:-1] + right
        else:
            return [path_cells[0], path_cells[-1]]


################################################################################
PATH: ./core/ia/planners/heuristics.py
################################################################################
"""
Heuristiques - Fonctions de Coût pour la Planification de Chemin

Fournit des fonctions heuristiques pour A* et autres planificateurs :
- Distance Euclidienne (standard, admissible)
- Distance de Manhattan (basée grille)
- Distance Diagonale (Chebyshev + coût diagonal)
- Heuristiques personnalisées sensibles à la costmap

Toutes les heuristiques doivent être admissibles (ne jamais surestimer).
"""

import numpy as np
from typing import Tuple


def euclidean_distance(cell1: Tuple[int, int], cell2: Tuple[int, int]) -> float:
    """
    Heuristique de distance Euclidienne.
    
    La plus précise pour la planification en espace libre.
    
    Args:
        cell1: (lig, col)
        cell2: (lig, col)
        
    Returns:
        Distance Euclidienne
    """
    return np.sqrt((cell1[0] - cell2[0])**2 + (cell1[1] - cell2[1])**2)


def manhattan_distance(cell1: Tuple[int, int], cell2: Tuple[int, int]) -> float:
    """
    Distance de Manhattan (L1).
    
    Bon pour les grilles 4-connexes.
    
    Args:
        cell1: (lig, col)
        cell2: (lig, col)
        
    Returns:
        Distance de Manhattan
    """
    return abs(cell1[0] - cell2[0]) + abs(cell1[1] - cell2[1])


def diagonal_distance(cell1: Tuple[int, int], cell2: Tuple[int, int]) -> float:
    """
    Distance Diagonale (Chebyshev + coût diagonal).
    
    Bon pour les grilles 8-connexes avec coût diagonal √2.
    
    Args:
        cell1: (lig, col)
        cell2: (lig, col)
        
    Returns:
        Distance prenant en compte les diagonales
    """
    dx = abs(cell1[0] - cell2[0])
    dy = abs(cell1[1] - cell2[1])
    
    # Coût : mouvement diagonal (√2 ≈ 1.414) puis droit (1.0)
    D = 1.0  # Coût droit
    D2 = 1.414  # Coût diagonal
    
    return D * (dx + dy) + (D2 - 2*D) * min(dx, dy)


def costmap_aware_heuristic(cell1: Tuple[int, int], 
                            cell2: Tuple[int, int],
                            costmap) -> float:
    """
    Heuristique sensible à la costmap.
    
    Incorpore la proximité des obstacles dans l'heuristique.
    Reste admissible si la base est euclidienne.
    
    Args:
        cell1: (lig, col)
        cell2: (lig, col)
        costmap: OccupancyGrid avec inflation
        
    Returns:
        Heuristique modifiée favorisant les chemins plus sûrs
    """
    base_h = euclidean_distance(cell1, cell2)
    
    # Optionnel : ajouter une petite pénalité basée sur la valeur moyenne de la costmap
    # Doit rester admissible !
    
    return base_h


################################################################################
PATH: ./core/ia/planners/__init__.py
################################################################################


################################################################################
PATH: ./core/ia/planners/path_utils.py
################################################################################
"""
Utilitaires de Chemin - Traitement et Optimisation de Chemin

Utilitaires pour travailler avec les chemins planifiés :
- Lissage de chemin
- Simplification de waypoints (Douglas-Peucker)
- Validation de chemin
- Calcul de distance
- Interpolation

Prend la sortie brute de A* et la rend prête à l'exécution.
"""

import numpy as np
from typing import List, Tuple, Optional


def smooth_path(waypoints: List[Tuple[float, float]], 
                weight_data: float = 0.5,
                weight_smooth: float = 0.3,
                tolerance: float = 0.01,
                max_iterations: int = 100) -> List[Tuple[float, float]]:
    """
    Lisse un chemin en utilisant la descente de gradient.
    
    Équilibre entre rester proche du chemin original vs lissage.
    
    Args:
        waypoints: Chemin original [(x1,y1), ...]
        weight_data: À quel point rester proche de l'original
        weight_smooth: À quel point lisser
        tolerance: Seuil de convergence
        max_iterations: Itérations maximum
        
    Returns:
        Chemin lissé
    """
    if len(waypoints) <= 2:
        return waypoints
    
    # Convertit en array numpy pour manipulation plus facile
    path = np.array(waypoints, dtype=np.float64)
    smoothed = path.copy()
    
    n_points = len(path)
    
    for iteration in range(max_iterations):
        change = 0.0
        
        # Ne modifie pas le premier et le dernier point
        for i in range(1, n_points - 1):
            for j in range(2):  # x et y
                old_val = smoothed[i, j]
                
                # Terme de données : rester proche de l'original
                data_term = weight_data * (path[i, j] - smoothed[i, j])
                
                # Terme de lissage : moyenne des voisins
                smooth_term = weight_smooth * (
                    smoothed[i-1, j] + smoothed[i+1, j] - 2 * smoothed[i, j]
                )
                
                smoothed[i, j] += data_term + smooth_term
                change += abs(smoothed[i, j] - old_val)
        
        # Vérifie la convergence
        if change < tolerance:
            break
    
    return [(float(p[0]), float(p[1])) for p in smoothed]


def simplify_path_douglas_peucker(waypoints: List[Tuple[float, float]], 
                                  epsilon: float = 0.05) -> List[Tuple[float, float]]:
    """
    Simplifie le chemin avec l'algorithme Douglas-Peucker.
    
    Supprime les waypoints qui sont presque colinéaires.
    
    Args:
        waypoints: Chemin original
        epsilon: Tolérance de déviation maximale en mètres
        
    Returns:
        Chemin simplifié avec moins de waypoints
        
    Algorithme :
        1. Trouve le point le plus éloigné de la ligne entre départ et fin
        2. Si distance < epsilon, supprime tous les points intermédiaires
        3. Sinon, applique récursivement à [départ, plus_lointain] et [plus_lointain, fin]
    """
    if len(waypoints) <= 2:
        return waypoints
    
    # Convertit en numpy pour les calculs
    points = np.array(waypoints)
    
    # Trouve le point avec la distance maximale de la ligne (départ -> fin)
    start = points[0]
    end = points[-1]
    
    # Vecteur ligne
    line_vec = end - start
    line_len = np.linalg.norm(line_vec)
    
    if line_len < 1e-10:
        # Départ et fin sont le même point
        return [waypoints[0], waypoints[-1]]
    
    line_unit = line_vec / line_len
    
    # Trouve les distances perpendiculaires
    max_dist = 0.0
    max_idx = 0
    
    for i in range(1, len(points) - 1):
        # Vecteur du départ au point
        vec_to_point = points[i] - start
        
        # Projette sur la ligne
        proj_length = np.dot(vec_to_point, line_unit)
        proj_point = start + proj_length * line_unit
        
        # Distance perpendiculaire
        dist = np.linalg.norm(points[i] - proj_point)
        
        if dist > max_dist:
            max_dist = dist
            max_idx = i
    
    # Si la distance max est inférieure à epsilon, simplifie aux extrémités
    if max_dist < epsilon:
        return [waypoints[0], waypoints[-1]]
    
    # Sinon, simplifie récursivement
    left_simplified = simplify_path_douglas_peucker(waypoints[:max_idx + 1], epsilon)
    right_simplified = simplify_path_douglas_peucker(waypoints[max_idx:], epsilon)
    
    # Combine (évite de dupliquer le point de coupure)
    return left_simplified[:-1] + right_simplified


def validate_path(waypoints: List[Tuple[float, float]], 
                 occupancy_grid) -> bool:
    """
    Vérifie si le chemin est sans collision.
    
    Args:
        waypoints: Chemin à valider
        occupancy_grid: Grille d'occupation actuelle
        
    Returns:
        True si le chemin est valide (pas de collisions)
    """
    if len(waypoints) < 2:
        return True
    
    # Vérifie chaque waypoint
    for x, y in waypoints:
        if not (0 <= x <= occupancy_grid.width_m and 0 <= y <= occupancy_grid.height_m):
            return False
        if occupancy_grid.is_occupied(x, y):
            return False
    
    # Vérifie les segments de ligne entre waypoints
    resolution = occupancy_grid.resolution
    
    for i in range(len(waypoints) - 1):
        x1, y1 = waypoints[i]
        x2, y2 = waypoints[i + 1]
        
        # Echantillonne les points le long du segment
        dist = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)
        n_samples = max(2, int(dist / resolution) + 1)
        
        for t in np.linspace(0, 1, n_samples):
            x = x1 + t * (x2 - x1)
            y = y1 + t * (y2 - y1)
            
            if occupancy_grid.is_occupied(x, y):
                return False
    
    return True


def calculate_path_length(waypoints: List[Tuple[float, float]]) -> float:
    """
    Calcule la longueur totale du chemin en mètres.
    
    Args:
        waypoints: Chemin [(x1,y1), ...]
        
    Returns:
        Longueur totale en mètres
    """
    if len(waypoints) < 2:
        return 0.0
    
    length = 0.0
    for i in range(len(waypoints) - 1):
        dx = waypoints[i+1][0] - waypoints[i][0]
        dy = waypoints[i+1][1] - waypoints[i][1]
        length += np.sqrt(dx**2 + dy**2)
    
    return length


def interpolate_path(waypoints: List[Tuple[float, float]], 
                    resolution: float = 0.05) -> List[Tuple[float, float]]:
    """
    Densifie le chemin en interpolant entre les waypoints.
    
    Utile pour une visualisation fluide ou un contrôle fin.
    
    Args:
        waypoints: Chemin clairsemé
        resolution: Espacement désiré en mètres
        
    Returns:
        Chemin dense avec des points tous les ~resolution mètres
    """
    if len(waypoints) < 2:
        return waypoints
    
    dense_path = [waypoints[0]]
    
    for i in range(len(waypoints) - 1):
        x1, y1 = waypoints[i]
        x2, y2 = waypoints[i + 1]
        
        # Distance entre waypoints consécutifs
        dist = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)
        
        if dist < resolution:
            # Pas d'interpolation nécessaire
            dense_path.append((x2, y2))
            continue
        
        # Nombre de points intermédiaires
        n_points = int(dist / resolution)
        
        for j in range(1, n_points + 1):
            t = j / (n_points + 1)
            x = x1 + t * (x2 - x1)
            y = y1 + t * (y2 - y1)
            dense_path.append((x, y))
        
        dense_path.append((x2, y2))
    
    return dense_path


def get_path_curvature(waypoints: List[Tuple[float, float]]) -> List[float]:
    """
    Calcule la courbure à chaque waypoint.
    
    Utile pour l'adaptation de vitesse (ralentir aux virages serrés).
    
    Args:
        waypoints: Chemin [(x1,y1), ...]
        
    Returns:
        Liste des valeurs de courbure (1/rayon, 0 pour ligne droite)
    """
    if len(waypoints) < 3:
        return [0.0] * len(waypoints)
    
    curvatures = [0.0]  # Le premier point n'a pas de courbure
    
    for i in range(1, len(waypoints) - 1):
        p0 = np.array(waypoints[i - 1])
        p1 = np.array(waypoints[i])
        p2 = np.array(waypoints[i + 1])
        
        # Vecteurs
        v1 = p1 - p0
        v2 = p2 - p1
        
        # Magnitude produit vectoriel (2D)
        cross = abs(v1[0] * v2[1] - v1[1] * v2[0])
        
        # Longueurs segments
        len1 = np.linalg.norm(v1)
        len2 = np.linalg.norm(v2)
        
        if len1 < 1e-10 or len2 < 1e-10:
            curvatures.append(0.0)
            continue
        
        # Approximation courbure : 2 * sin(angle) / longueur_corde
        # Simplifié : cross / (len1 * len2)
        curvature = cross / (len1 * len2)
        curvatures.append(curvature)
    
    curvatures.append(0.0)  # Le dernier point n'a pas de courbure
    
    return curvatures


################################################################################
PATH: ./core/ia/strategy.py
################################################################################
"""
Stratégie - Orchestrateur IA (Architecture Task-Based)

Rôle :
1. Interroge le Cerveau (Behavior Tree) pour connaître l'Intention (RETRAIT, ATTAQUE, POURSUITE).
2. Active la Tâche correspondante (FleeTask, AttackTask, NavigationTask).
3. Délègue l'exécution à la tâche active.
"""

from typing import Dict, Optional
from .behavior_tree import TankBehaviorTree
from core.tasks.navigation import NavigationTask
from core.tasks.survival import FleeTask
from core.tasks.combat import AttackTask
from core.ia.tactical import TacticalAnalyzer

class AIStrategy:
    def __init__(self, config):
        self.config = config
        self.brain = None
        self.world_model = None
        
        # Gestion des Tâches
        self.tasks = {
            'NAV': NavigationTask(target_type='ENEMY'),
            'FLEE': FleeTask(),
            'ATTACK': AttackTask()
        }
        self.current_task = self.tasks['NAV'] # Par défaut
        self.current_state = "INITIALISATION"
        
        # Pour visualisation
        self.current_path = [] 
        
        print("[AI] Stratégie v3 (Task-Based) : Initialisée")

    def set_planner(self, grid):
        pass # Obsolète mais gardé pour compatibilité

    def set_world_model(self, world_model):
        self.world_model = world_model
        # Le BT a besoin du world model
        bt_conf = self.config.get('behavior_tree', {})
        self.brain = TankBehaviorTree(world_model, config=bt_conf)
        
        # Certaines tâches ont besoin d'init (ex: TacticalAnalyzer pour Flee)
        # Mais elles le font en lazy loading dans execute(), donc c'est ok.

    def decide(self, world_state: Dict) -> Dict:
        # Sortie par défaut
        decision = {
            'state': "IDLE", 'fire_request': False, 
            'target_position': None, 'target_orientation': None, 'has_los': False,
            'v': 0.0, 'w': 0.0 # Nouvelles sorties directes !
        }

        ai_pose = world_state.get('ai_pose')
        enemy_pose = world_state.get('human_pose')
        
        if not ai_pose or not enemy_pose or not self.brain:
            return decision

        # 1. Cerveau : QUOI FAIRE ?
        context = {'ai_pose': ai_pose, 'human_pose': enemy_pose}
        bt_output = self.brain.execute(context)
        new_state = bt_output['state']
        
        # 2. Switching de Tâche
        # Mapping État BT -> Tâche
        if "RETRAIT" in new_state:
            self._switch_task('FLEE')
        elif "ATTAQUE" in new_state:
            # Subtilité : "ATTAQUE (ALIGNEMENT)" vs "ATTAQUE (VERROUILLÉ)"
            # Les deux sont gérés par AttackTask (qui vise puis tire)
            self._switch_task('ATTACK')
        elif "POURSUITE" in new_state:
            self._switch_task('NAV')
        else:
             # IDLE ou RECHERCHE -> Navigation par défaut (ou une tache SearchTask future)
             self._switch_task('NAV')
             
        # 3. Exécution Tâche : COMMENT LE FAIRE ?
        # On enrichit le contexte avec world et dt (approximatif ici, 
        # idéalement passé par run_game, mais BaseTask gère sans dt critique pour l'instant)
        task_context = {
            'ai_pose': ai_pose,
            'human_pose': enemy_pose,
            'world': self.world_model,
            'dt': 0.033 # Dummy dt, utilisé par Kalman interne si besoin
        }
        
        cmd = self.current_task.execute(task_context)
        
        # 4. Packaging Sortie
        decision['state'] = new_state
        decision['v'] = cmd.get('v', 0.0)
        decision['w'] = cmd.get('w', 0.0)
        decision['fire_request'] = cmd.get('fire_request', False)
        
        # Info debug pour le renderer
        debug = cmd.get('debug_info', {})
        self.current_path = debug.get('path', []) # Pour affichage chemin
        decision['target_position'] = debug.get('target') # Pour affichage cible
        decision['debug_info'] = debug # NEW: Propagation complète pour le debugger
        
        return decision

    def _switch_task(self, task_key):
        if self.tasks[task_key] != self.current_task:
            print(f"[AI] Switch Task : {self.current_task.name} -> {self.tasks[task_key].name}")
            self.current_task = self.tasks[task_key]
            # On pourrait appeler self.current_task.reset() ici

    # Interface Legacy pour run_game.py (affichage chemin)
    def get_full_path(self):
        return self.current_path
        
    def advance_waypoint(self):
        # Plus nécessaire car géré par NavigationTask en interne
        pass


################################################################################
PATH: ./core/ia/tactical.py
################################################################################
"""
Module Tactique IA

Gère l'analyse de l'environnement pour prendre des décisions stratégiques
comme la fuite, le contournement ou le choix de position de tir.
"""

import numpy as np

class TacticalAnalyzer:
    """
    Analyseur tactique pour le robot Tank.
    """
    def __init__(self, world_model):
        """
        Args:
            world_model: Instance de core.world.world_model.WorldModel
        """
        self.world = world_model

    def find_safest_zone(self, enemy_pose, segments_x=5, segments_y=4):
        """
        Trouve la zone la plus éloignée de l'ennemi dans l'arène.
        
        Args:
            enemy_pose: (x, y, theta) de l'ennemi
            segments_x: Nombre de divisions en X
            segments_y: Nombre de divisions en Y
            
        Returns:
            (x, y) de la zone la plus sûre ou None si rien trouvé
        """
        if enemy_pose is None:
            return None
            
        max_dist = -1
        best_zone = None
        
        # Dimensions des secteurs
        sector_w = self.world.arena_width / segments_x
        sector_h = self.world.arena_height / segments_y
        
        for ix in range(segments_x):
            for iy in range(segments_y):
                # Centre du secteur
                cx = (ix + 0.5) * sector_w
                cy = (iy + 0.5) * sector_h
                
                # Vérifie si le point est accessible (pas dans un mur)
                if self.world.is_position_valid(cx, cy):
                    dist = np.hypot(cx - enemy_pose[0], cy - enemy_pose[1])
                    
                    if dist > max_dist:
                        max_dist = dist
                        best_zone = (cx, cy)
                        
        return best_zone


################################################################################
PATH: ./core/__init__.py
################################################################################
"""Package principal de logique metier pour Tank Arena."""

__all__ = []


################################################################################
PATH: ./core/tasks/base_task.py
################################################################################
import sys
import os
import pygame
import numpy as np
import time
import yaml
import cv2
from pathlib import Path
from abc import ABC, abstractmethod

# Imports système de base nécessaires au standalone
from perception.camera.realsense_stream import RealSenseStream
from perception.camera.aruco_detector import ArucoDetector
from core.world.world_model import WorldModel
from core.world.unified_transform import load_calibration
from core.control.ros_bridge_client import ROSBridgeClient

class BaseTask(ABC):
    """
    Interface de base pour toutes les tâches du robot.
    Permet l'exécution intégrée (via Strategy) ou standalone (via __main__).
    """
    
    def __init__(self, name: str, config: dict = None):
        self.name = name
        self.config = config or {}
        
    @abstractmethod
    def execute(self, context: dict) -> dict:
        """
        Exécute la logique de la tâche pour un tick.
        
        Args:
            context: Dictionnaire contenant l'état du monde
                - 'ai_pose': (x, y, theta)
                - 'human_pose': (x, y, theta)
                - 'world': WorldModel instance
                - 'dt': delta time (s)
                
        Returns:
            dict: Commandes pour le robot
                - 'v': Vitesse linéaire (m/s)
                - 'w': Vitesse angulaire (rad/s)
                - 'fire_request': bool
                - 'status': str (RUNNING, SUCCESS, FAILURE)
                - 'debug_info': dict (pour affichage)
        """
        pass

    def draw(self, surface, transform_mgr, debug_info):
        """
        Méthode optionnelle pour dessiner des infos de debug sur l'écran Pygame.
        """
        pass

    @classmethod
    def run_standalone(cls):
        """
        Lance la tâche en mode autonome avec toute l'infrastructure (Caméra, Pygame, ROS).
        Copie intelligemment la logique de test de component_test.
        """
        print(f"[TASK] Lancement Standalone : {cls.__name__}")
        
        # 1. Config & Calibration
        root_dir = Path(__file__).parent.parent.parent
        config_dir = root_dir / 'config'
        transform_mgr = load_calibration(str(config_dir))
        
        proj_conf = {}
        if (config_dir / 'projector.yaml').exists():
            with open(config_dir / 'projector.yaml') as f:
                proj_conf = yaml.safe_load(f)
        
        # 2. Setup Pygame
        os.environ['SDL_VIDEO_WINDOW_POS'] = "{},{}".format(
            proj_conf.get('display', {}).get('monitor_offset_x', 1920),
            0
        )
        pygame.init()
        screen = pygame.display.set_mode((1024, 768), pygame.NOFRAME)
        clock = pygame.time.Clock()
        font = pygame.font.SysFont("Consolas", 18)

        # 3. Hardware
        camera = RealSenseStream(width=1280, height=720, fps=30)
        camera.start()
        aruco = ArucoDetector()
        
        # 4. World Model
        world = WorldModel(transform_mgr.arena_width_m, transform_mgr.arena_height_m)
        world.generate_costmap()
        
        # 5. Robot Connection
        ros_bridge = ROSBridgeClient(host='127.0.0.1', port=8765)
        connected = ros_bridge.connect()
        if not connected:
            print("[WARN] Robot non connecté (Mode Simulation)")

        # 6. Instanciation de la Tâche
        task = cls() # Suppose constructeur par défaut
        
        print("\n[INFO] Tâche démarrée. ESC pour quitter.\n")
        
        running = True
        try:
            while running:
                dt = clock.tick(30) / 1000.0
                screen.fill((10, 10, 30))
                
                # Input
                for event in pygame.event.get():
                    if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE:
                        running = False
                
                # Perception
                color_frame, _ = camera.get_frames()
                if color_frame is None: continue
                detections = aruco.detect(color_frame)
                
                ai_pose = None
                human_pose = None
                
                # Note: IDs codés en dur pour le standalone (4=AI, 5=Enemy)
                if 5 in detections:
                    ai_pose = cls._get_pose(detections[5], transform_mgr)
                if 4 in detections:
                    human_pose = cls._get_pose(detections[4], transform_mgr)
                
                # Context Build
                context = {
                    'ai_pose': ai_pose, # Standalone inverse souvent les IDS, à vérifier
                    'human_pose': human_pose,
                    'world': world,
                    'dt': dt
                }
                
                # --- EXECUTION TÂCHE ---
                # Dans component_test, AI=5 et Enemy=4. 
                # Adaptons le mapping pour que le standalone matche le jeu
                # Jeu : AI=4, Enemy=5.
                # Component Test : AI=5, Enemy=4.
                # On va dire : Robot contrôlé = celui défini dans la tache
                
                cmd = task.execute(context)
                
                # --- COMMANDE ROBOT ---
                if connected and ai_pose: # Si on a détecté notre robot
                     # Attention aux IDs ! Dans component_test AI=5.
                     # On envoie la commande au robot 5
                     ros_bridge.send_velocity_command(5, cmd.get('v', 0), cmd.get('w', 0))
                
                # --- RENDU ---
                # Dessin basique robots
                if ai_pose:
                    px = transform_mgr.world_to_projector(ai_pose[0], ai_pose[1])
                    pygame.draw.circle(screen, (0, 255, 255), px, 15, 2)
                if human_pose:
                    px = transform_mgr.world_to_projector(human_pose[0], human_pose[1])
                    pygame.draw.circle(screen, (255, 50, 50), px, 15, 2)
                    
                # Dessin Tâche
                task.draw(screen, transform_mgr, cmd.get('debug_info', {}))
                
                # Texte Status
                txt = f"Status: {cmd.get('status', 'N/A')} | v={cmd.get('v',0):.2f} w={cmd.get('w',0):.2f}"
                screen.blit(font.render(txt, True, (255, 255, 255)), (10, 10))
                
                pygame.display.flip()
                
        except KeyboardInterrupt:
            pass
        finally:
            if connected: ros_bridge.send_velocity_command(5, 0, 0)
            camera.stop()
            pygame.quit()

    @staticmethod
    def _get_pose(data, tm):
        # Utilitaire rapide pour standalone
        u, v = data['center']
        th_pix = data['orientation']
        x, y = tm.camera_to_world(u, v)
        u_f = u + 20 * np.cos(th_pix)
        v_f = v + 20 * np.sin(th_pix)
        xf, yf = tm.camera_to_world(u_f, v_f)
        th = np.arctan2(yf-y, xf-x)
        return (x, y, th)


################################################################################
PATH: ./core/tasks/brain.py
################################################################################
from core.tasks.base_task import BaseTask
from core.ia.strategy import AIStrategy
import pygame
import yaml
from pathlib import Path

class BrainTask(BaseTask):
    """
    Tâche Maître "Cerveau" : Exécute toute la pile IA (Vision -> BT -> Tâche).
    Permet de visualiser l'état mental du robot et la tâche active.
    """
    def __init__(self):
        super().__init__("MasterBrain")
        
        # Chargement Config IA
        root_dir = Path(__file__).parent.parent.parent
        config_path = root_dir / 'config' / 'ia.yaml'
        
        ia_config = {}
        if config_path.exists():
            with open(config_path) as f:
                ia_config = yaml.safe_load(f)
        
        # Instanciation Stratégie
        self.strategy = AIStrategy(ia_config)
        self.last_decision = {}
        
    def execute(self, context):
        """Délegue tout à la stratégie."""
        
        # Init Late (besoin du world model)
        if self.strategy.world_model is None:
            self.strategy.set_world_model(context['world'])
            
        # Appel Stratégie
        # Note: 'context' de BaseTask est compatible avec 'world_state' de Strategy
        decision = self.strategy.decide(context)
        self.last_decision = decision
        
        # Mapping Sortie Strategy -> IHM BaseTask
        cmd = {
            'v': decision.get('v', 0.0),
            'w': decision.get('w', 0.0),
            'fire_request': decision.get('fire_request', False),
            'status': decision.get('state', 'UNKNOWN'),
            # On passe tout le decision pack comme debug info pour le draw
            'debug_info': decision 
        }
        return cmd

    def draw(self, surface, tm, debug_info):
        # 1. Dessin de la Sous-Tâche Active
        # On récupère la tâche active via la stratégie
        current_task = self.strategy.current_task
        
        # On extrait les infos debug spécifiques à la sous-tâche
        sub_debug_info = debug_info.get('debug_info', {}) 
        
        if current_task:
            current_task.draw(surface, tm, sub_debug_info)
            
        # 2. Overlay "Cerveau" (État, Intention)
        font_big = pygame.font.SysFont("Impact", 30)
        font_small = pygame.font.SysFont("Consolas", 18)
        
        state = debug_info.get('state', 'N/A')
        task_name = current_task.name if current_task else "None"
        
        # Couleur selon état
        color = (200, 200, 200)
        if "ATTAQUE" in state: color = (255, 50, 50)
        elif "RETRAIT" in state: color = (50, 255, 50)
        elif "POURSUITE" in state: color = (50, 100, 255)
        
        # Rendu Texte
        y = 50
        lines = [
            (f"BRAIN: {state}", color, font_big),
            (f"TASK:  {task_name}", (255, 255, 255), font_small),
        ]
        
        for text, col, fnt in lines:
            s = fnt.render(text, True, col)
            surface.blit(s, (10, y))
            y += s.get_height() + 5

if __name__ == '__main__':
    BrainTask.run_standalone()


################################################################################
PATH: ./core/tasks/combat.py
################################################################################
from .base_task import BaseTask
import numpy as np
import pygame
import time

class AttackTask(BaseTask):
    """
    Tâche d'Attaque (Sniper) : Tourne sur place et tire si aligné.
    Basé sur component_test/point_and_tir.py
    """
    def __init__(self):
        super().__init__("Combat")
        self.align_precision = 3.0 # Degrés
        self.rotation_gain = 1.5
        self.last_shot_time = 0
        self.shoot_cooldown = 1.5
        
    def execute(self, context):
        ai_pose = context.get('ai_pose')
        enemy_pose = context.get('human_pose')
        
        cmd = {'v': 0.0, 'w': 0.0, 'fire_request': False, 'status': 'AIMING'}
        
        if not ai_pose or not enemy_pose:
            cmd['status'] = 'NO_TARGET'
            return cmd
            
        # Calcul angle
        dx = enemy_pose[0] - ai_pose[0]
        dy = enemy_pose[1] - ai_pose[1]
        desired_theta = np.arctan2(dy, dx)
        
        error_theta = desired_theta - ai_pose[2]
        # Normalize
        error_theta = (error_theta + np.pi) % (2 * np.pi) - np.pi
        error_deg = abs(np.degrees(error_theta))
        
        # Commande P (Signe inverse comme dans point_and_tir.py)
        w = np.clip(-self.rotation_gain * error_theta, -1.0, 1.0)
        
        # Deadband & Tir
        if error_deg < self.align_precision:
            w = 0.0 # Stop pour tirer
            cmd['status'] = 'LOCKED'
            
            # Gestion Tir
            current_time = time.time()
            if (current_time - self.last_shot_time) > self.shoot_cooldown:
                cmd['fire_request'] = True
                self.last_shot_time = current_time
                print("[TASK] FEU !")
        
        cmd['w'] = w
        # Pass poses for drawing
        cmd['debug_info'] = {
            'locked': (error_deg < self.align_precision), 
            'error': error_deg,
            'ai_pose': ai_pose,
            'enemy_pose': enemy_pose
        }
        return cmd

    def draw(self, surface, tm, debug_info):
        locked = debug_info.get('locked', False)
        ai_pose = debug_info.get('ai_pose')
        enemy_pose = debug_info.get('enemy_pose')
        error_deg = debug_info.get('error', 0.0)
        
        color = (0, 255, 0) if locked else (255, 0, 0)
        
        # Dessin Laser (Ligne de visée)
        if ai_pose and enemy_pose:
            p1 = tm.world_to_projector(ai_pose[0], ai_pose[1])
            p2 = tm.world_to_projector(enemy_pose[0], enemy_pose[1])
            pygame.draw.line(surface, color, p1, p2, 2 if locked else 1)
            
            # Petit réticule sur l'ennemi
            pygame.draw.circle(surface, color, p2, 5, 1)

        # Indicateur HUD
        font = pygame.font.SysFont("Impact", 20)
        
        lines = [
            f"STATUS: {'LOCKED' if locked else 'AIMING'}",
            f"ERROR:  {error_deg:.1f} deg"
        ]
        
        y = 40
        for line in lines:
            txt = font.render(line, True, color)
            surface.blit(txt, (10, y))
            y += 25

if __name__ == '__main__':
    AttackTask.run_standalone()


################################################################################
PATH: ./core/tasks/navigation.py
################################################################################
from .base_task import BaseTask
import numpy as np
import pygame
from core.ia.planners.a_star import AStarPlanner
from core.control.trajectory_follower import TrajectoryFollower

class NavigationTask(BaseTask):
    """
    Tâche de Navigation : Aller vers une cible (Ennemi ou Point fixe).
    Basé sur component_test/go_to_point.py
    """
    def __init__(self, target_type='ENEMY', safety_dist=0.40):
        super().__init__("Navigation")
        self.target_type = target_type # 'ENEMY' ou 'FIXED'
        self.safety_dist = safety_dist
        self.planner = None
        self.controller = TrajectoryFollower({
            'lookahead_distance_m': 0.15,
            'k_velocity': 0.15,
            'waypoint_threshold_m': 0.05,
            'max_linear_mps': 0.20,
            'max_angular_radps': 1.0
        })
        self.current_path = []
        self.current_wp_idx = 0
        self.replan_timer = 0
        
    def execute(self, context):
        ai_pose = context.get('ai_pose')
        enemy_pose = context.get('human_pose')
        world = context.get('world')
        
        cmd = {'v': 0.0, 'w': 0.0, 'status': 'RUNNING'}
        
        if not ai_pose or not enemy_pose:
            cmd['status'] = 'WAITING_POSE'
            return cmd

        # Init Planner (Lazy)
        if hasattr(world, 'grid') and self.planner is None:
             self.planner = AStarPlanner(world.grid, heuristic='euclidean')

        # 1. Calcul Cible
        target_pos = None
        if self.target_type == 'ENEMY':
            dx, dy = ai_pose[0] - enemy_pose[0], ai_pose[1] - enemy_pose[1]
            dist = np.hypot(dx, dy)
            if dist > 0.01:
                ux, uy = dx/dist, dy/dist
            else:
                ux, uy = 1, 0
            
            # Point à safety_dist de l'ennemi (entre lui et nous)
            target_pos = (enemy_pose[0] + ux * self.safety_dist,
                          enemy_pose[1] + uy * self.safety_dist)
            
        # 2. Replanification (tous les 30 ticks ou si path vide)
        self.replan_timer += 1
        if self.replan_timer > 30 or not self.current_path:
            self.replan_timer = 0
            if self.planner:
                path = self.planner.plan(ai_pose[:2], target_pos)
                if path:
                    # Smoothing simple (downsample)
                    self.current_path = path[::2] if len(path) > 4 else path
                    if self.current_path[-1] != target_pos:
                        self.current_path.append(target_pos)
                    self.current_wp_idx = 0
                else:
                    # Fallback direct
                    self.current_path = [target_pos]
                    self.current_wp_idx = 0

        # 3. Suivi de chemin
        if self.current_path and self.current_wp_idx < len(self.current_path):
            wp = self.current_path[self.current_wp_idx]
            
            # Check waypoint reached
            dist_wp = np.hypot(ai_pose[0]-wp[0], ai_pose[1]-wp[1])
            if dist_wp < 0.05:
                self.current_wp_idx += 1
                if self.current_wp_idx >= len(self.current_path):
                    cmd['status'] = 'SUCCESS'
                    return cmd # Stop
            
            # Compute Control
            future_path = self.current_path[self.current_wp_idx:]
            v, w = self.controller.compute_control(ai_pose, future_path)
            cmd['v'] = v
            cmd['w'] = w
            
        cmd['debug_info'] = {'path': self.current_path, 'target': target_pos}
        return cmd

    def draw(self, surface, tm, debug_info):
        path = debug_info.get('path', [])
        if path and len(path) > 1:
            px_points = [tm.world_to_projector(p[0], p[1]) for p in path]
            pygame.draw.lines(surface, (0, 255, 100), False, px_points, 3)
            
        target = debug_info.get('target')
        if target:
            px = tm.world_to_projector(target[0], target[1])
            pygame.draw.circle(surface, (255, 255, 0), px, 8, 2)

if __name__ == '__main__':
    NavigationTask.run_standalone()


################################################################################
PATH: ./core/tasks/survival.py
################################################################################
from .base_task import BaseTask
from .navigation import NavigationTask
import numpy as np
import pygame
from core.ia.tactical import TacticalAnalyzer

class FleeTask(NavigationTask):
    """
    Tâche de Fuite : Trouve une zone sûre et s'y rend.
    Basé sur component_test/run_away.py
    Hérite de NavigationTask pour réutiliser le code de déplacement A*.
    """
    def __init__(self):
        super().__init__(target_type='FIXED') # On fixe la cible manuellement
        self.name = "Survival"
        self.tactical = None
        self.safe_zone = None
        
    def execute(self, context):
        world = context.get('world')
        enemy_pose = context.get('human_pose')
        
        if self.tactical is None and world:
            self.tactical = TacticalAnalyzer(world)
            
        # Mise à jour de la zone sûre (périodique)
        # On utilise le même timer que la nav
        if self.replan_timer == 0 or self.safe_zone is None:
            if self.tactical and enemy_pose:
                new_zone = self.tactical.find_safest_zone(enemy_pose)
                if new_zone:
                    if self.safe_zone:
                        # Hysteresis: Only switch if significantly better
                        current_dist = np.hypot(self.safe_zone[0] - enemy_pose[0], self.safe_zone[1] - enemy_pose[1])
                        new_dist = np.hypot(new_zone[0] - enemy_pose[0], new_zone[1] - enemy_pose[1])
                        
                        if new_dist > current_dist + 0.20: # 20cm improvement required
                            self.safe_zone = new_zone
                    else:
                        self.safe_zone = new_zone
                
        # Hack pour utiliser NavigationTask : on surcharge la logique de "Calcul Cible"
        # NavigationTask.execute va appeler son planner vers 'target_pos'
        # On va ruser en copiant le code de NavigationTask mais avec notre cible
        
        # Pour faire propre, on réutilise simplement la logique de NavigationTask
        # mais on injecte notre cible dans un "faux" context via une propriété
        # Ou mieux, on réimplémente juste la partie target selection.
        
        # Approche "Override" :
        # On définit target_pos pour que le replanner de NavigationTask l'utilise
        # Mais NavigationTask recalcule target_pos localement si target_type='ENEMY'
        # Ici target_type='FIXED', donc on doit gérer la mise à jour de self.current_target ? 
        # Non, NavigationTask est un peu rigide. Réimplémentons Execute proprement.
        
        ai_pose = context.get('ai_pose')
        cmd = {'v': 0.0, 'w': 0.0, 'status': 'RUNNING'}
        
        if not ai_pose or not self.safe_zone:
            cmd['status'] = 'SEARCHING_ZONE'
            return cmd

        # Init Planner
        if hasattr(world, 'grid') and self.planner is None:
             from core.ia.planners.a_star import AStarPlanner
             self.planner = AStarPlanner(world.grid, heuristic='euclidean')

        # Replan
        self.replan_timer += 1
        if self.replan_timer > 30 or not self.current_path:
            self.replan_timer = 0
            if self.planner:
                path = self.planner.plan(ai_pose[:2], self.safe_zone)
                if path:
                    self.current_path = path[::2] if len(path) > 4 else path
                    if self.current_path[-1] != self.safe_zone:
                         self.current_path.append(self.safe_zone)
                    self.current_wp_idx = 0
        
        # Suivi (copié de NavigationTask)
        if self.current_path and self.current_wp_idx < len(self.current_path):
            wp = self.current_path[self.current_wp_idx]
            dist_wp = np.hypot(ai_pose[0]-wp[0], ai_pose[1]-wp[1])
            if dist_wp < 0.05: # Aligned with NavigationTask
                self.current_wp_idx += 1
                if self.current_wp_idx >= len(self.current_path):
                    cmd['status'] = 'SAFE'
                    return cmd
            
            future_path = self.current_path[self.current_wp_idx:]
            v, w = self.controller.compute_control(ai_pose, future_path)
            # Fuite : on peut aller un peu plus vite ?
            cmd['v'] = v
            cmd['w'] = w
            
        cmd['debug_info'] = {'path': self.current_path, 'safe_zone': self.safe_zone}
        return cmd

    def draw(self, surface, tm, debug_info):
        super().draw(surface, tm, debug_info)
        sz = debug_info.get('safe_zone')
        if sz:
            px = tm.world_to_projector(sz[0], sz[1])
            pygame.draw.circle(surface, (0, 255, 0), px, 40, 2) # Grand cercle vert

if __name__ == '__main__':
    FleeTask.run_standalone()


################################################################################
PATH: ./core/utils/state_file.py
################################################################################
import json
import os
import tempfile
import time
import shutil

class StateFile:
    """
    Gère la lecture et l'écriture atomique d'un fichier d'état JSON.
    Garantit que les lecteurs ne voient jamais un fichier partiel.
    """
    def __init__(self, filepath):
        self.filepath = filepath
        self.last_read_time = 0
        self.cached_state = None

    def write(self, state):
        """
        Ecrit l'état de manière atomique :
        1. Ecrit dans un fichier temporaire
        2. Renomme le fichier temporaire (opération atomique sur Linux/POSIX)
        """
        # Creer un fichier temporaire dans le meme repertoire (important pour le rename atomique)
        directory = os.path.dirname(self.filepath)
        if not os.path.exists(directory):
            os.makedirs(directory, exist_ok=True)
            
        fd, temp_path = tempfile.mkstemp(dir=directory, text=True)
        try:
            with os.fdopen(fd, 'w') as f:
                json.dump(state, f)
            
            # Atomic replace
            os.replace(temp_path, self.filepath)
        except Exception as e:
            if os.path.exists(temp_path):
                os.remove(temp_path)
            raise e

    def read(self):
        """
        Lit l'état courant. Retourne None si le fichier n'existe pas ou est corrompu (rare avec atomic write).
        """
        if not os.path.exists(self.filepath):
            return None
            
        try:
            with open(self.filepath, 'r') as f:
                return json.load(f)
        except json.JSONDecodeError:
            # Peut arriver si on lit pile pendant le très court instant du swap (très rare)
            # ou si le disque est plein.
            print(f"[StateFile] Warning: JSON Decode Error on {self.filepath}")
            return None


################################################################################
PATH: ./core/world/coordinate_frames.py
################################################################################
"""
Trames de Coordonnées - Gestion des Transformations

Gère toutes les transformations de coordonnées :
- Caméra -> Arène Virtuelle (H_C2AV) - homographie depuis les coins projetés
- Arène Virtuelle -> Monde (mise à l'échelle) - étalonnage métrique
- Caméra -> Monde (H_C2W) - transformation combinée
- Monde -> Pygame (affichage projection)
- Monde -> Projecteur (affichage superposition)

Toutes les transformations sont des homographies 2D ou des transformations affines.

Logs : préfixe [TRANSFORM] pour toutes les opérations de transformation
"""

import numpy as np
import cv2
from typing import Tuple, List


class TransformManager:
    """
    Gère les transformations de trames de coordonnées.
    
    Stocke et applique les homographies entre différents systèmes de coordonnées.
    """
    
    def __init__(self):
        """Initialise le gestionnaire de transformations."""
        self.H_C2AV = None  # Caméra -> Arène Virtuelle
        self.H_AV2W = None  # Arène Virtuelle -> Monde (échelle)
        self.H_C2W = None   # Caméra -> Monde (combiné)
        self.H_W2Proj = None  # Monde -> Projecteur (affichage)
        
        self.scale_m_per_av = None  # Metric scale factor
        
    def set_camera_to_av(self, src_points: np.ndarray, dst_points: np.ndarray):
        """
        Calcule H_C2AV à partir des correspondances de coins.
        
        Args:
            src_points: Tableau 4x2 de coordonnées pixels caméra
            dst_points: Tableau 4x2 de coordonnées arène virtuelle (ex: carré unitaire)
            
        Calcule l'homographie utilisant cv2.findHomography.
        
        Logs:
            [TRANSFORM] H_C2AV calculé à partir de 4 coins
        """
        self.H_C2AV, _ = cv2.findHomography(src_points, dst_points)
        self._update_combined()
        
    def set_av_to_world_scale(self, scale: float):
        """
        Définit l'échelle de l'Arène Virtuelle vers le Monde (mètres).
        
        Args:
            scale: mètres par unité AV
            
        Crée la transformation d'échelle H_AV2W.
        
        Logs:
            [TRANSFORM] Echelle définie : 1.15 m/unité_AV
        """
        self.scale_m_per_av = scale
        self.H_AV2W = np.array([
            [scale, 0, 0],
            [0, scale, 0],
            [0, 0, 1]
        ], dtype=np.float32)
        self._update_combined()
        
    def _update_combined(self):
        """Update H_C2W = H_AV2W @ H_C2AV."""
        if self.H_C2AV is not None and self.H_AV2W is not None:
            self.H_C2W = self.H_AV2W @ self.H_C2AV
            
    def camera_to_world(self, u: float, v: float) -> Tuple[float, float]:
        """
        Transforme pixel caméra en mètres monde.
        
        Args:
            u, v: Coordonnées pixel caméra
            
        Returns:
            (x, y) en mètres
        """
        if self.H_C2W is None:
            raise ValueError("H_C2W non défini, lancez l'étalonnage d'abord")
        
        # Coordonnées homogènes
        p_cam = np.array([u, v, 1.0])
        p_world = self.H_C2W @ p_cam
        
        # Normalisation
        x = p_world[0] / p_world[2]
        y = p_world[1] / p_world[2]
        
        return (x, y)
    
    def world_to_projector(self, x: float, y: float, 
                          arena_width_m: float, arena_height_m: float,
                          proj_width_px: int, proj_height_px: int,
                          margin_px: int = 50) -> Tuple[int, int]:
        """
        Transforme position monde en pixel projecteur.
        
        Args:
            x, y: Position monde en mètres
            arena_width_m, arena_height_m: Taille arène
            proj_width_px, proj_height_px: Résolution projecteur
            margin_px: Marge de sécurité
            
        Returns:
            (px, py) coordonnées pixel projecteur
        """
        # Mise à l'échelle vers projecteur (avec marge)
        draw_width = proj_width_px - 2 * margin_px
        draw_height = proj_height_px - 2 * margin_px
        
        scale_x = draw_width / arena_width_m
        scale_y = draw_height / arena_height_m
        scale = min(scale_x, scale_y)  # Conserve le ratio d'aspect
        
        px = margin_px + int(x * scale)
        py = margin_px + int((arena_height_m - y) * scale)  # Inverse Y (origine pygame en haut à gauche)
        
        return (px, py)
    
    def batch_camera_to_world(self, points_cam: np.ndarray) -> np.ndarray:
        """
        Transforme plusieurs points caméra vers monde.
        
        Args:
            points_cam: Tableau Nx2 de coordonnées caméra
            
        Returns:
            Tableau Nx2 de coordonnées monde
        """
        if self.H_C2W is None:
            raise ValueError("H_C2W non défini")
        
        # Ajoute coordonnée homogène
        ones = np.ones((points_cam.shape[0], 1))
        points_h = np.hstack([points_cam, ones])
        
        # Transform
        points_world_h = (self.H_C2W @ points_h.T).T
        
        # Normalisation
        points_world = points_world_h[:, :2] / points_world_h[:, 2:3]
        
        return points_world


################################################################################
PATH: ./core/world/inflation.py
################################################################################
"""
Inflation - Gonflement du Coût des Obstacles

Gonfle les obstacles dans la costmap pour une planification de chemin sûre :
- Ajoute une marge de sécurité autour des obstacles
- Crée un gradient pour une planification plus douce
- Prend en compte la taille du robot

Utilise la transformée de distance pour un calcul efficace.

Logs : [INFLATION] Gonflé avec rayon : Xm -> Y cellules
"""

import numpy as np
from scipy.ndimage import distance_transform_edt


class CostmapInflation:
    """
    Gonfle les obstacles dans la costmap pour une planification sûre.
    
    Crée un gradient de coût autour des obstacles.
    """
    
    def __init__(self, inflation_radius_m: float, resolution_m: float):
        """
        Initialise l'inflation.
        
        Args:
            inflation_radius_m: Jusqu'où gonfler en mètres
            resolution_m: Résolution de la grille
        """
        self.inflation_radius_m = inflation_radius_m
        self.resolution = resolution_m
        self.inflation_cells = int(inflation_radius_m / resolution_m)
        
    def inflate(self, binary_grid: np.ndarray) -> np.ndarray:
        """
        Gonfle les obstacles dans la grille.
        
        Args:
            binary_grid: Grille avec 0 = libre, 1 = occupé
            
        Returns:
            Costmap gonflée avec gradient (0-1 float)
            
        Algorithme :
            1. Calcule la transformée de distance (distance à l'obstacle le plus proche)
            2. Convertit les distances en coûts :
               - d = 0 : coût = 1.0 (occupé)
               - d < inflation_radius : coût = 1.0 - (d / rayon)
               - d >= inflation_radius : coût = 0.0 (libre)
               
        Logs:
            [INFLATION] Grille gonflée avec rayon : 0.24m (12 cellules)
        """
        # Transformée de distance : chaque cellule = distance à l'obstacle le plus proche
        distances = distance_transform_edt(1 - binary_grid) * self.resolution
        
        # Convertit en coûts
        costmap = np.zeros_like(distances, dtype=np.float32)
        
        # Cellules occupées
        costmap[binary_grid == 1] = 1.0
        
        # Région gonflée
        inflation_mask = (distances > 0) & (distances < self.inflation_radius_m)
        costmap[inflation_mask] = 1.0 - (distances[inflation_mask] / self.inflation_radius_m)
        
        return costmap
    
    def inflate_discrete(self, binary_grid: np.ndarray, 
                        lethal: int = 100, inscribed: int = 99) -> np.ndarray:
        """
        Gonfle avec des valeurs de coût discrètes (style costmap ROS).
        
        Args:
            binary_grid: Grille d'occupation binaire
            lethal: Coût pour les cellules occupées (défaut 100)
            inscribed: Coût pour les cellules dans le rayon d'inflation
            
        Returns:
            Costmap avec valeurs [0, inscribed, lethal]
        """
        distances = distance_transform_edt(1 - binary_grid)
        
        costmap = np.zeros_like(distances, dtype=np.uint8)
        
        # Obstacles létaux
        costmap[binary_grid == 1] = lethal
        
        # Région inscrite
        inflation_mask = (distances > 0) & (distances <= self.inflation_cells)
        costmap[inflation_mask] = inscribed
        
        return costmap


################################################################################
PATH: ./core/world/__init__.py
################################################################################


################################################################################
PATH: ./core/world/occupancy_grid.py
################################################################################
"""
Grille d'Occupation - Représentation Spatiale 2D

Représente l'arène comme une grille 2D avec résolution métrique :
- Valeurs cellules : 0 = libre, 1 = occupé, 0-1 = partiel
- Résolution : typiquement 2cm (0.02m) par cellule
- Dimensions : dérivées de l'étalonnage

La grille stocke :
- Obstacles statiques (depuis l'étalonnage)
- Obstacles dynamiques (robots, mis à jour chaque frame)
- Obstacles gonflés (marges de sécurité)

Système de coordonnées : mètres, origine en bas à gauche de l'arène.

Logs : préfixe [GRID] pour toutes les opérations de grille
"""

import numpy as np
from typing import Tuple, List


class OccupancyGrid:
    """
    Grille d'occupation 2D pour représentation spatiale.
    
    Fournit une vérification de collision efficace et des requêtes spatiales.
    """
    
    def __init__(self, width_m: float, height_m: float, resolution_m: float = 0.02):
        """
        Initialise la grille d'occupation.
        
        Args:
            width_m: Largeur de l'arène en mètres
            height_m: Hauteur de l'arène en mètres  
            resolution_m: Taille de cellule en mètres (défaut 2cm)
            
        La grille aura les dimensions :
            n_cols = ceil(width_m / resolution_m)
            n_rows = ceil(height_m / resolution_m)
            
        Logs:
            [GRID] Grille créée : 2.85m x 1.90m @ 0.02m -> 143 x 95 cellules
        """
        self.width_m = width_m
        self.height_m = height_m
        self.resolution = resolution_m
        
        self.n_cols = int(np.ceil(width_m / resolution_m))
        self.n_rows = int(np.ceil(height_m / resolution_m))
        
        # Données grille : 0 = libre, 1 = occupé
        self.grid = np.zeros((self.n_rows, self.n_cols), dtype=np.float32)
        
        # Obstacles statiques (depuis étalonnage, ne changent jamais)
        self.static_grid = np.zeros((self.n_rows, self.n_cols), dtype=np.float32)
        
    def world_to_grid(self, x_m: float, y_m: float) -> Tuple[int, int]:
        """
        Convertit coordonnées monde en cellule grille.
        
        Args:
            x_m: Position X en mètres
            y_m: Position Y en mètres
            
        Returns:
            (lig, col) indices cellule grille
        """
        col = int(x_m / self.resolution)
        row = int(y_m / self.resolution)
        return (row, col)
    
    def grid_to_world(self, row: int, col: int) -> Tuple[float, float]:
        """
        Convertit cellule grille en coordonnées monde (centre cellule).
        
        Args:
            row: Ligne grille
            col: Colonne grille
            
        Returns:
            (x_m, y_m) en mètres
        """
        x_m = (col + 0.5) * self.resolution
        y_m = (row + 0.5) * self.resolution
        return (x_m, y_m)
    
    def is_occupied(self, x_m: float, y_m: float, threshold: float = 0.5) -> bool:
        """
        Vérifie si une position monde est occupée.
        
        Args:
            x_m: Position X en mètres
            y_m: Position Y en mètres
            threshold: Seuil d'occupation (0-1)
            
        Returns:
            True si occupée
        """
        row, col = self.world_to_grid(x_m, y_m)
        
        if not self._is_valid_cell(row, col):
            return True  # Hors limites = occupé
        
        return self.grid[row, col] >= threshold
    
    def get_value(self, x_m: float, y_m: float) -> float:
        """
        Obtient la valeur d'occupation à une position monde.
        
        Args:
            x_m: Position X en mètres
            y_m: Position Y en mètres
            
        Returns:
            Valeur d'occupation 0-100 (0=libre, 100=occupé)
        """
        row, col = self.world_to_grid(x_m, y_m)
        
        if not self._is_valid_cell(row, col):
            return 100  # Hors limites = occupé
        
        return self.grid[row, col] * 100
    
    def set_static_obstacles(self, obstacle_cells: List[Tuple[int, int]]):
        """
        Définit les obstacles statiques depuis l'étalonnage.
        
        Args:
            obstacle_cells: Liste des cellules occupées (lig, col)
            
        Logs:
            [GRID] Obstacles statiques définis : N cellules
        """
        for row, col in obstacle_cells:
            if self._is_valid_cell(row, col):
                self.static_grid[row, col] = 1.0
        
        # Par défaut, inflated_grid = static_grid (pas d'inflation)
        self.inflated_grid = self.static_grid.copy()
        self.grid = self.inflated_grid.copy()
        
        print(f"[GRID] Obstacles statiques définis : {len(obstacle_cells)} cellules")
    
    def inflate_static_obstacles(self, robot_radius_m: float, safety_margin_m: float = 0.0):
        """
        Génère une Costmap en gonflant les obstacles avec cv2.dilate (RAPIDE).
        
        Utilise OpenCV pour un calcul instantané même sur Raspberry Pi.
        Crée une zone tampon autour de chaque obstacle basée sur la taille physique du robot.
        
        Args:
            robot_radius_m: Rayon physique du robot (ex: 0.09m pour Turtlebot)
            safety_margin_m: Marge de sécurité supplémentaire (ex: 0.05m)
            
        Logs:
            [GRID] Inflation calculée: rayon + marge = total => kernel size
        """
        import cv2
        
        # 1. Calcul du rayon total à gonfler
        total_inflation_m = robot_radius_m + safety_margin_m
        
        # 2. Conversion Mètres -> Cellules (dynamique selon résolution)
        radius_cells = int(np.ceil(total_inflation_m / self.resolution))
        
        # 3. Calcul du Kernel pour OpenCV (doit être impair: 3x3, 5x5, 7x7...)
        kernel_size = (radius_cells * 2) + 1
        
        print(f"[GRID] Inflation calculée :")
        print(f"       Robot : {robot_radius_m}m + Marge : {safety_margin_m}m = {total_inflation_m}m")
        print(f"       Cellules : {total_inflation_m:.3f}m / {self.resolution}m = {radius_cells} cellules")
        print(f"       Noyau OpenCV : {kernel_size}x{kernel_size}")
        
        # 4. Création du Kernel Circulaire (forme du robot)
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
        
        # 5. Application de la dilatation (ULTRA RAPIDE vs boucles Python)
        static_u8 = (self.static_grid * 255).astype(np.uint8)
        inflated_u8 = cv2.dilate(static_u8, kernel)
        
        # 6. Sauvegarde dans costmap (pour A*) et inflated_grid
        self.costmap = (inflated_u8 > 127).astype(np.float32)
        self.inflated_grid = self.costmap.copy()
        
        # Appliquer à la grille courante
        self.grid = self.inflated_grid.copy()
    
    def update_dynamic_obstacles(self, robot_poses: List[Tuple[float, float, float]], 
                                 robot_radius_m: float):
        """
        Met à jour la grille avec les positions actuelles des robots.
        
        Args:
            robot_poses: Liste de (x, y, theta) pour chaque robot
            robot_radius_m: Rayon du robot en mètres
            
        Algorithm:
            1. Réinitialise la grille aux obstacles statiques gonflés
            2. Pour chaque robot, marque les cellules dans le rayon comme occupées
        """
        # Reset to inflated static (pas static_grid brut)
        self.grid = self.inflated_grid.copy() if hasattr(self, 'inflated_grid') else self.static_grid.copy()
        
        # Ajoute les empreintes des robots
        radius_cells = int(np.ceil(robot_radius_m / self.resolution))
        
        for x, y, _ in robot_poses:
            center_row, center_col = self.world_to_grid(x, y)
            
            # Marque le cercle de cellules
            for dr in range(-radius_cells, radius_cells + 1):
                for dc in range(-radius_cells, radius_cells + 1):
                    if dr**2 + dc**2 <= radius_cells**2:
                        r, c = center_row + dr, center_col + dc
                        if self._is_valid_cell(r, c):
                            self.grid[r, c] = 1.0
    
    def _is_valid_cell(self, row: int, col: int) -> bool:
        """Vérifie si la cellule est dans les limites de la grille."""
        return 0 <= row < self.n_rows and 0 <= col < self.n_cols
    
    def get_costmap(self):
        """
        Retourne la costmap actuelle pour la planification.
        
        Returns:
            numpy array (n_rows x n_cols) avec coûts 0-100
        """
        return (self.grid * 100).astype(np.uint8)



################################################################################
PATH: ./core/world/unified_transform.py
################################################################################
"""
Système de Transformation Unifié - Version 2.0
-----------------------------------------------
Utilise l'homographie directe Caméra → Projecteur calculée par le wizard.
Charge les données de calibration depuis un fichier JSON.

Transformations disponibles:
- camera_to_projector(u, v): Pixel caméra → Pixel projecteur
- projector_to_world(px, py): Pixel projecteur → Mètres monde
- camera_to_world(u, v): Pixel caméra → Mètres monde
- world_to_projector(x, y): Mètres monde → Pixel projecteur
"""

import json
import numpy as np
import cv2
from pathlib import Path
from typing import Tuple, Optional


class UnifiedTransform:
    """
    Gestionnaire de transformations unifié.
    
    Utilise l'homographie directe H_CamToProj pour toutes les transformations.
    L'échelle pixels_per_meter permet la conversion vers/depuis les mètres.
    """
    
    def __init__(self, calibration_path: str = None):
        """
        Initialise le gestionnaire de transformations.
        
        Args:
            calibration_path: Chemin vers le fichier calibration.json
        """
        self.H_CamToProj = None      # Homographie Caméra → Projecteur
        self.H_ProjToCam = None      # Inverse: Projecteur → Caméra
        self.pixels_per_meter = 0.0
        self.proj_width = 0
        self.proj_height = 0
        self.margin = 0
        
        # Zone de jeu en pixels projecteur
        self.arena_x1 = 0
        self.arena_y1 = 0
        self.arena_x2 = 0
        self.arena_y2 = 0
        
        # Dimensions arène en mètres
        self.arena_width_m = 0.0
        self.arena_height_m = 0.0
        
        if calibration_path:
            self.load(calibration_path)
    
    def load(self, filepath: str) -> bool:
        """
        Charge les données de calibration.
        
        Supporte deux formats:
        - Nouveau (hybride): calibration.npz + calibration_meta.json
        - Ancien: calibration.json (tout en un)
        
        Args:
            filepath: Chemin vers le dossier config OU le fichier calibration.json
            
        Returns:
            True si chargement réussi
        """
        try:
            filepath = Path(filepath)
            
            # Déterminer si c'est un dossier ou un fichier
            if filepath.is_dir():
                config_path = filepath
            else:
                config_path = filepath.parent
            
            # Essayer le nouveau format hybride (NPZ + JSON)
            npz_path = config_path / "calibration.npz"
            meta_path = config_path / "calibration_meta.json"
            
            if npz_path.exists() and meta_path.exists():
                return self._load_hybrid(npz_path, meta_path)
            
            # Fallback: ancien format JSON unique
            json_path = config_path / "calibration.json"
            if json_path.exists():
                return self._load_legacy_json(json_path)
            
            print(f"[TRANSFORM] ERREUR: Aucun fichier de calibration trouvé dans {config_path}")
            return False
            
        except Exception as e:
            print(f"[TRANSFORM] ERREUR chargement: {e}")
            return False
    
    def _load_hybrid(self, npz_path: Path, meta_path: Path) -> bool:
        """Charge le format hybride NPZ + JSON."""
        print(f"[TRANSFORM] Chargement format hybride...")
        
        # 1. Matrices depuis NPZ
        data = np.load(npz_path)
        if 'H_CamToProj' in data:
            self.H_CamToProj = data['H_CamToProj']
            self.H_ProjToCam = np.linalg.inv(self.H_CamToProj)
        
        # 2. Métadonnées depuis JSON
        with open(meta_path, 'r') as f:
            meta = json.load(f)
        
        version = meta.get('version', '2.0')
        print(f"[TRANSFORM] Version calibration: {version}")
        
        # Projecteur
        proj = meta['projector']
        self.proj_width = proj['width']
        self.proj_height = proj['height']
        self.margin = proj['margin']
        
        # Zone de jeu
        self.arena_x1 = self.margin
        self.arena_y1 = self.margin
        self.arena_x2 = self.proj_width - self.margin
        self.arena_y2 = self.proj_height - self.margin
        
        # Échelle
        self.pixels_per_meter = meta['scale']['pixels_per_meter']
        
        # Calcul dimensions arène
        arena_width_px = self.arena_x2 - self.arena_x1
        arena_height_px = self.arena_y2 - self.arena_y1
        self.arena_width_m = arena_width_px / self.pixels_per_meter if self.pixels_per_meter > 0 else 0
        self.arena_height_m = arena_height_px / self.pixels_per_meter if self.pixels_per_meter > 0 else 0
        
        print(f"[TRANSFORM] [OK] Chargé: {self.proj_width}x{self.proj_height}, "
              f"échelle={self.pixels_per_meter:.1f} px/m, "
              f"arène={self.arena_width_m:.2f}x{self.arena_height_m:.2f}m")
        
        return True
    
    def _load_legacy_json(self, json_path: Path) -> bool:
        """Charge l'ancien format JSON unique."""
        print(f"[TRANSFORM] Chargement format legacy JSON: {json_path}")
        
        with open(json_path, 'r') as f:
            data = json.load(f)
        
        # Projecteur
        proj = data['projector']
        self.proj_width = proj['width']
        self.proj_height = proj['height']
        self.margin = proj['margin']
        
        # Zone de jeu
        self.arena_x1 = self.margin
        self.arena_y1 = self.margin
        self.arena_x2 = self.proj_width - self.margin
        self.arena_y2 = self.proj_height - self.margin
        
        # Homographie
        H_list = data['homography']['H_CamToProj']
        if H_list:
            self.H_CamToProj = np.array(H_list, dtype=np.float32)
            self.H_ProjToCam = np.linalg.inv(self.H_CamToProj)
        
        # Échelle
        self.pixels_per_meter = data['scale']['pixels_per_meter']
        
        # Calcul dimensions arène
        arena_width_px = self.arena_x2 - self.arena_x1
        arena_height_px = self.arena_y2 - self.arena_y1
        self.arena_width_m = arena_width_px / self.pixels_per_meter if self.pixels_per_meter > 0 else 0
        self.arena_height_m = arena_height_px / self.pixels_per_meter if self.pixels_per_meter > 0 else 0
        
        print(f"[TRANSFORM] [OK] Chargé (legacy): {self.proj_width}x{self.proj_height}, "
              f"échelle={self.pixels_per_meter:.1f} px/m")
        
        return True
    
    def is_calibrated(self) -> bool:
        """Vérifie si la calibration est chargée."""
        return self.H_CamToProj is not None and self.pixels_per_meter > 0
    
    # =========================================================================
    # TRANSFORMATIONS PRINCIPALES
    # =========================================================================
    
    def camera_to_projector(self, u: float, v: float) -> Tuple[float, float]:
        """
        Transforme un pixel caméra vers un pixel projecteur.
        
        Args:
            u, v: Coordonnées pixel caméra
            
        Returns:
            (px, py): Coordonnées pixel projecteur
        """
        if self.H_CamToProj is None:
            raise ValueError("Calibration non chargée")
        
        p_cam = np.array([[[u, v]]], dtype=np.float32)
        p_proj = cv2.perspectiveTransform(p_cam, self.H_CamToProj)[0][0]
        return float(p_proj[0]), float(p_proj[1])
    
    def projector_to_camera(self, px: float, py: float) -> Tuple[float, float]:
        """
        Transforme un pixel projecteur vers un pixel caméra.
        
        Args:
            px, py: Coordonnées pixel projecteur
            
        Returns:
            (u, v): Coordonnées pixel caméra
        """
        if self.H_ProjToCam is None:
            raise ValueError("Calibration non chargée")
        
        p_proj = np.array([[[px, py]]], dtype=np.float32)
        p_cam = cv2.perspectiveTransform(p_proj, self.H_ProjToCam)[0][0]
        return float(p_cam[0]), float(p_cam[1])
    
    def projector_to_world(self, px: float, py: float) -> Tuple[float, float]:
        """
        Transforme un pixel projecteur vers des coordonnées monde (mètres).
        
        Origine: coin haut-gauche de la zone de jeu
        X: vers la droite
        Y: vers le bas (convention écran) OU vers le haut (convention robot)
        
        Args:
            px, py: Coordonnées pixel projecteur
            
        Returns:
            (x, y): Coordonnées en mètres (Y inversé pour convention robot)
        """
        if self.pixels_per_meter <= 0:
            raise ValueError("Échelle non définie")
        
        # Relatif à l'origine de la zone de jeu
        dx = px - self.arena_x1
        dy = py - self.arena_y1
        
        # Conversion en mètres
        x = dx / self.pixels_per_meter
        y = dy / self.pixels_per_meter
        
        # Convention robot: Y vers le haut (inverser si nécessaire)
        # y = self.arena_height_m - y
        
        return x, y
    
    def world_to_projector(self, x: float, y: float) -> Tuple[int, int]:
        """
        Transforme des coordonnées monde (mètres) vers un pixel projecteur.
        
        Args:
            x, y: Coordonnées en mètres
            
        Returns:
            (px, py): Coordonnées pixel projecteur
        """
        if self.pixels_per_meter <= 0:
            raise ValueError("Échelle non définie")
        
        # Conversion en pixels
        dx = x * self.pixels_per_meter
        dy = y * self.pixels_per_meter
        
        # Position absolue
        px = int(self.arena_x1 + dx)
        py = int(self.arena_y1 + dy)
        
        return px, py
    
    def camera_to_world(self, u: float, v: float) -> Tuple[float, float]:
        """
        Transforme un pixel caméra directement vers des coordonnées monde (mètres).
        
        Args:
            u, v: Coordonnées pixel caméra
            
        Returns:
            (x, y): Coordonnées en mètres
        """
        px, py = self.camera_to_projector(u, v)
        return self.projector_to_world(px, py)
    
    def world_to_camera(self, x: float, y: float) -> Tuple[float, float]:
        """
        Transforme des coordonnées monde vers un pixel caméra.
        
        Args:
            x, y: Coordonnées en mètres
            
        Returns:
            (u, v): Coordonnées pixel caméra
        """
        px, py = self.world_to_projector(x, y)
        return self.projector_to_camera(float(px), float(py))
    
    # =========================================================================
    # TRANSFORMATIONS PAR LOT
    # =========================================================================
    
    def batch_camera_to_projector(self, points_cam: np.ndarray) -> np.ndarray:
        """
        Transforme plusieurs points caméra vers projecteur.
        
        Args:
            points_cam: Tableau Nx2 de coordonnées caméra
            
        Returns:
            Tableau Nx2 de coordonnées projecteur
        """
        if self.H_CamToProj is None:
            raise ValueError("Calibration non chargée")
        
        points_cam = np.array(points_cam, dtype=np.float32)
        if points_cam.ndim == 2:
            points_cam = points_cam.reshape(1, -1, 2)
        
        return cv2.perspectiveTransform(points_cam, self.H_CamToProj)[0]
    
    def batch_camera_to_world(self, points_cam: np.ndarray) -> np.ndarray:
        """
        Transforme plusieurs points caméra vers monde (mètres).
        
        Args:
            points_cam: Tableau Nx2 de coordonnées caméra
            
        Returns:
            Tableau Nx2 de coordonnées monde
        """
        points_proj = self.batch_camera_to_projector(points_cam)
        
        # Conversion en mètres
        points_world = np.zeros_like(points_proj)
        points_world[:, 0] = (points_proj[:, 0] - self.arena_x1) / self.pixels_per_meter
        points_world[:, 1] = (points_proj[:, 1] - self.arena_y1) / self.pixels_per_meter
        
        return points_world
    
    # =========================================================================
    # WARP IMAGE
    # =========================================================================
    
    def warp_camera_to_projector(self, image: np.ndarray) -> np.ndarray:
        """
        Warpe une image caméra vers l'espace projecteur.
        
        Args:
            image: Image BGR de la caméra
            
        Returns:
            Image warpée aux dimensions du projecteur
        """
        if self.H_CamToProj is None:
            raise ValueError("Calibration non chargée")
        
        return cv2.warpPerspective(
            image, 
            self.H_CamToProj, 
            (self.proj_width, self.proj_height)
        )
    
    # =========================================================================
    # UTILITAIRES
    # =========================================================================
    
    def get_arena_bounds_m(self) -> Tuple[float, float, float, float]:
        """
        Retourne les limites de l'arène en mètres.
        
        Returns:
            (x_min, y_min, x_max, y_max) en mètres
        """
        return 0.0, 0.0, self.arena_width_m, self.arena_height_m
    
    def get_arena_bounds_px(self) -> Tuple[int, int, int, int]:
        """
        Retourne les limites de l'arène en pixels projecteur.
        
        Returns:
            (x1, y1, x2, y2) en pixels
        """
        return self.arena_x1, self.arena_y1, self.arena_x2, self.arena_y2
    
    def meters_to_pixels(self, distance_m: float) -> float:
        """Convertit une distance en mètres vers pixels."""
        return distance_m * self.pixels_per_meter
    
    def pixels_to_meters(self, distance_px: float) -> float:
        """Convertit une distance en pixels vers mètres."""
        return distance_px / self.pixels_per_meter if self.pixels_per_meter > 0 else 0


# Fonction de compatibilité pour charger facilement
def load_calibration(config_dir: str = None) -> UnifiedTransform:
    """
    Charge la calibration depuis le fichier par défaut.
    
    Args:
        config_dir: Dossier de configuration (optionnel)
        
    Returns:
        UnifiedTransform initialisé
    """
    if config_dir is None:
        # Chemin par défaut relatif au module
        config_dir = Path(__file__).parent.parent.parent / "config"
    else:
        config_dir = Path(config_dir)
    
    # Vérification: soit le legacy JSON, soit le hybrid (npz + meta)
    legacy_path = config_dir / "calibration.json"
    hybrid_flag = (config_dir / "calibration.npz").exists() and (config_dir / "calibration_meta.json").exists()
    
    if not legacy_path.exists() and not hybrid_flag:
        print(f"[TRANSFORM] ATTENTION: Aucune calibration trouvée dans {config_dir}")
        print("[TRANSFORM] Lancez d'abord: python -m perception.calibration.standalone_wizard")
        return UnifiedTransform()
    
    # On passe le DOSSIER au constructeur, qui saura quoi chercher
    return UnifiedTransform(str(config_dir))


################################################################################
PATH: ./core/world/world_model.py
################################################################################
"""
Modèle du Monde - Représentation Unifiée du Monde

Référentiel central pour tout l'état du monde :
- Poses robots (filtrées par Kalman)
- Grille d'occupation (obstacles)
- Limites de l'arène
- Trames de coordonnées

C'est la source unique de vérité pour l'information spatiale.
Tous les autres modules interrogent le modèle du monde.

NE contient PAS la logique de jeu (scores, etc.) - seulement l'état spatial.
"""

from typing import Dict, List, Tuple
from .occupancy_grid import OccupancyGrid
from .coordinate_frames import TransformManager


class WorldModel:
    """
    Représentation spatiale complète du monde.
    
    Gère :
    - État des robots (positions, vitesses, orientations)
    - Obstacles (statiques + dynamiques)
    - Transformations de coordonnées
    - Limites de l'arène
    """
    
    def __init__(self, arena_width_m: float, arena_height_m: float, 
                 grid_resolution_m: float = 0.02,
                 robot_radius_m: float = 0.09,
                 inflation_margin_m: float = 0.05):
        """
        Initialise le modèle du monde.
        
        Args:
            arena_width_m: Largeur de l'arène depuis l'étalonnage
            arena_height_m: Hauteur de l'arène depuis l'étalonnage
            grid_resolution_m: Taille de cellule grille
            robot_radius_m: Rayon physique du robot (depuis config)
            inflation_margin_m: Marge de sécurité pour pathfinding (depuis config)
        """
        self.arena_width = arena_width_m
        self.arena_height = arena_height_m
        self.robot_radius_m = robot_radius_m
        self.inflation_margin_m = inflation_margin_m
        
        # Grille d'occupation
        self.grid = OccupancyGrid(arena_width_m, arena_height_m, grid_resolution_m)
        
        # État du robot
        self.robots = {
            4: {  # Robot IA
                'pose': (0.0, 0.0, 0.0),  # (x, y, theta)
                'velocity': (0.0, 0.0, 0.0),  # (vx, vy, omega)
                'radius_m': robot_radius_m,
            },
            5: {  # Robot Humain
                'pose': (0.0, 0.0, 0.0),
                'velocity': (0.0, 0.0, 0.0),
                'radius_m': robot_radius_m,
            }
        }
        
        # Transformations de coordonnées
        self.transforms = TransformManager()
        
    def update_robot_pose(self, robot_id: int, pose: Tuple[float, float, float]):
        """
        Met à jour la pose du robot depuis le filtre de Kalman.
        
        Args:
            robot_id: 4 ou 5
            pose: (x, y, theta) en mètres/radians
        """
        if robot_id in self.robots:
            self.robots[robot_id]['pose'] = pose
    
    def update_robot_velocity(self, robot_id: int, 
                             velocity: Tuple[float, float, float]):
        """
        Met à jour la vitesse du robot depuis le filtre de Kalman.
        
        Args:
            robot_id: 4 ou 5
            velocity: (vx, vy, omega) en m/s et rad/s
        """
        if robot_id in self.robots:
            self.robots[robot_id]['velocity'] = velocity
    
    def update_occupancy(self):
        """
        Met à jour la grille d'occupation avec les positions actuelles des robots.
        
        Appelé chaque frame après la mise à jour des poses des robots.
        """
        robot_poses = [self.robots[rid]['pose'] for rid in [4, 5]]
        self.grid.update_dynamic_obstacles(robot_poses, self.robot_radius_m)
    
    def generate_costmap(self):
        """
        Génère la costmap gonflée pour le pathfinding A*.
        
        Appelle après avoir chargé les obstacles statiques.
        Utilise les paramètres robot du config.
        """
        self.grid.inflate_static_obstacles(self.robot_radius_m, self.inflation_margin_m)
        print(f"[WORLD] Costmap générée avec rayon={self.robot_radius_m}m, marge={self.inflation_margin_m}m")
    
    def get_robot_pose(self, robot_id: int) -> Tuple[float, float, float]:
        """Obtient la pose actuelle du robot."""
        return self.robots[robot_id]['pose']
    
    def get_robot_velocity(self, robot_id: int) -> Tuple[float, float, float]:
        """Obtient la vitesse actuelle du robot."""
        return self.robots[robot_id]['velocity']
    
    def is_position_valid(self, x: float, y: float) -> bool:
        """
        Vérifie si une position est dans l'arène et non occupée.
        
        Args:
            x, y: Position en mètres
            
        Returns:
            True si la position est valide (dans les limites et libre)
        """
        # Vérifie les limites
        if not (0 <= x <= self.arena_width and 0 <= y <= self.arena_height):
            return False
        
        # Vérifie l'occupation
        return not self.grid.is_occupied(x, y)
    
    def get_state_dict(self) -> Dict:
        """
        Exporte l'état complet du monde sous forme de dictionnaire.
        
        Utilisé par l'IA, le moteur de jeu, la visualisation.
        
        Returns:
            dict avec toutes les informations du monde
        """
        return {
            'arena_size': (self.arena_width, self.arena_height),
            'robot_4_pose': self.robots[4]['pose'],
            'robot_5_pose': self.robots[5]['pose'],
            'robot_4_velocity': self.robots[4]['velocity'],
            'robot_5_velocity': self.robots[5]['velocity'],
            'occupancy_grid': self.grid,
        }


################################################################################
PATH: ./help
################################################################################
➜  ros2_ws history                                                                
    1  ls
    2  ros2 node list
    3  ros2 topic list
    4  sudo apt update &&  sudo apt install iputils-ping  iproute2 -y     
    5  source /opt/ros/humble/setup.zsh
    6  ros2 topic list
    7  ros2 node list
    8  ping 192.168.100.1
    9  echo $ROS_DOMAIN_ID
   10  ifconfig
   11  ip a
   12  ping 192.168.100.1
   13  ros2 topic 
   14  ros2 topic list
   15  ping 192.168.50.1
   16  ros2 topic list
   17  ros2 node list
   18  ls
   19  cd TP/TP1
   20  ls
   21  cd safe_zone_cpp
   22  ros2 node list
   23  ros2 run safe_zone_cpp safe_zone_node\nros2 topic echo /safe\n
   24  source install/setup.zsh
   25  ros2 run safe_zone_cpp safe_zone_node\nros2 topic echo /safe\n
   26  colcon build --packages-select safe_zone_cpp\nsource install/setup.zsh
   27  ros2 run safe_zone_cpp safe_zone_node\nros2 topic echo /safe\n
   28  ls
   29  tree -L 3
   30  tree -L 2
   31  colcon build --packages-select safe_zone_cpp\nsource install/setup.zsh
   32  ros2 node list
   33  cat src/safe_zone_node.cpp
   34  colcon build --packages-select safe_zone_cpp\nsource install/setup.zsh
   35  ros2 node list
   36  ros2 topic list
   37  ros2 node list
   38  ros2 run safe_zone_cpp safe_zone_node\nros2 topic echo /safe\n
   39  ros2 topic list
   40  ros2 echo braitenberg_robot /robot_description
   41  ros2 topic echo braitenberg_robot /robot_description
   42  ros2 topic echo  /robot_description
   43  ros2 topic echo  /initialpose
   44  ros2 topic echo /goal_pose
   45  ros2 topic pub /cmd_vel geometry_msgs/msg/Twist "{linear: {x: 0.2, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 0.0}}"\n
   46  source install/setup.zsh
   47  ros2 topic pub /cmd_vel geometry_msgs/msg/Twist "{linear: {x: 0.2, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 0.0}}"\n
   48  ros2 topic list
   49  ros2 topic info\n
   50  ros2 topic info /cmd_vel\n
   51  ros2 topic info --verbose /cmd_vel\n
   52  ros2 topic pub /cmd_vel geometry_msgs/msg/Twist "{linear: {x: 0.2, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 0.0}}"\n
   53  ros2 run teleop_twist_keyboard teleop_twist_keyboard\n
   54  source install/setup.zsh
   55  ros2 topic pub /cmd_vel geometry_msgs/msg/Twist "{linear: {x: 0.2, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 0.0}}"\n
   56  rviz2
   57  source install/setup.zsh
   58  ros2 topic pub /cmd_vel geometry_msgs/msg/Twist "{linear: {x: 0.2, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 0.0}}" && ros2 topic echo /odo --once\n
   59  ping google.com
   60  ufw
   61  ls
   62  export ROS_DOMAIN_ID=30
   63  source /opt/ros/humble/setup.zsh
   64  ls
   65  ros2 topic list
   66  ls
   67  cd
   68  ls
   69  cd ros2_ws
   70  ls
   71  tree .
   72  exit
   73  ls
   74  ros2 topic list
   75  ls
   76  cd turtlebot3_ws
   77  ls
   78  cd src
   79  ls
   80  cd ../..
   81  ls
   82  cd turtlebot_game
   83  ls
   84  cd ..
   85  ls
   86  mkdir src
   87  rm -rf turtlebot3_ws
   88  sudo rm -rf turtlebot3_ws
   89  ls
   90  cat run.sh
   91  ls
   92  rm run.sh
   93  ls
   94  clear
   95  ls
   96  colcon build \nsource install/setup.zsh
   97  ls
   98  ls src
   99  ls
  100  ls src/turtlebot_game
  101  colcon build \nsource install/setup.zsh
  102  ls
  103  ros2 launch turtlebot_game safety_bridge.launch.py
  104  ros2 launch turtlebot_game safety_bridge.launch
  105  ros2 launch turtlebot_game safety_bridge
  106  ls
  107  colcon build --packages-select turtlebot_game\n
  108  ls
  109  ls braitenberg_ws
  110  ls
  111  colcon build --packages-select turtlebot_game\n
  112  source install/setup.bash\n
  113  source install/setup.zsh\n
  114  ros2 launch turtlebot_game safety_bridge.launch.py\n
  115  colcon build --packages-select turtlebot_game
  116  colcon build --packages-select turtlebot_game --symlink-install
  117  source install/setup.zsh
  118  ros2 launch turtlebot_game safety_bridge.launch.py
  119  pip list
  120  pip install websocket
  121  ros2 launch turtlebot_game safety_bridge.launch.py
  122  # 1. Installer la bonne librairie\npip install websockets\n\n# 2. Nettoyer le workspace (construits précédents)\ncd ~/ros2_ws  # ou votre dossier racine\nrm -rf build install log\n\n# 3. Reconstruire proprement\ncolcon build --packages-select turtlebot_game --symlink-install\n\n# 4. Sourcer le nouvel environnement\nsource install/setup.zsh\n\n# 5. Lancer\nros2 launch turtlebot_game safety_bridge.launch.py
  123  ls
  124  cat docker-compose.yml
  125  ros2 launch turtlebot_game safety_bridge.launch.py
  126  ros2 topic list
  127  export ROS_DOMAIN_ID=30
  128  source install/setup.zsh
  129  source /opt/ros/humble/setup.zsh
  130  ros2 topic list
  131  ros2 launch turtlebot_game safety_bridge.launch.py
  132  clear
  133  ros2 launch turtlebot_game safety_bridge.launch.py
  134  # Test 1 : Topic avec namespace (ce que tu utilises actuellement)\nros2 topic pub --once /robot1/cmd_vel geometry_msgs/msg/Twist "{linear: {x: 0.1, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 0.0}}"
  135  ros2 topic list
  136  # Test 2 : Topic standard (sans namespace)\nros2 topic pub --once /cmd_vel geometry_msgs/msg/Twist "{linear: {x: 0.1, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 0.0}}"
  137  # Test 2 : Topic standard (sans namespace)\nros2 topic pub --once /cmd_vel geometry_msgs/msg/Twist "{linear: {x: 1, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 0.0}}"
  138  ros2 launch turtlebot_game safety_bridge.launch.py
  139  ros2 launch turtlebot_game safety_bridge.launch.py --ros-args -r /robot1/cmd_vel:=/cmd_vel
  140  cat src/turtlebot_game/launch/safety_bridge.launch.py
  141  ros2 launch turtlebot_game safety_bridge.launch.py
  142  colcon build --packages-select turtlebot_game --symlink-install
  143  ros2 launch turtlebot_game safety_bridge.launch.py
  144  clear
  145  ros2 launch turtlebot_game safety_bridge.launch.py
  146  clear
  147  ros2 topic list
  148  ros2 launch turtlebot_game safety_bridge.launch.py
  149  clear
  150  ros2 launch turtlebot_game safety_bridge.launch.py
➜  ros2_ws 



################################################################################
PATH: ./__main__.py
################################################################################
"""
Tank Project - Module Package
"""

__version__ = '1.0.0'
__author__ = 'Julien'

# Point d'entrée module
if __name__ == '__main__':
    from main import main
    main()


################################################################################
PATH: ./main.py
################################################################################
#!/usr/bin/env python3
"""
Tank Arena - Main Entry Point
Lance le GameManager qui orchestre:
- Perception (Realsense/ArUco)
- IA (Behavior Tree)
- Rendu (Pygame)
- Contrôle (ROS Bridge)
"""

import sys
import argparse
from pathlib import Path

# Add project root to path
ROOT_DIR = Path(__file__).parent.resolve()
sys.path.insert(0, str(ROOT_DIR))

from core.game.game_manager import GameManager

def main():
    parser = argparse.ArgumentParser(description="Tank Arena - Unified Game Launcher")
    parser.add_argument('--mock', action='store_true', help="Force Mock Mode (Simulation sans hardware)")
    args = parser.parse_args()

    print("==========================================")
    print("   TANK ARENA - AUTO MODE INITIALIZED     ")
    print("==========================================")
    print(f"Mode: {'MOCK/SIMULATION' if args.mock else 'REAL HARDWARE'}")
    
    try:
        game = GameManager(mock=args.mock)
        game.run()
    except Exception as e:
        print(f"[MAIN] Critical Error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()


################################################################################
PATH: ./perception/calibration/arena_solver.py
################################################################################
"""
Arena Solver - Calcul Dimensions Arène

Déduit les dimensions physiques de l'arène (Lx, Ly)
à partir de la calibration.

Logs: [ARENA_SOLVER] prefix
"""

import numpy as np
from typing import Tuple


class ArenaSolver:
    """
    Calcule dimensions arène depuis calibration.
    """
    
    def __init__(self):
        """Initialize arena solver."""
        self.width_m = None
        self.height_m = None
        
    def solve_from_av_and_scale(self,
                                av_width: float,
                                av_height: float,
                                scale: float) -> Tuple[float, float]:
        """
        Calcule dimensions arène depuis taille AV et échelle.
        
        Args:
            av_width: Largeur en unités AV (typiquement 1.0)
            av_height: Hauteur en unités AV (typiquement 1.0)
            scale: Échelle m/unité_av
            
        Returns:
            (width_m, height_m) dimensions en mètres
            
        Logs:
            [ARENA_SOLVER] Arena dimensions: Lx x Ly meters
        """
        self.width_m = av_width * scale
        self.height_m = av_height * scale
        
        print("[ARENA_SOLVER] Dimensions de l'arène : "
              "{:.2f}m x {:.2f}m".format(self.width_m, self.height_m))
        
        return (self.width_m, self.height_m)
    
    def solve_from_corners(self,
                          corners_world: np.ndarray) -> Tuple[float, float]:
        """
        Calcule dimensions depuis coins arène en coordonnées monde.
        
        Args:
            corners_world: 4 coins en mètres (ordre: BL, BR, TR, TL)
            
        Returns:
            (width_m, height_m)
        """
        # Distance entre coins bas
        width = np.linalg.norm(corners_world[1] - corners_world[0])
        
        # Distance entre coins gauche
        height = np.linalg.norm(corners_world[3] - corners_world[0])
        
        self.width_m = width
        self.height_m = height
        
        print("[ARENA_SOLVER] Dimensions de l'arène depuis coins : "
              "{:.2f}m x {:.2f}m".format(width, height))
        
        return (width, height)
    
    def get_dimensions(self) -> Tuple[float, float]:
        """
        Retourne dimensions calculées.
        
        Returns:
            (width_m, height_m)
            
        Raises:
            ValueError: Si dimensions pas encore calculées
        """
        if self.width_m is None:
            raise ValueError("Dimensions pas encore calculées")
        
        return (self.width_m, self.height_m)
    
    def get_aspect_ratio(self) -> float:
        """Retourne ratio aspect width/height."""
        if self.width_m is None:
            raise ValueError("Dimensions pas encore calculées")
        
        return self.width_m / self.height_m


################################################################################
PATH: ./perception/calibration/calibration_wizard.py
################################################################################
"""
╔═══════════════════════════════════════════════════════════════════════════════╗
║                              ⚠️  OBSOLÈTE  ⚠️                                 ║
╠═══════════════════════════════════════════════════════════════════════════════╣
║  Ce module utilise l'ancien système de calibration.                           ║
║                                                                               ║
║  UTILISEZ PLUTÔT: standalone_wizard.py                                        ║
║    python -m perception.calibration.standalone_wizard                         ║
╚═══════════════════════════════════════════════════════════════════════════════╝

Assistant de Calibration - VERSION LEGACY (OBSOLÈTE)

Correctif :
- La boucle d'attente redessine les marqueurs ArUco au lieu de les effacer.
- Utilise la correction de distorsion optique (cv2.undistort) pour améliorer la précision aux bords.
"""

import cv2
import numpy as np
import yaml
import time
import sys
import pygame  # Import nécessaire pour les événements
from typing import Tuple, List
from ..camera.aruco_detector import ArucoDetector
from core.world.coordinate_frames import TransformManager
from .projector_display import ProjectorDisplay


class CalibrationWizard:
    def __init__(self, camera, projector_width=1024, projector_height=768, 
                 margin_px=50, monitor_offset_x=1920, monitor_offset_y=0,
                 borderless=True, hide_cursor=True, marker_size_m=0.10):
        
        self.camera = camera
        self.proj_w = projector_width
        self.proj_h = projector_height
        self.marker_size_m = marker_size_m
        
        self.aruco = ArucoDetector()
        self.transform_mgr = TransformManager()
        
        # Récupération des paramètres intrinsèques pour la correction optique
        self.K, self.D = self.camera.get_intrinsics_matrix()
        if self.K is not None:
             print("[CALIB] Correction de distorsion ACTIVE")
        else:
             print("[CALIB] ATTENTION: Pas de correction de distorsion (Intrinsics non trouvés)")

        self.projector = ProjectorDisplay(
            width=projector_width,
            height=projector_height,
            margin=margin_px,
            monitor_offset_x=monitor_offset_x,
            monitor_offset_y=monitor_offset_y,
            borderless=borderless,
            hide_cursor=hide_cursor
        )
        
        self.margin_px = margin_px
        self.arena_width_m = None
        self.arena_height_m = None
        self.H_C2W = None
        self.static_obstacles = []
    
    def _get_undistorted_frame(self):
        """Récupère une frame et applique la correction de distorsion si possible."""
        color, _ = self.camera.get_frames()
        if self.K is not None and self.D is not None and color is not None:
            color = cv2.undistort(color, self.K, self.D)
        return color

    def _wait_for_user_validation(self, message: str = "Appuyez sur ESPACE...", draw_callback=None):
        """
        Boucle d'attente générique qui supporte un redessin continu (draw_callback).
        """
        print(f"[CALIB] ATTENTE : {message}")
        
        # Si pas de callback de dessin spécifique, on affiche juste le message (fond noir)
        if draw_callback is None:
            self.projector.show_message(message, color=(255, 255, 255), bg_color=(50, 50, 50))
        
        waiting = True
        while waiting:
            # Si on a une fonction de dessin (ex: afficher les marqueurs), on l'appelle en boucle
            if draw_callback:
                draw_callback()
            
            # Gestion des événements Pygame
            events = self.projector.get_events()
            for event in events:
                if event.type == pygame.QUIT:
                    sys.exit(0)
                elif event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_SPACE:
                        waiting = False
                        print("[CALIB] Validé !")
                    elif event.key == pygame.K_ESCAPE or event.key == pygame.K_q:
                        print("[CALIB] Annulation...")
                        sys.exit(0)
            
            time.sleep(0.05) # Petite pause pour le CPU
    
    def run(self) -> dict:
        print("[CALIB] ========== Démarrage Calibration ==========")
        self.projector.start()
        
        try:
            # Étape 1 : Zone sûre (juste pour info console)
            print(f"[CALIB] MARGE définie à {self.margin_px} px")
            
            # Étape 2 : Calibration géométrique (Coins projetés)
            H_C2AV = self._step_geometric_calibration()
            
            # Étape 3 : Calibration métrique (Taille réelle)
            scale = self._step_metric_calibration(H_C2AV)
            
            # Étape 4 : Obstacles (Optionnel)
            obstacles = self._step_obstacle_mapping()
            
            # Construction des résultats
            results = {
                'projector': {
                    'width': self.proj_w,
                    'height': self.proj_h,
                    'margin': self.margin_px,
                    'margin_px': self.margin_px
                },
                'display': {
                    'fullscreen': False,
                    'display_index': 0
                },
                'arena': {
                    'width_m': self.arena_width_m,
                    'height_m': self.arena_height_m
                },
                'transform': {
                    'H_C2W': self.H_C2W.tolist() if self.H_C2W is not None else None,
                    'scale': scale,
                    'scale_m_per_av': scale
                },
                'grid': {
                    'resolution_m': 0.02,
                    'inflation_radius_m': 0.15
                },
                'obstacles': obstacles
            }
            
            print("[CALIB] Calibration terminée et sauvegardée en mémoire.")
            return results
            
        finally:
            self.projector.stop()
    
    def _step_geometric_calibration(self) -> np.ndarray:
        print("[CALIB] Étape 2/4 : Calibration géométrique")
        print("[CALIB] Affichage des 4 marqueurs ArUco (IDs 0-3)...")
        
        # On passe la fonction d'affichage en callback
        self._wait_for_user_validation(
            "Vérifiez que la caméra voit les 4 coins, puis appuyez sur ESPACE",
            draw_callback=lambda: self.projector.show_corner_markers(marker_size_px=200)
        )
        
        # Capture avec UNDISTORT
        print("[CALIB] Capture de l'image (Undistorted)...")
        color = self._get_undistorted_frame()
        detections = self.aruco.detect(color)
        
        corner_ids = [0, 1, 2, 3]
        detected_corners = {k: v for k, v in detections.items() if k in corner_ids}
        
        if len(detected_corners) != 4:
            print(f"[CALIB] ERREUR : Seulement {len(detected_corners)}/4 coins détectés !")
            print(f"[CALIB] IDs vus : {list(detected_corners.keys())}")
            raise ValueError("Échec détection des 4 coins projetés")
            
        # Calcul Homographie
        src_points = []
        dst_points = []
        
        ar = self.proj_w / self.proj_h
        av_coords = {
            0: (0.0, 0.0), 1: (ar, 0.0), 2: (ar, 1.0), 3: (0.0, 1.0)
        }
        
        for marker_id in corner_ids:
            center = detected_corners[marker_id]['center']
            src_points.append(center)
            dst_points.append(av_coords[marker_id])
            
        src_points = np.array(src_points, dtype=np.float32)
        dst_points = np.array(dst_points, dtype=np.float32)
        
        H_C2AV, _ = cv2.findHomography(src_points, dst_points)
        print("[CALIB] Matrice H_C2AV calculée.")
        return H_C2AV

    def _step_metric_calibration(self, H_C2AV: np.ndarray) -> float:
        print("[CALIB] Étape 3/4 : Calibration métrique")
        
        self._wait_for_user_validation("Placez le ROBOT (ID 4 ou 5) au centre et appuyez sur ESPACE")
        
        # Capture avec UNDISTORT
        color = self._get_undistorted_frame()
        detections = self.aruco.detect(color)
        
        marker_data = detections.get(4) or detections.get(5)
        if not marker_data:
            raise ValueError("Aucun marqueur robot (4 ou 5) trouvé !")
            
        corners_pix = marker_data['corners']
        
        # Transformation perspective
        pts_src = np.array([corners_pix], dtype=np.float32)
        pts_av = cv2.perspectiveTransform(pts_src, H_C2AV)[0]
        
        # Taille moyenne en unités virtuelles
        side_lengths = [np.linalg.norm(pts_av[j] - pts_av[(j+1)%4]) for j in range(4)]
        size_av = np.mean(side_lengths)
        
        scale_m_per_av = self.marker_size_m / size_av
        print(f"[CALIB] Échelle : 1.0 unité virtuelle = {scale_m_per_av:.4f} mètres")
        
        # Calcul final H_C2W
        self.transform_mgr.H_C2AV = H_C2AV
        self.transform_mgr.set_av_to_world_scale(scale_m_per_av)
        self.H_C2W = self.transform_mgr.H_C2W
        
        # Dimensions physiques
        ar = self.proj_w / self.proj_h
        self.arena_width_m = scale_m_per_av * ar
        self.arena_height_m = scale_m_per_av * 1.0
        
        return scale_m_per_av

    def _step_obstacle_mapping(self) -> List:
        print("[CALIB] Étape 4/4 : Cartographie")
        
        self._wait_for_user_validation(
            "Placez les obstacles, puis ESPACE (Écran deviendra BLANC)",
            draw_callback=lambda: self.projector.show_white_screen()
        )
        
        print("[CALIB] Mapping ignoré pour l'instant (retourne liste vide)")
        return []


################################################################################
PATH: ./perception/calibration/__init__.py
################################################################################


################################################################################
PATH: ./perception/calibration/projector_display.py
################################################################################
"""
Affichage Projecteur pour Étalonnage - VERSION FINALE

Affiche les marqueurs ArUco sur le projecteur pour l'assistant d'étalonnage.
Fonctionnalités :
- Positionnement automatique sur l'écran secondaire (VGA/HDMI)
- Mode san bordure pour masquer les décorations de fenêtre
- Curseur masqué pour l'immersion

Logs : préfixe [PROJ_DISPLAY]
"""

import os
import pygame
import cv2
import numpy as np
from typing import Tuple, Optional


class ProjectorDisplay:
    """
    Fenêtre Pygame pour projeter les motifs d'étalonnage.
    Force l'affichage sur le moniteur secondaire en utilisant les variables d'environnement SDL.
    """
    
    def __init__(self, width=1024, height=768, margin=50, monitor_offset_x=1920, monitor_offset_y=0,
                 borderless=True, hide_cursor=True):
        """
        Initialise l'affichage projecteur.
        
        Args:
            width: Largeur résolution projecteur (ex: 1024 pour VGA)
            height: Hauteur résolution projecteur (ex: 768 pour VGA)
            margin: Marge de sécurité depuis les bords (px)
            monitor_offset_x: Position X du projecteur (généralement largeur écran principal, ex: 1920)
            monitor_offset_y: Position Y (généralement 0)
            borderless: Utilise mode fenêtre sans bordure (NOFRAME)
            hide_cursor: Masque le curseur souris
        """
        self.width = width
        self.height = height
        self.margin = margin
        
        # --- CONFIGURATION MULTI-ECRAN ---
        self.monitor_x = monitor_offset_x
        self.monitor_y = monitor_offset_y
        self.borderless = borderless
        self.hide_cursor = hide_cursor
        
        self.screen = None
        self.running = False
        
        # Rectangle arène (avec marges)
        self.arena_x1 = margin
        self.arena_y1 = margin
        self.arena_x2 = width - margin
        self.arena_y2 = height - margin
        self.arena_w = self.arena_x2 - self.arena_x1
        self.arena_h = self.arena_y2 - self.arena_y1
        
        print("[PROJ_DISPLAY] Init : {}x{}, marge={}px".format(width, height, margin))
        print("[PROJ_DISPLAY] Décalage Moniteur Cible : X={}, Y={}".format(self.monitor_x, self.monitor_y))
    
    def start(self):
        """Démarre Pygame et crée la fenêtre sur le projecteur."""
        
        # 1. LE HACK: On force la position avant l'init de l'écran
        os.environ['SDL_VIDEO_WINDOW_POS'] = "%d,%d" % (self.monitor_x, self.monitor_y)
        
        pygame.init()
        
        # 2. Configure les drapeaux de fenêtre selon paramètres
        # CORRECTION : On utilise NOFRAME par défaut pour le borderless, plus stable que FULLSCREEN
        flags = pygame.DOUBLEBUF
        if self.borderless:
            flags |= pygame.NOFRAME
        else:
            flags |= pygame.RESIZABLE
        
        self.screen = pygame.display.set_mode((self.width, self.height), flags)
        
        # Force le remplissage noir immédiat pour voir la taille réelle
        self.screen.fill((0,0,0))
        pygame.display.flip()
        
        # 3. IMMERSION - Masque curseur si configuré
        if self.hide_cursor:
            pygame.mouse.set_visible(False)
        
        pygame.display.set_caption("Arène Tank - Vue Projecteur")
        self.running = True
        
        mode_str = "borderless" if self.borderless else "windowed"
        print(f"[PROJ_DISPLAY] Fenêtre ouverte au décalage {self.monitor_x} ({mode_str})")
    
    def stop(self):
        """Ferme l'affichage."""
        if self.running:
            pygame.quit()
            self.running = False
            print("[PROJ_DISPLAY] Affichage fermé")
    
    def clear(self, color=(0, 0, 0)):
        """Efface l'écran avec une couleur unie."""
        if self.screen:
            self.screen.fill(color)
    
    def get_events(self):
        """
        Retourne les événements au contrôleur externe (Wizard).
        Utilisez ceci au lieu de handle_events quand la logique est contrôlée à l'extérieur.
        """
        if not self.running:
            return []
        return pygame.event.get()

    def show_corner_markers(self, marker_size_px=200, aruco_dict=cv2.aruco.DICT_4X4_100):
        """
        Affiche les marqueurs ArUco aux 4 coins de l'arène.
        """
        if not self.running:
            return
        
        # Efface avec fond blanc
        self.clear((255, 255, 255))
        
        # Génère marqueurs
        aruco_dict_obj = cv2.aruco.getPredefinedDictionary(aruco_dict)
        
        # Positions coins (centre du marqueur)
        corners = {
            0: (self.arena_x1 + marker_size_px // 2, self.arena_y2 - marker_size_px // 2),  # Bottom-left
            1: (self.arena_x2 - marker_size_px // 2, self.arena_y2 - marker_size_px // 2),  # Bottom-right
            2: (self.arena_x2 - marker_size_px // 2, self.arena_y1 + marker_size_px // 2),  # Top-right
            3: (self.arena_x1 + marker_size_px // 2, self.arena_y1 + marker_size_px // 2),  # Top-left
        }
        
        print("[PROJ_DISPLAY] Projection de 4 marqueurs de coin (IDs 0-3)")
        
        for marker_id, (cx, cy) in corners.items():
            # Génère image marqueur
            marker_img = cv2.aruco.generateImageMarker(aruco_dict_obj, marker_id, marker_size_px)
            
            # Convertit en surface pygame
            marker_img_rgb = cv2.cvtColor(marker_img, cv2.COLOR_GRAY2RGB)
            marker_surface = pygame.surfarray.make_surface(
                np.transpose(marker_img_rgb, (1, 0, 2))
            )
            
            # Calcule coin haut-gauche
            x = cx - marker_size_px // 2
            y = cy - marker_size_px // 2
            
            # Dessine marqueur
            self.screen.blit(marker_surface, (x, y))
            
            # Ajoute étiquette ID sous marqueur
            font = pygame.font.Font(None, 36)
            text = font.render(f"ID {marker_id}", True, (0, 0, 0))
            text_rect = text.get_rect(center=(cx, cy + marker_size_px // 2 + 30))
            self.screen.blit(text, text_rect)
        
        # Met à jour affichage
        pygame.display.flip()
        
        print("[PROJ_DISPLAY] Marqueurs de coin affichés")
    
    def show_white_screen(self):
        """Affiche écran blanc uni (pour détection obstacles)."""
        if not self.running:
            return
        
        self.clear((255, 255, 255))
        
        # Ajoute instruction texte
        font = pygame.font.Font(None, 48)
        text = font.render("Placez des obstacles dans l'arène", True, (0, 0, 0))
        text_rect = text.get_rect(center=(self.width // 2, 100))
        self.screen.blit(text, text_rect)
        
        pygame.display.flip()
        print("[PROJ_DISPLAY] Écran blanc affiché")
    
    def show_message(self, message: str, color=(255, 255, 255), bg_color=(0, 0, 0)):
        """
        Affiche un message texte.
        """
        if not self.running:
            return
        
        self.clear(bg_color)
        
        # Ajustement taille police selon longueur
        # 72 est souvent trop gros pour du 1024px de large avec une longue phrase
        font_size = 48 
        if len(message) > 20: font_size = 36
        if len(message) > 40: font_size = 24
            
        font = pygame.font.Font(None, font_size)
        text = font.render(message, True, color)
        text_rect = text.get_rect(center=(self.width // 2, self.height // 2))
        self.screen.blit(text, text_rect)
        
        pygame.display.flip()
    
    def _pump_events(self):
        """
        Interne : Garde la fenêtre réactive sans consommer les événements.
        Appelez ceci dans les opérations longues si vous n'utilisez pas get_events().
        """
        pygame.event.pump()


################################################################################
PATH: ./perception/calibration/projector_mapping.py
################################################################################
"""
Projector Mapping - Transformation Monde -> Projecteur

Gère la conversion des coordonnées monde (mètres)
vers pixels projecteur pour affichage Pygame.

Logs: [PROJ_MAP] prefix
"""

import numpy as np
from typing import Tuple


class ProjectorMapping:
    """
    Mapping Monde -> Pixels Projecteur.
    """
    
    def __init__(self,
                 projector_width: int = 1024,
                 projector_height: int = 768,
                 margin: int = 50):
        """
        Initialize projector mapping.
        
        Args:
            projector_width: Résolution projecteur largeur
            projector_height: Résolution projecteur hauteur
            margin: Marge sécurité (pixels)
        """
        self.proj_w = projector_width
        self.proj_h = projector_height
        self.margin = margin
        
        self.draw_w = projector_width - 2 * margin
        self.draw_h = projector_height - 2 * margin
        
        # Paramètres monde (à définir après calibration)
        self.arena_width_m = None
        self.arena_height_m = None
        self.scale = None
        
    def set_arena_dimensions(self, width_m: float, height_m: float):
        """
        Définit dimensions arène et calcule échelle d'affichage.
        
        Args:
            width_m: Largeur arène en mètres
            height_m: Hauteur arène en mètres
            
        Logs:
            [PROJ_MAP] Arena set: WxH m, scale: S px/m
        """
        self.arena_width_m = width_m
        self.arena_height_m = height_m
        
        # Calculer échelle (maintenir aspect ratio)
        scale_x = self.draw_w / width_m
        scale_y = self.draw_h / height_m
        self.scale = min(scale_x, scale_y)
        
        print("[PROJ_MAP] Arène définie : {:.2f}x{:.2f}m, "
              "échelle : {:.1f} px/m".format(width_m, height_m, self.scale))
        
    def world_to_projector(self, x_m: float, y_m: float) -> Tuple[int, int]:
        """
        Convertit coordonnées monde -> pixels projecteur.
        
        Args:
            x_m, y_m: Position en mètres
            
        Returns:
            (px, py) position en pixels projecteur
        """
        if self.scale is None:
            raise ValueError("Appeler set_arena_dimensions d'abord")
        
        # Conversion avec flip Y (pygame origin top-left)
        px = self.margin + int(x_m * self.scale)
        py = self.margin + int((self.arena_height_m - y_m) * self.scale)
        
        return (px, py)
    
    def projector_to_world(self, px: int, py: int) -> Tuple[float, float]:
        """
        Convertit pixels projecteur -> coordonnées monde.
        
        Args:
            px, py: Position en pixels
            
        Returns:
            (x_m, y_m) position en mètres
        """
        if self.scale is None:
            raise ValueError("Appeler set_arena_dimensions d'abord")
        
        x_m = (px - self.margin) / self.scale
        y_m = self.arena_height_m - (py - self.margin) / self.scale
        
        return (x_m, y_m)
    
    def scale_length(self, length_m: float) -> int:
        """
        Convertit longueur mètres -> pixels.
        
        Args:
            length_m: Longueur en mètres
            
        Returns:
            Longueur en pixels
        """
        if self.scale is None:
            raise ValueError("Appeler set_arena_dimensions d'abord")
        
        return int(length_m * self.scale)
    
    def get_safe_zone_rect(self) -> Tuple[int, int, int, int]:
        """
        Retourne rectangle zone sécurité.
        
        Returns:
            (x, y, width, height) en pixels
        """
        return (self.margin, self.margin, self.draw_w, self.draw_h)


################################################################################
PATH: ./perception/calibration/scale_estimator.py
################################################################################
"""
Scale Estimator - Estimation Échelle Métrique

Estime le facteur d'échelle de AV -> Monde en mètres
à partir d'un marqueur ArUco physique de taille connue.

Logs: [SCALE_EST] prefix
"""

import numpy as np
from typing import List, Tuple


class ScaleEstimator:
    """
    Estime l'échelle métrique depuis marqueur physique.
    """
    
    def __init__(self, marker_real_size_m: float = 0.10):
        """
        Initialize scale estimator.
        
        Args:
            marker_real_size_m: Taille réelle marqueur en mètres
        """
        self.marker_size_real = marker_real_size_m
        self.samples = []
        
    def estimate_from_corners(self,
                            corners_av: np.ndarray) -> float:
        """
        Estime échelle depuis coins marqueur en coordonnées AV.
        
        Args:
            corners_av: 4 coins en unités AV (4x2)
            
        Returns:
            Échelle en m/unité_av
        """
        # Calculer longueur moyenne des côtés en AV
        side_lengths = []
        for i in range(4):
            j = (i + 1) % 4
            length = np.linalg.norm(corners_av[j] - corners_av[i])
            side_lengths.append(length)
        
        avg_size_av = np.mean(side_lengths)
        
        # Échelle
        scale = self.marker_size_real / avg_size_av
        
        return scale
    
    def add_sample(self, corners_av: np.ndarray):
        """
        Ajoute une mesure d'échelle.
        
        Args:
            corners_av: Coins marqueur en AV
        """
        scale = self.estimate_from_corners(corners_av)
        self.samples.append(scale)
        print("[SCALE_EST] Échantillon {} : échelle={:.4f} m/unité".format(len(self.samples), scale))
        
    def get_average_scale(self) -> float:
        """
        Retourne échelle moyenne de tous les échantillons.
        
        Returns:
            Échelle moyenne
            
        Logs:
            [SCALE_EST] Average scale from N samples: X m/unit (std=Y)
        """
        if not self.samples:
            raise ValueError("Aucun échantillon disponible")
        
        avg = np.mean(self.samples)
        std = np.std(self.samples)
        
        print("[SCALE_EST] Échelle moyenne sur {} échantillons : "
              "{:.4f} m/unité (std={:.4f})".format(len(self.samples), avg, std))
        
        return avg
    
    def reset(self):
        """Réinitialise les échantillons."""
        self.samples = []


################################################################################
PATH: ./perception/calibration/standalone_wizard.py
################################################################################
#!/usr/bin/env python3
"""
CALIBRATION WIZARD STANDALONE - Intégré au Projet
--------------------------------------------------
Wizard de calibration basé sur l'homographie directe Caméra → Projecteur.
Sauvegarde les données en JSON pour une meilleure fiabilité.

Usage:
    python -m perception.calibration.standalone_wizard

Auteur: Julien (version standalone validée)
"""

import sys
import os
import time
import json
import yaml
import cv2
import numpy as np
import pygame
import pyrealsense2 as rs
from datetime import datetime
from pathlib import Path


class StandaloneCamera:
    """Gestionnaire Caméra RealSense avec correction de distorsion."""
    
    def __init__(self, width=1280, height=720, fps=30):
        self.pipeline = rs.pipeline()
        config = rs.config()
        config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)
        self.profile = self.pipeline.start(config)
        
        # Warmup
        time.sleep(1.0)
        self.K, self.D = self._get_intrinsics()
        
    def _get_intrinsics(self):
        """Récupère les paramètres intrinsèques de la caméra."""
        stream = self.profile.get_stream(rs.stream.color)
        intr = stream.as_video_stream_profile().get_intrinsics()
        K = np.array([
            [intr.fx, 0, intr.ppx],
            [0, intr.fy, intr.ppy],
            [0, 0, 1]
        ], dtype=np.float32)
        D = np.array(intr.coeffs, dtype=np.float32)
        print(f"[CAM] Intrinsèques: fx={intr.fx:.1f}, fy={intr.fy:.1f}, cx={intr.ppx:.1f}, cy={intr.ppy:.1f}")
        return K, D

    def get_frame(self):
        """Récupère une frame corrigée de la distorsion."""
        frames = self.pipeline.wait_for_frames()
        color_frame = frames.get_color_frame()
        if not color_frame:
            return None
        img = np.asanyarray(color_frame.get_data())
        # Correction Distorsion Immédiate
        return cv2.undistort(img, self.K, self.D)

    def stop(self):
        """Arrête le pipeline caméra."""
        self.pipeline.stop()


class CalibrationData:
    """
    Structure de données de calibration.
    
    Stockage hybride :
    - calibration.npy : Matrices numpy (H_CamToProj, K, D)
    - calibration_meta.json : Métadonnées (date, échelle, version)
    - projector.yaml : Config manuelle (résolution, marges) - existant
    """
    
    def __init__(self):
        self.H_CamToProj = None      # Homographie Caméra → Projecteur (3x3)
        self.pixels_per_meter = 0.0   # Échelle en pixels/mètre
        self.proj_width = 0
        self.proj_height = 0
        self.margin = 0
        self.marker_size_m = 0.0
        self.calibration_date = None
        
        # Intrinsèques caméra (pour référence)
        self.camera_K = None
        self.camera_D = None
        
    def save(self, config_dir: str):
        """
        Sauvegarde la calibration en format hybride.
        
        Args:
            config_dir: Dossier de configuration (ex: tank_project/config/)
            
        Crée:
            - calibration.npy : Matrices numpy
            - calibration_meta.json : Métadonnées
        """
        config_path = Path(config_dir)
        
        # 1. Matrices numpy → .npz (np.savez génère un fichier .npz)
        npz_path = config_path / "calibration.npz"
        np.savez(
            npz_path,
            H_CamToProj=self.H_CamToProj,
            camera_K=self.camera_K,
            camera_D=self.camera_D
        )
        print(f"[CALIB] Matrices sauvées: {npz_path}")
        
        # 2. Métadonnées → .json
        meta_path = config_path / "calibration_meta.json"
        meta = {
            'version': '2.1',
            'calibration_date': self.calibration_date,
            'scale': {
                'pixels_per_meter': self.pixels_per_meter,
                'marker_size_m': self.marker_size_m
            },
            'projector': {
                'width': self.proj_width,
                'height': self.proj_height,
                'margin': self.margin
            }
        }
        with open(meta_path, 'w') as f:
            json.dump(meta, f, indent=2)
        print(f"[CALIB] Métadonnées sauvées: {meta_path}")
        
    @classmethod
    def load(cls, config_dir: str) -> 'CalibrationData':
        """
        Charge la calibration depuis les fichiers hybrides.
        
        Args:
            config_dir: Dossier de configuration
            
        Returns:
            CalibrationData initialisé
        """
        config_path = Path(config_dir)
        calib = cls()
        
        # 1. Charger matrices numpy (.npz)
        npz_path = config_path / "calibration.npz"
        if npz_path.exists():
            data = np.load(npz_path)
            if 'H_CamToProj' in data:
                calib.H_CamToProj = data['H_CamToProj']
            if 'camera_K' in data:
                calib.camera_K = data['camera_K']
            if 'camera_D' in data:
                calib.camera_D = data['camera_D']
            print(f"[CALIB] Matrices chargées: {npz_path}")
        
        # 2. Charger métadonnées JSON
        meta_path = config_path / "calibration_meta.json"
        if meta_path.exists():
            with open(meta_path, 'r') as f:
                meta = json.load(f)
            
            calib.calibration_date = meta.get('calibration_date')
            calib.pixels_per_meter = meta['scale']['pixels_per_meter']
            calib.marker_size_m = meta['scale']['marker_size_m']
            calib.proj_width = meta['projector']['width']
            calib.proj_height = meta['projector']['height']
            calib.margin = meta['projector']['margin']
            print(f"[CALIB] Métadonnées chargées: {meta_path}")
            
        return calib


class StandaloneCalibrationWizard:
    """
    Wizard de calibration standalone.
    Calcule une homographie directe Caméra → Projecteur.
    """
    
    # Configuration par défaut (peut être overridé)
    DEFAULT_CONFIG = {
        'proj_width': 1024,
        'proj_height': 768,
        'offset_x': 1920,
        'offset_y': 0,
        'margin': 50,
        'marker_size_m': 0.10,
        'marker_size_px': 150
    }
    
    def __init__(self, config: dict = None):
        """
        Initialise le wizard.
        
        Args:
            config: Dictionnaire de configuration optionnel
        """
        cfg = {**self.DEFAULT_CONFIG, **(config or {})}
        
        self.proj_w = cfg['proj_width']
        self.proj_h = cfg['proj_height']
        self.offset_x = cfg['offset_x']
        self.offset_y = cfg['offset_y']
        self.margin = cfg['margin']
        self.marker_size_m = cfg['marker_size_m']
        self.marker_size_px = cfg['marker_size_px']
        
        # Hardware
        self.cam = None
        self.screen = None
        
        # ArUco
        self.aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
        self.aruco_params = cv2.aruco.DetectorParameters()
        self.detector = cv2.aruco.ArucoDetector(self.aruco_dict, self.aruco_params)
        
        # Store full config
        self.config = cfg
        
        # Fonts
        self.font = None
        self.small_font = None
        
        # State
        self.step = 0
        self.calibration = CalibrationData()
        self.running = True
        
    def _init_hardware(self):
        """Initialise la caméra et le projecteur."""
        print("[WIZARD] Initialisation Caméra RealSense...")
        # Utiliser la config si disponible, sinon défauts
        w = self.config.get('camera_width', 1280)
        h = self.config.get('camera_height', 720)
        fps = self.config.get('camera_fps', 30)
        
        self.cam = StandaloneCamera(width=w, height=h, fps=fps)
        self.calibration.camera_K = self.cam.K
        self.calibration.camera_D = self.cam.D
        
        print(f"[WIZARD] Initialisation Projecteur ({self.proj_w}x{self.proj_h} à {self.offset_x},{self.offset_y})...")
        os.environ['SDL_VIDEO_WINDOW_POS'] = f"{self.offset_x},{self.offset_y}"
        pygame.init()
        self.screen = pygame.display.set_mode(
            (self.proj_w, self.proj_h), 
            pygame.NOFRAME | pygame.DOUBLEBUF
        )
        pygame.mouse.set_visible(False)
        pygame.display.set_caption("Calibration Wizard")
        
        self.font = pygame.font.SysFont("Arial", 40, bold=True)
        self.small_font = pygame.font.SysFont("Arial", 24)
        
        # Store config in calibration data
        self.calibration.proj_width = self.proj_w
        self.calibration.proj_height = self.proj_h
        self.calibration.margin = self.margin
        self.calibration.marker_size_m = self.marker_size_m

    def _draw_text_center(self, text, y_off=0, color=(0, 0, 0), bg=(255, 255, 255)):
        """Dessine du texte centré."""
        surf = self.font.render(text, True, color, bg)
        rect = surf.get_rect(center=(self.proj_w // 2, self.proj_h // 2 + y_off))
        self.screen.blit(surf, rect)

    def _get_corner_positions(self):
        """
        Retourne les positions des 4 coins (centres des marqueurs).
        ORDRE CRITIQUE : Cohérent avec les IDs ArUco.
        """
        s = self.marker_size_px
        half = s // 2
        m = self.margin
        
        # IDs 0-3 dans l'ordre : Haut-Gauche, Haut-Droite, Bas-Droite, Bas-Gauche
        return {
            0: (m + half, m + half),                          # Top-Left
            1: (self.proj_w - m - half, m + half),            # Top-Right
            2: (self.proj_w - m - half, self.proj_h - m - half),  # Bottom-Right
            3: (m + half, self.proj_h - m - half)             # Bottom-Left
        }

    def _draw_markers(self, positions: dict, size: int):
        """Dessine les marqueurs ArUco aux positions spécifiées."""
        for marker_id, pos in positions.items():
            # Génère l'image du marqueur
            img = cv2.aruco.generateImageMarker(self.aruco_dict, marker_id, size)
            img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
            img_rgb = np.transpose(img_rgb, (1, 0, 2))
            surf = pygame.surfarray.make_surface(img_rgb)
            rect = surf.get_rect(center=pos)
            self.screen.blit(surf, rect)
            
            # Affiche l'ID
            lbl = self.small_font.render(str(marker_id), True, (0, 0, 0))
            lbl_rect = lbl.get_rect(center=pos)
            pygame.draw.rect(self.screen, (255, 255, 255), lbl_rect.inflate(4, 4))
            self.screen.blit(lbl, lbl_rect)

    def _handle_input(self) -> str:
        """Gère les entrées utilisateur. Retourne 'space', 'escape', ou None."""
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                return 'escape'
            elif event.type == pygame.KEYDOWN:
                if event.key == pygame.K_ESCAPE:
                    return 'escape'
                if event.key == pygame.K_SPACE:
                    return 'space'
        return None

    def run(self, output_path: str = None) -> CalibrationData:
        """
        Exécute le wizard de calibration.
        
        Args:
            output_path: Chemin de sauvegarde du fichier JSON (optionnel)
            
        Returns:
            CalibrationData: Données de calibration
        """
        self._init_hardware()
        
        try:
            while self.running:
                action = self._handle_input()
                if action == 'escape':
                    self.running = False
                    break
                
                self.screen.fill((255, 255, 255))
                
                if self.step == 0:
                    self._step_intro(action)
                elif self.step == 1:
                    self._step_geometric(action)
                elif self.step == 2:
                    self._step_metric(action)
                elif self.step == 3:
                    self._step_verify()
                    
                pygame.display.flip()
                
        finally:
            if self.cam:
                self.cam.stop()
            pygame.quit()
        
        # Sauvegarde si chemin fourni
        if output_path and self.calibration.H_CamToProj is not None:
            self.calibration.calibration_date = datetime.now().isoformat()
            self.calibration.save(output_path)
            
        return self.calibration

    def _step_intro(self, action):
        """Étape 0: Introduction."""
        self.screen.fill((0, 0, 0))
        self._draw_text_center("CALIBRATION WIZARD", -80, (0, 255, 0), (0, 0, 0))
        self._draw_text_center("1. Videz l'arène", 0, (255, 255, 255), (0, 0, 0))
        self._draw_text_center("2. Vérifiez que la caméra voit tout", 50, (255, 255, 255), (0, 0, 0))
        self._draw_text_center("ESPACE pour commencer", 120, (255, 255, 0), (0, 0, 0))
        
        if action == 'space':
            self.step = 1

    def _step_geometric(self, action):
        """Étape 1: Calibration géométrique (4 coins)."""
        corners = self._get_corner_positions()
        self._draw_markers(corners, self.marker_size_px)
        self._draw_text_center("La caméra voit-elle les 4 coins ?", 0)
        self._draw_text_center("ESPACE pour capturer", 50)
        
        if action == 'space':
            self._capture_geometric(corners)

    def _capture_geometric(self, expected_corners: dict):
        """Capture et calcule l'homographie géométrique."""
        print("[WIZARD] Capture géométrique...")
        img = self.cam.get_frame()
        corners, ids, _ = self.detector.detectMarkers(img)
        
        if ids is None or len(ids) < 4:
            found = len(ids) if ids is not None else 0
            print(f"[WIZARD] ERREUR: Seulement {found}/4 marqueurs détectés")
            if ids is not None:
                print(f"[WIZARD] IDs vus: {ids.flatten().tolist()}")
            return
        
        # Collecte les correspondances
        src_list = []  # Points caméra
        dst_list = []  # Points projecteur
        
        flat_ids = ids.flatten()
        for i, marker_id in enumerate(flat_ids):
            if marker_id in expected_corners:
                center_cam = corners[i][0].mean(axis=0)
                center_proj = expected_corners[marker_id]
                src_list.append(center_cam)
                dst_list.append(center_proj)
        
        if len(src_list) < 4:
            print("[WIZARD] ERREUR: IDs manquants (besoin 0,1,2,3)")
            return
        
        # Calcul homographie directe Caméra → Projecteur
        self.calibration.H_CamToProj, _ = cv2.findHomography(
            np.array(src_list), 
            np.array(dst_list)
        )
        print("[WIZARD] Homographie Caméra->Projecteur calculée!")
        self.step = 2

    def _step_metric(self, action):
        """Étape 2: Calibration métrique (échelle)."""
        self._draw_text_center("Posez le robot (ID 4 ou 5) au centre", -50)
        self._draw_text_center(f"Taille marqueur: {self.marker_size_m*100:.0f} cm", 0)
        self._draw_text_center("ESPACE pour mesurer l'échelle", 50)
        
        if action == 'space':
            self._capture_metric()

    def _capture_metric(self):
        """Calcule l'échelle pixels/mètre (Robuste: Médiane sur 60 frames)."""
        print("[WIZARD] Capture métrique... (Patientez ~2s)")
        
        scales = []
        target_frames = 60
        
        for _ in range(target_frames):
            img = self.cam.get_frame()
            corners, ids, _ = self.detector.detectMarkers(img)
            
            robot_idx = -1
            if ids is not None:
                for i, marker_id in enumerate(ids.flatten()):
                    if marker_id in [4, 5]:
                        robot_idx = i
                        break
            
            if robot_idx != -1:
                # Transforme les coins du marqueur vers l'espace projecteur
                c_cam = corners[robot_idx][0]
                c_proj = cv2.perspectiveTransform(
                    np.array([c_cam]), 
                    self.calibration.H_CamToProj
                )[0]
                
                # Mesure la taille en pixels projecteur
                w1 = np.linalg.norm(c_proj[0] - c_proj[1])
                h1 = np.linalg.norm(c_proj[1] - c_proj[2])
                avg_px = (w1 + h1) / 2.0
                
                current_scale = avg_px / self.marker_size_m
                scales.append(current_scale)
            
            # Petit délai pour laisser varier le bruit
            time.sleep(0.01)
            
        if not scales:
             print("[WIZARD] ERREUR: Aucun robot (ID 4/5) détecté sur la durée")
             return

        # Calcul robuste : Médiane
        self.calibration.pixels_per_meter = float(np.median(scales))
        print(f"[WIZARD] Échelle (Médiane/{len(scales)}): {self.calibration.pixels_per_meter:.2f} px/m")
        self.step = 3

    def _step_verify(self):
        """Étape 3: Vérification AR temps réel."""
        self.screen.fill((0, 0, 0))
        
        img = self.cam.get_frame()
        if img is None:
            return
        
        # Warp l'image caméra vers l'espace projecteur
        warped = cv2.warpPerspective(
            img, 
            self.calibration.H_CamToProj, 
            (self.proj_w, self.proj_h)
        )
        
        # Conversion pour Pygame
        warped = cv2.cvtColor(warped, cv2.COLOR_BGR2RGB)
        warped = np.transpose(warped, (1, 0, 2))
        surf = pygame.surfarray.make_surface(warped)
        
        # Grille de référence
        self._draw_grid()
        
        # Fantôme semi-transparent
        surf.set_alpha(150)
        self.screen.blit(surf, (0, 0))
        
        # Tracking robot en temps réel
        corners, ids, _ = self.detector.detectMarkers(img)
        if ids is not None:
            for i, marker_id in enumerate(ids.flatten()):
                if marker_id in [4, 5]:
                    c_cam = corners[i][0].mean(axis=0)
                    c_proj = cv2.perspectiveTransform(
                        np.array([[c_cam]]), 
                        self.calibration.H_CamToProj
                    )[0][0]
                    px, py = int(c_proj[0]), int(c_proj[1])
                    
                    # Point cyan pour le robot
                    pygame.draw.circle(self.screen, (0, 255, 255), (px, py), 10)
                    pygame.draw.circle(self.screen, (0, 255, 255), (px, py), 20, 2)
        
        # UI
        label = self.small_font.render(
            f"AR CHECK - Échelle: {self.calibration.pixels_per_meter:.1f} px/m | ESC=Quitter", 
            True, (0, 255, 0)
        )
        self.screen.blit(label, (10, 10))
        self._draw_text_center(
            "Le point cyan doit être sur le robot réel", 
            300, (255, 255, 0), (0, 0, 0)
        )

    def _draw_grid(self):
        """Dessine une grille de référence (50cm)."""
        if self.calibration.pixels_per_meter == 0:
            return
            
        px_step = int(0.5 * self.calibration.pixels_per_meter)
        if px_step <= 0:
            px_step = 100
        
        col = (0, 100, 0)
        for x in range(0, self.proj_w, px_step):
            pygame.draw.line(self.screen, col, (x, 0), (x, self.proj_h))
        for y in range(0, self.proj_h, px_step):
            pygame.draw.line(self.screen, col, (0, y), (self.proj_w, y))


def main():
    """Point d'entrée principal."""
    # Dossier de configuration
    project_root = Path(__file__).parent.parent.parent
    config_dir = project_root / "config"
    
    print(f"[WIZARD] Dossier config: {config_dir}")
    
    # Validation du chargement de la config caméra
    cam_width = 1280
    cam_height = 720
    cam_fps = 30
    
    camera_yaml = config_dir / "camera.yaml"
    if camera_yaml.exists():
        try:
            with open(camera_yaml, 'r') as f:
                cam_conf = yaml.safe_load(f)
                rs_conf = cam_conf.get('realsense', {})
                cam_width = rs_conf.get('width', 1280)
                cam_height = rs_conf.get('height', 720)
                cam_fps = rs_conf.get('fps', 30)
                print(f"[WIZARD] Config caméra chargée: {cam_width}x{cam_height} @ {cam_fps}fps")
        except Exception as e:
            print(f"[WIZARD] Erreur chargement camera.yaml: {e}")
            
    # Configuration (peut être modifiée)
    config = {
        'proj_width': 1024,
        'proj_height': 768,
        'offset_x': 1920,
        'offset_y': 0,
        'margin': 50,
        'marker_size_m': 0.10,
        'camera_width': cam_width,
        'camera_height': cam_height,
        'camera_fps': cam_fps
    }
    
    wizard = StandaloneCalibrationWizard(config)
    calibration = wizard.run(str(config_dir))  # Passe le dossier, pas le fichier
    
    if calibration.H_CamToProj is not None:
        print("\n" + "="*50)
        print(" CALIBRATION REUSSIE")
        print(f"  Échelle: {calibration.pixels_per_meter:.2f} px/m")
        print(f"  Fichiers: calibration.npz + calibration_meta.json")
        print("="*50)
    else:
        print("\n Calibration annulée ou échouée")


if __name__ == "__main__":
    main()


################################################################################
PATH: ./perception/camera/aruco_detector.py
################################################################################
"""
Détecteur ArUco - Détection de Marqueurs & Estimation de Pose

Détecte les marqueurs ArUco dans les images caméra :
- Marqueurs projetés (ID 0-3) : coins de l'arène pour l'étalonnage
- Marqueurs robots (ID 4, 5) : suivi des robots

Fournit :
- Positions centrales des marqueurs (pixels)
- Orientations des marqueurs (radians)
- Positions des coins pour l'estimation de l'échelle

Logs : [ARUCO] Détecté N marqueurs : [IDs]
"""

import cv2
import numpy as np
from typing import List, Dict, Tuple, Optional


class ArucoDetector:
    """
    Détection de marqueurs ArUco et estimation de pose.
    Utilise cv2.aruco pour la détection et cv2.solvePnP pour la pose 3D.
    """
    
    def __init__(self, 
                 dictionary_type=cv2.aruco.DICT_4X4_50,
                 marker_size_m: float = 0.10):
        """
        Initialise le détecteur ArUco.
        
        Args:
            dictionary_type: Dictionnaire ArUco (défaut : 4x4, 50 marqueurs)
            marker_size_m: Taille physique du marqueur en mètres (pour estimation échelle)
        """
        self.dictionary = cv2.aruco.getPredefinedDictionary(dictionary_type)
        self.parameters = cv2.aruco.DetectorParameters()
        
        # Compatibilité OpenCV: utilise ArucoDetector (4.7+) avec fallback vers API legacy
        try:
            self.detector = cv2.aruco.ArucoDetector(self.dictionary, self.parameters)
            self.use_legacy_api = False
        except AttributeError:
            # OpenCV < 4.7.0 n'a pas ArucoDetector, utiliser l'API legacy
            self.detector = None
            self.use_legacy_api = True
            print("[ARUCO] Utilisation de l'API legacy (OpenCV < 4.7)")
        
        self.marker_size_m = marker_size_m
        
        # Pré-calcul des points objets 3D pour un marqueur (centré à 0,0,0)
        # Ordre: Haut-Gauche, Haut-Droite, Bas-Droite, Bas-Gauche (Sens horaire)
        half_size = marker_size_m / 2.0
        self.obj_points = np.array([
            [-half_size, half_size, 0],
            [half_size, half_size, 0],
            [half_size, -half_size, 0],
            [-half_size, -half_size, 0]
        ], dtype=np.float32)
        
    def detect(self, image: np.ndarray) -> Dict[int, Dict]:
        """
        Détecte les marqueurs ArUco dans l'image.
        """
        # Convertit en niveaux de gris si nécessaire
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image
        
        # Détecte les marqueurs (API moderne ou legacy selon version OpenCV)
        if self.use_legacy_api:
            corners, ids, rejected = cv2.aruco.detectMarkers(gray, self.dictionary, parameters=self.parameters)
        else:
            corners, ids, rejected = self.detector.detectMarkers(gray)
        
        results = {}
        
        if ids is not None:
            for i, marker_id in enumerate(ids.flatten()):
                marker_corners = corners[i][0]  # Shape: (4, 2)
                
                # Calcule le centre
                center = marker_corners.mean(axis=0)
                
                # Calcule l'orientation (du coin 0 au coin 1)
                dx = marker_corners[1][0] - marker_corners[0][0]
                dy = marker_corners[1][1] - marker_corners[0][1]
                orientation = np.arctan2(dy, dx)
                
                results[marker_id] = {
                    'center': tuple(center),
                    'corners': [tuple(c) for c in marker_corners],
                    'orientation': orientation
                }
            
            # Décommentez pour debug verbeux si besoin
            # print("[ARUCO] Détecté {} marqueurs : {}".format(len(ids), ids.flatten().tolist()))
        
        return results
    
    def estimate_marker_size_av(self, marker_corners, H_C2AV):
        """
        Estime la taille du marqueur en unités de l'Arène Virtuelle.
        Utilisé pendant l'étalonnage.
        """
        corners_av = []
        for u, v in marker_corners:
            p_cam = np.array([u, v, 1.0])
            p_av = H_C2AV @ p_cam
            p_av = p_av[:2] / p_av[2]
            corners_av.append(p_av)
        
        corners_av = np.array(corners_av)
        side_lengths = []
        for i in range(4):
            j = (i + 1) % 4
            length = np.linalg.norm(corners_av[j] - corners_av[i])
            side_lengths.append(length)
        
        return np.mean(side_lengths)
    
    def get_corrected_pose(self, corners: np.ndarray, mtx: np.ndarray, dist: np.ndarray, 
                          obj_height_m: float = 0.30) -> Tuple[Tuple[float, float], float]:
        """
        Calcule la position corrigée de la parallaxe.
        Remplace estimatePoseSingleMarkers par solvePnP (plus robuste).
        """
        # Formater les coins image pour solvePnP (doit être float32)
        # corners arrive souvent en shape (1, 4, 2) ou (4, 2)
        img_points = corners.reshape(4, 2).astype(np.float32)
        
        # 1. Estimation de Pose (PnP)
        # Trouve la rotation et translation du marqueur par rapport à la caméra
        success, rvec, tvec = cv2.solvePnP(self.obj_points, img_points, mtx, dist, flags=cv2.SOLVEPNP_IPPE_SQUARE)
        
        if not success:
            # Fallback si PnP échoue
            center = img_points.mean(axis=0)
            return (center[0], center[1]), 2.5
            
        # Z est la distance caméra <-> marqueur (composante Z du vecteur translation)
        z_camera = float(tvec[2])
        
        # Sécurité division par zéro
        if z_camera < 0.1: z_camera = 0.1
            
        # 2. Facteur de correction (Thalès)
        correction_factor = 1.0 - (obj_height_m / z_camera)
        
        # 3. Correction géométrique vers le centre optique
        cx, cy = mtx[0, 2], mtx[1, 2]
        
        # Centre détecté (brut)
        u_raw, v_raw = img_points.mean(axis=0)
        
        # Vecteur radial
        dx = u_raw - cx
        dy = v_raw - cy
        
        # Application
        u_corr = cx + dx * correction_factor
        v_corr = cy + dy * correction_factor
        
        return (u_corr, v_corr), z_camera

    def draw_detections(self, image: np.ndarray, detections: Dict) -> np.ndarray:
        """Dessine les marqueurs détectés sur l'image."""
        img_draw = image.copy()
        for marker_id, data in detections.items():
            center = data['center']
            corners = data['corners']
            corners_array = np.array(corners, dtype=np.int32)
            cv2.polylines(img_draw, [corners_array], True, (0, 255, 0), 2)
            cv2.circle(img_draw, (int(center[0]), int(center[1])), 5, (0, 0, 255), -1)
            cv2.putText(img_draw, f"ID:{marker_id}", 
                       (int(center[0]), int(center[1]) - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)
        return img_draw


################################################################################
PATH: ./perception/camera/color_segmentation.py
################################################################################
"""
Color Segmentation - Détection Obstacles par Seuillage

Segmente les obstacles sur fond blanc par seuillage couleur:
- Détection zones sombres (obstacles)
- Masques binaires
- Filtrage bruit

Utilisé pendant la calibration pour cartographier obstacles statiques.

Logs: [SEGMENT] prefix
"""

import cv2
import numpy as np
from typing import Tuple


def threshold_obstacles(image: np.ndarray, 
                       threshold_value: int = 200) -> np.ndarray:
    """
    Seuillage simple pour détecter obstacles sur fond blanc.
    
    Args:
        image: Image BGR ou grayscale
        threshold_value: Seuil (pixels < threshold = obstacles)
        
    Returns:
        Masque binaire (0 = libre, 255 = obstacle)
    """
    # Convertir en niveaux de gris si nécessaire
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image
    
    # Seuillage inverse (obstacles sont sombres)
    _, binary = cv2.threshold(gray, threshold_value, 255, cv2.THRESH_BINARY_INV)
    
    return binary


def adaptive_threshold_obstacles(image: np.ndarray) -> np.ndarray:
    """
    Seuillage adaptatif pour conditions éclairage variables.
    
    Args:
        image: Image BGR ou grayscale
        
    Returns:
        Masque binaire
    """
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image
    
    # Seuillage adaptatif
    binary = cv2.adaptiveThreshold(
        gray, 255, 
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY_INV,
        blockSize=11,
        C=2
    )
    
    return binary


def remove_noise(binary_mask: np.ndarray, 
                kernel_size: int = 5) -> np.ndarray:
    """
    Retire le bruit du masque binaire.
    
    Args:
        binary_mask: Masque binaire
        kernel_size: Taille kernel morphologie
        
    Returns:
        Masque filtré
    """
    kernel = np.ones((kernel_size, kernel_size), np.uint8)
    
    # Opening (erosion puis dilatation) pour retirer petits points
    opened = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel)
    
    # Closing (dilatation puis erosion) pour remplir trous
    closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel)
    
    return closed


def segment_obstacles(image: np.ndarray, 
                     method: str = 'simple',
                     denoise: bool = True) -> np.ndarray:
    """
    Pipeline complet de segmentation obstacles.
    
    Args:
        image: Image source
        method: 'simple' ou 'adaptive'
        denoise: Appliquer filtrage bruit
        
    Returns:
        Masque binaire nettoyé
        
    Logs:
        [SEGMENT] Obstacles detected: N pixels
    """
    if method == 'adaptive':
        mask = adaptive_threshold_obstacles(image)
    else:
        mask = threshold_obstacles(image)
    
    if denoise:
        mask = remove_noise(mask)
    
    # Compter pixels obstacles
    obstacle_pixels = np.count_nonzero(mask)
    print("[SEGMENT] Obstacles détectés : {} pixels".format(obstacle_pixels))
    
    return mask


################################################################################
PATH: ./perception/camera/homography.py
################################################################################
"""
Homography - Calculs Transformations Homographiques

Calcule et applique les homographies:
- H_C2AV: Caméra -> Arène Virtuelle
- H_AV2W: Arène Virtuelle -> Monde (scaling)
- H_C2W: Caméra -> Monde (combinée)

Utilisé par calibration_wizard et coordinate_frames.

Logs: [HOMOGRAPHY] prefix
"""

import cv2
import numpy as np
from typing import List, Tuple


def compute_homography(src_points: np.ndarray,
                      dst_points: np.ndarray) -> np.ndarray:
    """
    Calcule homographie entre points source et destination.
    
    Args:
        src_points: Points source (Nx2) en pixels caméra
        dst_points: Points destination (Nx2) en coordonnées cible
        
    Returns:
        Matrice homographie 3x3
        
    Raises:
        ValueError: Si moins de 4 points
        
    Logs:
        [HOMOGRAPHY] Computed from N points
    """
    if len(src_points) < 4 or len(dst_points) < 4:
        raise ValueError("Au moins 4 points requis pour homographie")
    
    # Assurer type float32
    src = np.array(src_points, dtype=np.float32)
    dst = np.array(dst_points, dtype=np.float32)
    
    # Calculer homographie
    H, _ = cv2.findHomography(src, dst)
    
    print("[HOMOGRAPHY] Calculé depuis {} points".format(len(src_points)))
    
    return H


def apply_homography(points: np.ndarray, H: np.ndarray) -> np.ndarray:
    """
    Applique homographie à des points.
    
    Args:
        points: Points à transformer (Nx2)
        H: Matrice homographie 3x3
        
    Returns:
        Points transformés (Nx2)
    """
    # Convertir en coordonnées homogènes
    ones = np.ones((points.shape[0], 1))
    points_h = np.hstack([points, ones])
    
    # Appliquer transformation
    transformed_h = (H @ points_h.T).T
    
    # Normaliser (diviser par coordonnée w)
    transformed = transformed_h[:, :2] / transformed_h[:, 2:3]
    
    return transformed


def apply_homography_single(point: Tuple[float, float], 
                           H: np.ndarray) -> Tuple[float, float]:
    """
    Applique homographie à un point unique.
    
    Args:
        point: (x, y) point source
        H: Matrice homographie
        
    Returns:
        (x', y') point transformé
    """
    # Coordonnées homogènes
    p_h = np.array([point[0], point[1], 1.0])
    
    # Transformation
    p_transformed = H @ p_h
    
    # Normalisation
    x = p_transformed[0] / p_transformed[2]
    y = p_transformed[1] / p_transformed[2]
    
    return (x, y)


def create_scaling_matrix(scale: float) -> np.ndarray:
    """
    Crée matrice de scaling homogène.
    
    Args:
        scale: Facteur d'échelle
        
    Returns:
        Matrice 3x3
    """
    S = np.array([
        [scale, 0, 0],
        [0, scale, 0],
        [0, 0, 1]
    ], dtype=np.float32)
    
    return S


def combine_homographies(H1: np.ndarray, H2: np.ndarray) -> np.ndarray:
    """
    Combine deux homographies: H_combined = H2 @ H1.
    
    Args:
        H1: Première transformation
        H2: Deuxième transformation
        
    Returns:
        Homographie combinée
    """
    return H2 @ H1


def estimate_scale_from_marker(marker_corners_px: List[Tuple[float, float]],
                               H_C2AV: np.ndarray,
                               real_size_m: float) -> float:
    """
    Estime échelle métrique depuis marqueur ArUco.
    
    Args:
        marker_corners_px: 4 coins marqueur en pixels caméra
        H_C2AV: Homographie Caméra -> Arène Virtuelle
        real_size_m: Taille réelle marqueur en mètres
        
    Returns:
        Scale en mètres/unité AV
        
    Algorithm:
        1. Transformer coins en AV
        2. Calculer taille moyenne en AV
        3. scale = real_size_m / size_av
        
    Logs:
        [HOMOGRAPHY] Scale estimation: real=Xm, av=Y units -> scale=Z m/unit
    """
    # Transformer coins en AV
    corners_av = apply_homography(np.array(marker_corners_px), H_C2AV)
    
    # Calculer longueurs des 4 côtés
    side_lengths = []
    for i in range(4):
        j = (i + 1) % 4
        length = np.linalg.norm(corners_av[j] - corners_av[i])
        side_lengths.append(length)
    
    # Moyenne
    avg_size_av = np.mean(side_lengths)
    
    # Échelle
    scale = real_size_m / avg_size_av
    
    print("[HOMOGRAPHY] Estimation échelle : réel={:.3f}m, "
          "av={:.3f} unités -> échelle={:.3f} m/unité".format(real_size_m, avg_size_av, scale))
    
    return scale


################################################################################
PATH: ./perception/camera/__init__.py
################################################################################


################################################################################
PATH: ./perception/camera/kalman_filter.py
################################################################################
"""
Filtre de Kalman - Suivi de Pose Robot

Filtre de Kalman Étendu (EKF) pour l'estimation d'état du robot :
- État : [x, y, vx, vy, theta, omega]
- Mesures : [x, y, theta] depuis ArUco
- Prédiction : modèle à vitesse constante

Lisse les détections ArUco bruitées et estime les vitesses.

Logs : [KALMAN] RobotX state: x=X, y=Y, theta=T, vx=VX, vy=VY
"""

import numpy as np
from typing import Tuple


class KalmanFilter:
    """
    Filtre de Kalman Étendu pour l'estimation de pose et vitesse du robot 2D.
    
    Vecteur d'état : [x, y, vx, vy, theta, omega]
    """
    
    def __init__(self, dt: float = 1/30.0):
        """
        Initialise le filtre de Kalman.
        
        Args:
            dt: Pas de temps (défaut 30 FPS = 0.033s)
        """
        self.dt = dt
        
        # État : [x, y, vx, vy, theta, omega]
        self.state = np.zeros(6)
        
        # Covariance de l'état
        self.P = np.eye(6) * 1.0
        
        # Bruit de processus
        self.Q = np.diag([0.01, 0.01, 0.1, 0.1, 0.01, 0.1])
        
        # Bruit de mesure
        self.R = np.diag([0.05, 0.05, 0.1])  # [x, y, theta]
        
    def predict(self, dt: float = None):
        """
        Étape de prédiction : propage l'état vers l'avant.
        
        Args:
            dt: Pas de temps en secondes. Si None, utilise self.dt.
                IMPORTANT: Pour une précision optimale, passez le vrai
                temps écoulé depuis le dernier appel à predict().
        
        Transition d'état :
            x += vx * dt
            y += vy * dt
            vx (constant)
            vy (constant)
            theta += omega * dt
            omega (constant)
        """
        # Utilise le dt passé ou le dt par défaut
        if dt is not None:
            self.dt = dt
        
        # Matrice de transition d'état
        F = np.array([
            [1, 0, self.dt, 0, 0, 0],
            [0, 1, 0, self.dt, 0, 0],
            [0, 0, 1, 0, 0, 0],
            [0, 0, 0, 1, 0, 0],
            [0, 0, 0, 0, 1, self.dt],
            [0, 0, 0, 0, 0, 1]
        ])
        
        # Prédit l'état
        self.state = F @ self.state
        
        # Normalise theta
        self.state[4] = np.arctan2(np.sin(self.state[4]), np.cos(self.state[4]))
        
        # Prédit la covariance
        self.P = F @ self.P @ F.T + self.Q
        
    def update(self, measurement: Tuple[float, float, float]):
        """
        Étape de mise à jour : incorpore la mesure.
        
        Args:
            measurement: (x, y, theta) depuis la détection ArUco
        """
        # Matrice de mesure (observe x, y, theta)
        H = np.array([
            [1, 0, 0, 0, 0, 0],
            [0, 1, 0, 0, 0, 0],
            [0, 0, 0, 0, 1, 0]
        ])
        
        z = np.array(measurement)
        
        # Innovation
        y = z - H @ self.state
        
        # Normalise l'innovation angulaire
        y[2] = np.arctan2(np.sin(y[2]), np.cos(y[2]))
        
        # Covariance de l'innovation
        S = H @ self.P @ H.T + self.R
        
        # Gain de Kalman
        K = self.P @ H.T @ np.linalg.inv(S)
        
        # Met à jour l'état
        self.state = self.state + K @ y
        
        # Normalise theta
        self.state[4] = np.arctan2(np.sin(self.state[4]), np.cos(self.state[4]))
        
        # Met à jour la covariance
        self.P = (np.eye(6) - K @ H) @ self.P
        
    def get_pose(self) -> Tuple[float, float, float]:
        """
        Obtient l'estimation de pose actuelle.
        
        Returns:
            (x, y, theta)
        """
        return (self.state[0], self.state[1], self.state[4])
    
    def get_velocity(self) -> Tuple[float, float, float]:
        """
        Obtient l'estimation de vitesse actuelle.
        
        Returns:
            (vx, vy, omega)
        """
        return (self.state[2], self.state[3], self.state[5])
    
    def get_full_state(self) -> np.ndarray:
        """
        Obtient le vecteur d'état complet.
        
        Returns:
            [x, y, vx, vy, theta, omega]
        """
        return self.state.copy()
    
    def reset(self, initial_pose: Tuple[float, float, float]):
        """
        Réinitialise le filtre avec une nouvelle pose initiale.
        
        Args:
            initial_pose: (x, y, theta)
        """
        self.state = np.array([
            initial_pose[0],  # x
            initial_pose[1],  # y
            0.0,              # vx
            0.0,              # vy
            initial_pose[2],  # theta
            0.0               # omega
        ])
        
        self.P = np.eye(6) * 1.0


################################################################################
PATH: ./perception/camera/realsense_stream.py
################################################################################
"""
Flux RealSense - Interface Caméra Intel RealSense

Gère la caméra RealSense D435/D455 :
- Acquisition flux couleur
- Flux profondeur (optionnel)
- Configuration caméra
- Gestion fréquence d'images

Fournit frames couleur et profondeur synchronisées à 30 FPS.

Logs : préfixe [REALSENSE] pour les opérations caméra
"""

import pyrealsense2 as rs
import numpy as np
from typing import Tuple, Optional


class RealSenseStream:
    """
    Interface pour caméra Intel RealSense.
    
    Gère l'initialisation de la caméra et l'acquisition des frames.
    """
    
    def __init__(self, 
                 width: int = 1280, 
                 height: int = 720, 
                 fps: int = 30,
                 enable_depth: bool = False):
        """
        Initialise la caméra RealSense.
        
        Args:
            width: Largeur frame
            height: Hauteur frame
            fps: Fréquence d'images
            enable_depth: Activer flux profondeur
            
        Logs:
            [REALSENSE] Camera initialized: WxH @ FPS fps
        """
        self.width = width
        self.height = height
        self.fps = fps
        self.enable_depth = enable_depth
        
        self.pipeline = None
        self.config = None
        
    def start(self):
        """
        Démarre le pipeline caméra.
        
        Logs:
            [REALSENSE] Pipeline démarré
            [REALSENSE] Échec du démarrage : erreur
        """
        try:
            self.pipeline = rs.pipeline()
            self.config = rs.config()
            
            # Configure les flux
            self.config.enable_stream(rs.stream.color, 
                                     self.width, self.height, 
                                     rs.format.bgr8, self.fps)
            
            if self.enable_depth:
                self.config.enable_stream(rs.stream.depth, 
                                         self.width, self.height, 
                                         rs.format.z16, self.fps)
            
            # Démarre pipeline
            self.pipeline.start(self.config)
            
            print("[REALSENSE] Pipeline démarré : {}x{} @ {} fps".format(self.width, self.height, self.fps))
            
        except Exception as e:
            print("[REALSENSE] Échec du démarrage : {}".format(e))
            raise
    
    def get_intrinsics_matrix(self):
        """
        Retourne la matrice de caméra (K) et les coefficients de distorsion (D).
        """
        if self.pipeline:
            try:
                profile = self.pipeline.get_active_profile()
                color_stream = profile.get_stream(rs.stream.color)
                intr = color_stream.as_video_stream_profile().get_intrinsics()
                
                # Matrice K (3x3)
                K = np.array([
                    [intr.fx, 0, intr.ppx],
                    [0, intr.fy, intr.ppy],
                    [0, 0, 1]
                ])
                
                # Coefficients D (Distortion)
                D = np.array(intr.coeffs)
                
                return K, D
            except Exception as e:
                print(f"[REALSENSE] Erreur récupération intrinsics : {e}")
                return None, None
        return None, None

    def stop(self):
        """Arrête le pipeline caméra."""
        if self.pipeline:
            self.pipeline.stop()
            print("[REALSENSE] Pipeline arrêté")
    
    def get_frames(self) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
        """
        Obtient les dernières frames couleur et profondeur.
        
        Returns:
            (color_frame, depth_frame): tableaux numpy
            color_frame: Image BGR HxWx3
            depth_frame: Carte profondeur HxW (mm) ou None
        """
        if not self.pipeline:
            return (None, None)
        
        try:
            # Attend les frames
            frames = self.pipeline.wait_for_frames()
            
            # Obtient frame couleur
            color_frame = frames.get_color_frame()
            color_image = np.asanyarray(color_frame.get_data()) if color_frame else None
            
            # Obtient frame profondeur (si activé)
            depth_image = None
            if self.enable_depth:
                depth_frame = frames.get_depth_frame()
                depth_image = np.asanyarray(depth_frame.get_data()) if depth_frame else None
            
            return (color_image, depth_image)
            
        except Exception as e:
            print("[REALSENSE] Erreur d'acquisition frame : {}".format(e))
            return (None, None)
    
    def get_intrinsics(self):
        """
        Obtient les paramètres intrinsèques de la caméra.
        
        Returns:
            objet rs.intrinsics avec fx, fy, cx, cy
        """
        if self.pipeline:
            profile = self.pipeline.get_active_profile()
            color_stream = profile.get_stream(rs.stream.color)
            intrinsics = color_stream.as_video_stream_profile().get_intrinsics()
            return intrinsics
        return None


################################################################################
PATH: ./perception/__init__.py
################################################################################


################################################################################
PATH: ./perception/preprocessing/contours.py
################################################################################
"""
Contours - Extraction Contours

Extraction et traitement des contours d'image.

Logs: [CONTOURS] prefix
"""

import cv2
import numpy as np
from typing import List, Tuple


def find_contours(binary_image: np.ndarray) -> List[np.ndarray]:
    """
    Trouve contours dans image binaire.
    
    Args:
        binary_image: Image binaire
        
    Returns:
        Liste de contours
    """
    contours, _ = cv2.findContours(
        binary_image,
        cv2.RETR_EXTERNAL,
        cv2.CHAIN_APPROX_SIMPLE
    )
    
    print("[CONTOURS] Trouvé {} contours".format(len(contours)))
    
    return contours


def filter_contours_by_area(contours: List[np.ndarray],
                           min_area: float = 100.0,
                           max_area: float = np.inf) -> List[np.ndarray]:
    """
    Filtre contours par aire.
    
    Args:
        contours: Liste contours
        min_area: Aire minimum
        max_area: Aire maximum
        
    Returns:
        Contours filtrés
    """
    filtered = []
    for cnt in contours:
        area = cv2.contourArea(cnt)
        if min_area <= area <= max_area:
            filtered.append(cnt)
    
    print("[CONTOURS] Filtré {} -> {} contours".format(len(contours), len(filtered)))
    
    return filtered


def get_bounding_boxes(contours: List[np.ndarray]) -> List[Tuple[int, int, int, int]]:
    """
    Extrait rectangles englobants.
    
    Args:
        contours: Liste contours
        
    Returns:
        Liste (x, y, w, h) rectangles
    """
    boxes = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        boxes.append((x, y, w, h))
    
    return boxes


def approximate_polygons(contours: List[np.ndarray],
                        epsilon_factor: float = 0.02) -> List[np.ndarray]:
    """
    Approxime contours par polygones.
    
    Args:
        contours: Liste contours
        epsilon_factor: Facteur précision (% périmètre)
        
    Returns:
        Contours approximés
    """
    approximated = []
    for cnt in contours:
        epsilon = epsilon_factor * cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, epsilon, True)
        approximated.append(approx)
    
    return approximated


################################################################################
PATH: ./perception/preprocessing/image_utils.py
################################################################################
"""
Image Utils - Utilitaires Traitement Image

Fonctions utilitaires pour traitement d'images.

Logs: [IMG_UTILS] prefix
"""

import cv2
import numpy as np
from typing import Tuple


def convert_to_grayscale(image: np.ndarray) -> np.ndarray:
    """
    Convertit image en niveaux de gris.
    
    Args:
        image: Image BGR ou RGB
        
    Returns:
        Image grayscale
    """
    if len(image.shape) == 2:
        return image  # Déjà grayscale
    
    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)


def resize_image(image: np.ndarray,
                width: int,
                height: int,
                interpolation=cv2.INTER_LINEAR) -> np.ndarray:
    """
    Redimensionne image.
    
    Args:
        image: Image source
        width: Nouvelle largeur
        height: Nouvelle hauteur
        interpolation: Méthode interpolation
        
    Returns:
        Image redimensionnée
    """
    return cv2.resize(image, (width, height), interpolation=interpolation)


def gaussian_blur(image: np.ndarray, kernel_size: int = 5) -> np.ndarray:
    """
    Applique flou gaussien.
    
    Args:
        image: Image source
        kernel_size: Taille kernel (impair)
        
    Returns:
        Image floutée
    """
    return cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)


def morphological_open(image: np.ndarray, kernel_size: int = 5) -> np.ndarray:
    """
    Opening morphologique (erosion puis dilatation).
    
    Args:
        image: Image binaire
        kernel_size: Taille kernel
        
    Returns:
        Image traitée
    """
    kernel = np.ones((kernel_size, kernel_size), np.uint8)
    return cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)


def morphological_close(image: np.ndarray, kernel_size: int = 5) -> np.ndarray:
    """
    Closing morphologique (dilatation puis erosion).
    
    Args:
        image: Image binaire
        kernel_size: Taille kernel
        
    Returns:
        Image traitée
    """
    kernel = np.ones((kernel_size, kernel_size), np.uint8)
    return cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)


def warp_perspective(image: np.ndarray,
                     H: np.ndarray,
                     output_size: Tuple[int, int]) -> np.ndarray:
    """
    Applique transformation perspective (warp).
    
    Args:
        image: Image source
        H: Matrice homographie 3x3
        output_size: (width, height) sortie
        
    Returns:
        Image transformée
    """
    return cv2.warpPerspective(image, H, output_size)


def equalize_histogram(image: np.ndarray) -> np.ndarray:
    """
    Égalisation histogramme.
    
    Args:
        image: Image grayscale
        
    Returns:
        Image égalisée
    """
    return cv2.equalizeHist(image)


################################################################################
PATH: ./perception/preprocessing/__init__.py
################################################################################


################################################################################
PATH: ./perception/preprocessing/thresholding.py
################################################################################
"""
Thresholding - Seuillage Image

Fonctions de seuillage pour détection obstacles.

Logs: [THRESHOLD] prefix
"""

import cv2
import numpy as np


def simple_threshold(image: np.ndarray,
                    threshold: int = 200,
                    max_value: int = 255) -> np.ndarray:
    """
    Seuillage binaire simple.
    
    Args:
        image: Image grayscale
        threshold: Valeur seuil
        max_value: Valeur maximum
        
    Returns:
        Image binaire
    """
    _, binary = cv2.threshold(image, threshold, max_value, cv2.THRESH_BINARY)
    return binary


def inverse_threshold(image: np.ndarray,
                     threshold: int = 200) -> np.ndarray:
    """
    Seuillage inverse (pour obstacles sombres sur fond clair).
    
    Args:
        image: Image grayscale
        threshold: Valeur seuil
        
    Returns:
        Image binaire (obstacles = 255)
    """
    _, binary = cv2.threshold(image, threshold, 255, cv2.THRESH_BINARY_INV)
    return binary


def otsu_threshold(image: np.ndarray) -> np.ndarray:
    """
    Seuillage automatique Otsu.
    
    Args:
        image: Image grayscale
        
    Returns:
        Image binaire
    """
    _, binary = cv2.threshold(image, 0, 255, 
                              cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    return binary


def adaptive_threshold(image: np.ndarray,
                      block_size: int = 11,
                      C: int = 2) -> np.ndarray:
    """
    Seuillage adaptatif.
    
    Args:
        image: Image grayscale
        block_size: Taille bloc voisinage (impair)
        C: Constante soustraite de moyenne
        
    Returns:
        Image binaire
    """
    binary = cv2.adaptiveThreshold(
        image, 255,
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY,
        block_size, C
    )
    return binary


################################################################################
PATH: ./renderer/game_renderer.py
################################################################################
#!/usr/bin/env python3
import sys
import os
import time
import json
import pygame
import numpy as np
import uuid
from pathlib import Path

# Setup Paths
ROOT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT_DIR))

from core.world.unified_transform import load_calibration
from core.utils.state_file import StateFile

# --- CONFIGURATION ---
STATE_FILE = "renderer/game_state.json"

# Couleurs
C_BG = (10, 10, 20)
C_GRID = (30, 30, 50)
C_AI = (0, 150, 255)
C_HUMAN = (255, 50, 0)
C_TEXT = (255, 255, 255)
C_GOLD = (255, 215, 0)
C_LASER = (0, 255, 0)

class GameRenderer:
    """
    Moteur de Rendu du Jeu.
    Peut être utilisé :
    1. En standalone (main) via la lecture du JSON.
    2. Comme librairie importée par le Game Manager (update direct).
    """
    def __init__(self, config_path=None, transform_manager=None, config=None):
        if transform_manager:
            self.tm = transform_manager
        else:
            if not config_path:
                config_path = str(ROOT_DIR / 'config')
            self.tm = load_calibration(config_path)
            
        # Optional: update visual config if provided
        if config:
            # Merge logic or simplistic override
            pass # We will handle this below
        
        # Init Defaults
        proj_w, proj_h = 1024, 768
        proj_off_x = 0
        
        # Default Visuals
        self.vis_cfg = {
            'speed': 800.0,
            'colors': {
                'bg': (10, 10, 20), 'grid': (30, 30, 50),
                'ai': (0, 150, 255), 'human': (255, 50, 0),
                'hitbox': (255, 255, 0), 'text': (255, 255, 255),
                'gold': (255, 215, 0), 'laser': (0, 255, 0)
            },
            'hitbox_r': 0.25
        }

        try:
             import yaml
             # Si config injectée, on l'utilise, sinon on charge du fichier
             if config:
                 cfg = config
             else:
                 cfg_file = Path(config_path) / 'projector.yaml'
                 if cfg_file.exists():
                     with open(cfg_file) as f:
                         cfg = yaml.safe_load(f)
                 else:
                     cfg = {}

             # 1. Display
             proj = cfg.get('projector', {})
             disp = cfg.get('display', {})
             proj_w = proj.get('width', 1024)
             proj_h = proj.get('height', 768)
             proj_off_x = disp.get('monitor_offset_x', 0)
                     
             # 2. Visuals
             vis = cfg.get('visuals', {})
             self.vis_cfg['speed'] = vis.get('projectile_speed_px_s', 800.0)
             
             c = vis.get('colors', {})
             if c:
                self.vis_cfg['colors']['bg'] = tuple(c.get('background', (10,10,20)))
                self.vis_cfg['colors']['grid'] = tuple(c.get('grid', (30,30,50)))
                self.vis_cfg['colors']['ai'] = tuple(c.get('ai', (0,150,255)))
                self.vis_cfg['colors']['human'] = tuple(c.get('human', (255,50,0)))
                self.vis_cfg['colors']['hitbox'] = tuple(c.get('hitbox', (255,255,0)))
                
             hb = vis.get('hitbox', {})
             self.vis_cfg['hitbox_r'] = hb.get('radius_m', 0.25)

        except Exception as e:
            print(f"[RENDER] Config Error: {e}")
        
        os.environ['SDL_VIDEO_WINDOW_POS'] = f"{proj_off_x},0"
        pygame.init()
        self.screen = pygame.display.set_mode((proj_w, proj_h), pygame.NOFRAME)
        self.clock = pygame.time.Clock()
        
        # Assets
        self.font_score = pygame.font.SysFont("Impact", 60)
        self.font_info = pygame.font.SysFont("Consolas", 18)
        self.font_big = pygame.font.SysFont("Impact", 120)
        
        # Internal FX State
        self.floating_texts = []
        self.projectiles = []
        self.processed_events = set()
        
        self.last_ai_score = 0
        self.last_human_score = 0
        self.last_time = time.time()
        
        self.running = True

    def process_events(self):
        """Gère la boucle d'événements Pygame (Quit, etc.). Retourne False si demande d'arrêt."""
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                self.running = False
            elif event.type == pygame.KEYDOWN:
                if event.key == pygame.K_ESCAPE:
                    self.running = False
        return self.running

    def render(self, state):
        """Dessine une frame en fonction de l'état fourni (Dict)."""
        current_time = time.time()
        dt = current_time - self.last_time
        self.last_time = current_time
        
        self.screen.fill(C_BG)
        
        if not state:
            self._draw_overlay("WAITING FOR SIGNAL...")
        else:
            self._process_logic(state, dt)
            self._draw_grid()
            self._draw_entities(state)
            self._draw_fx(dt)
            self._draw_hud(state)
            
        pygame.display.flip()
        self.clock.tick(60)

    def close(self):
        pygame.quit()

    # --- SIMULATION HELPERS (Public API) ---
    def trigger_shot(self, start_pos, end_pos, is_ai):
        """Ajoute un projectile visuel (utile si appelé directement sans passer par JSON event)."""
        self._add_projectile(start_pos, end_pos, is_ai)

    def trigger_hit(self, pos, color=C_GOLD):
        """Ajoute un effet de hit/score."""
        self._add_floating_text("+1", pos[0], pos[1], color)

    # --- INTERNAL RENDERING LOGIC ---
    def _process_logic(self, state, dt):
        # 1. Score auto-animation
        scores = state.get('scores', {})
        s_ai = scores.get('ai', 0)
        s_hu = scores.get('human', 0)
        ents = state.get('entities', {})

        if s_ai > self.last_ai_score:
            pos = ents.get('ai', {}).get('pos', [self.tm.arena_width_m*0.2, 0.5])
            self._add_floating_text("+1", pos[0], pos[1], self.vis_cfg['colors']['ai'])
        
        if s_hu > self.last_human_score:
            pos = ents.get('human', {}).get('pos', [self.tm.arena_width_m*0.8, 0.5])
            self._add_floating_text("+1", pos[0], pos[1], self.vis_cfg['colors']['human'])
            
        self.last_ai_score = s_ai
        self.last_human_score = s_hu
        
        # 2. Events from JSON
        if len(self.processed_events) > 100:
            self.processed_events.clear()
            
        for evt in state.get('events', []):
            eid = evt.get('id')
            if eid and eid in self.processed_events:
                continue 
            if eid: self.processed_events.add(eid)
            
            if evt.get('type') == 'shot':
                p1 = evt.get('shooter_pos')
                p2 = evt.get('target_pos')
                if p1 and p2:
                     self._add_projectile(p1, p2, evt.get('shooter') == 'ai')

    def _draw_grid(self):
        for x in np.arange(0, self.tm.arena_width_m, 0.2):
            p1 = self.tm.world_to_projector(x, 0)
            p2 = self.tm.world_to_projector(x, self.tm.arena_height_m)
            pygame.draw.line(self.screen, self.vis_cfg['colors']['grid'], p1, p2, 1)

    def _draw_robot(self, px_pos, theta, color):
        # Hitbox
        radius_m = self.vis_cfg.get('hitbox_r', 0.25)
        radius_px = int(self.tm.meters_to_pixels(radius_m))
        pygame.draw.circle(self.screen, self.vis_cfg['colors']['hitbox'], (int(px_pos[0]), int(px_pos[1])), radius_px, 1)

        # Corps Robot
        pygame.draw.circle(self.screen, color, (int(px_pos[0]), int(px_pos[1])), 25)
        # Canon
        barrel_len = 50; barrel_width = 10
        c, s = np.cos(theta), np.sin(theta)
        bx, by = px_pos[0] - 10*c, px_pos[1] - 10*s
        tx, ty = px_pos[0] + barrel_len*c, px_pos[1] + barrel_len*s
        px, py = -s * (barrel_width/2), c * (barrel_width/2)
        poly = [(bx+px,by+py), (tx+px,ty+py), (tx-px,ty-py), (bx-px,by-py)]
        pygame.draw.polygon(self.screen, (80, 80, 80), poly)
        pygame.draw.polygon(self.screen, (200, 200, 200), poly, 2)
        pygame.draw.circle(self.screen, (40, 40, 40), (int(px_pos[0]), int(px_pos[1])), 15)

    def _draw_entities(self, state):
        ents = state.get('entities', {})
        if 'ai' in ents:
            pos = ents['ai']['pos']
            px = self.tm.world_to_projector(pos[0], pos[1])
            self._draw_robot(px, pos[2], self.vis_cfg['colors']['ai']) 
        if 'human' in ents:
            pos = ents['human']['pos']
            px = self.tm.world_to_projector(pos[0], pos[1])
            self._draw_robot(px, pos[2], self.vis_cfg['colors']['human'])

    def _draw_hud(self, state):
        scores = state.get('scores', {})
        w = self.screen.get_width()
        
        lbl_ai = self.font_info.render("AI ROBOT", True, self.vis_cfg['colors']['ai'])
        lbl_hu = self.font_info.render("PLAYER", True, self.vis_cfg['colors']['human'])
        self.screen.blit(lbl_ai, (50, 20))
        self.screen.blit(lbl_hu, (w - 50 - lbl_hu.get_width(), 20))

        txt_ai = self.font_score.render(str(scores.get('ai', 0)), True, self.vis_cfg['colors']['ai'])
        self.screen.blit(txt_ai, (50, 50))
        txt_hu = self.font_score.render(str(scores.get('human', 0)), True, self.vis_cfg['colors']['human'])
        self.screen.blit(txt_hu, (w - 50 - txt_hu.get_width(), 50))
        
        status = state.get('status', 'WAITING')
        if status != 'RUNNING':
            self._draw_overlay(status)

    def _draw_overlay(self, text):
        s = pygame.Surface(self.screen.get_size(), pygame.SRCALPHA)
        s.fill((0, 0, 0, 150))
        self.screen.blit(s, (0,0))
        txt = self.font_big.render(text, True, C_GOLD) # Keep Gold for overlay
        cx, cy = self.screen.get_width()//2, self.screen.get_height()//2
        self.screen.blit(txt, (cx - txt.get_width()//2, cy - txt.get_height()//2))

    def _add_floating_text(self, text, wx, wy, color):
        px = self.tm.world_to_projector(wx, wy)
        self.floating_texts.append({
            'text': text, 'x': px[0], 'y': px[1] - 50, 'life': 40, 'color': color
        })
        
    def _add_projectile(self, p1_world, p2_world, is_ai):
        start = self.tm.world_to_projector(p1_world[0], p1_world[1])
        end = self.tm.world_to_projector(p2_world[0], p2_world[1])
        dx = end[0] - start[0]; dy = end[1] - start[1]
        dist = np.sqrt(dx**2 + dy**2)
        if dist < 1: return
        
        SPEED = self.vis_cfg.get('speed', 800.0)
        vx = (dx/dist)*SPEED; vy = (dy/dist)*SPEED
        color = self.vis_cfg['colors']['ai'] if is_ai else self.vis_cfg['colors']['human']
        self.projectiles.append({
             'pos': [start[0], start[1]], 'vel': [vx, vy], 'color': color, 
             'life': dist/SPEED
        })

    def _draw_fx(self, dt):
        for p in self.projectiles[:]:
            p['life'] -= dt
            p['pos'][0] += p['vel'][0] * dt
            p['pos'][1] += p['vel'][1] * dt
            if p['life'] <= 0:
                self.projectiles.remove(p); continue
            px, py = p['pos']
            pygame.draw.circle(self.screen, p['color'], (int(px), int(py)), 8)
            pygame.draw.circle(self.screen, (255, 255, 255), (int(px), int(py)), 3)
        
        for ft in self.floating_texts[:]:
            ft['life'] -= 1; ft['y'] -= 1
            if ft['life'] <= 0:
                self.floating_texts.remove(ft); continue
            rend = self.font_score.render(ft['text'], True, ft['color'])
            self.screen.blit(rend, (int(ft['x']), int(ft['y'])))


# --- STANDALONE / MANUAL SIMULATION ---

class ManualSimulator:
    def __init__(self, tm):
        self.tm = tm
        self.ai_pos = [tm.arena_width_m * 0.2, tm.arena_height_m * 0.5, 0.0]  
        self.hu_pos = [tm.arena_width_m * 0.8, tm.arena_height_m * 0.5, 3.14]
    
    def update(self, state, action):
        if not state:
            state = {
                "timestamp": time.time(), "status": "WAITING", 
                "scores": { "ai": 0, "human": 0 },
                "entities": {
                    "ai": { "pos": self.ai_pos },
                    "human": { "pos": self.hu_pos }
                }, "events": []
            }
        
        state['timestamp'] = time.time()
        state['events'] = [] 
        evt_id = str(uuid.uuid4())
        
        if action == 'start': state['status'] = 'RUNNING'; state['scores'] = { "ai": 0, "human": 0 }
        elif action == 'end': state['status'] = 'GAME OVER'
        elif state['status'] == 'RUNNING':
            if action == 'shoot_ai':
                state['events'].append({ 'id': evt_id, 'type': 'shot', 'shooter': 'ai', 'shooter_pos': self.ai_pos, 'target_pos': self.hu_pos })
            elif action == 'shoot_human':
                state['events'].append({ 'id': evt_id, 'type': 'shot', 'shooter': 'human', 'shooter_pos': self.hu_pos, 'target_pos': self.ai_pos })
            elif action == 'hit':
                state['scores']['ai'] += 1 
                state['events'].append({ 'id': evt_id, 'type': 'hit', 'target': 'human' })

        sf = StateFile(STATE_FILE)
        sf.write(state)
        return state

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--sim', action='store_true', help="Mode Simulation Manuelle")
    args = parser.parse_args()
    
    renderer = GameRenderer()
    
    # Standalone Loop
    sim = ManualSimulator(renderer.tm) if args.sim else None
    sim_state = None
    
    # Init SIM
    if sim:
        print("[Manual Sim] Controls: SPACE=Start, ENTER=End, A/Z=Shoot, E=Hit")
        sim_state = sim.update(None, None)
    
    sf = StateFile(STATE_FILE)
    
    while renderer.running:
        # Input for Sim
        if not renderer.process_events():
            break
            
        keys = pygame.key.get_pressed()
        # Note: process_events consumant les events, on devrait plutôt hooker GameRenderer. 
        # Mais pour ce test rapide, on va patcher process_events ou juste lire l'état global si on veut...
        # Mieux : on réimplémente une lecture d'event basique ici ou on modifie GameRenderer pour exposer les events.
        # Pour faire simple et propre dans le style "Library", GameRenderer devrait juste rendre.
        # Le Input Handling spécifique à la Sim est un hack de debug.
        
        # Hack direct sur pygame pour la simu (puisque renderer.process_events vide la queue)
        # On va plutôt faire un petit switch dans renderer pour récupérer les inputs si besoin?
        # Non, on va laisser GameRenderer gérer le Quit, et on va juste chequer les touches ici si on peut.
        # pygame.key.get_pressed() marche toujours.
        
        if sim:
            action = None
            if keys[pygame.K_SPACE]: action = 'start'
            if keys[pygame.K_RETURN]: action = 'end'
            if keys[pygame.K_a]: action = 'shoot_ai'
            if keys[pygame.K_z]: action = 'shoot_human'
            if keys[pygame.K_e]: action = 'hit'
            
            # Debounce très basic (sinon ça mitraille à 60fps)
            if action and (time.time() - getattr(main, 'last_act', 0) > 0.2):
                sim_state = sim.update(sim_state, action)
                main.last_act = time.time()

        # Read State
        state = sf.read()
        renderer.update(state)
        
    renderer.close()

if __name__ == '__main__':
    main()


################################################################################
PATH: ./renderer/game_state.json
################################################################################
{"timestamp": 1767124468.13178, "status": "RUNNING", "scores": {"ai": 0, "human": 0}, "entities": {"ai": {"pos": [0.20335808881103185, 0.367541134539419, 0.0]}, "human": {"pos": [0.8134323552441274, 0.367541134539419, 3.14]}}, "events": [{"id": "8509b551-9a81-40b3-8a53-ca01a25c2ea3", "type": "shot", "shooter": "ai", "shooter_pos": [0.20335808881103185, 0.367541134539419, 0.0], "target_pos": [0.8134323552441274, 0.367541134539419, 3.14]}]}

################################################################################
PATH: ./requirements.txt
################################################################################
# Dépendances Projet Tank Arena

# Vision & Traitement Image
numpy>=1.20.0
opencv-python>=4.5.0
pyrealsense2>=2.50.0

# Visualisation
pygame>=2.1.0

# Traitement Signal & Math
scipy>=1.7.0

# Configuration
pyyaml>=5.4.0

# Utilitaires
matplotlib>=3.3.0


################################################################################
PATH: ./scripts/detect_projector_resolution.py
################################################################################
#!/usr/bin/env python3
"""
Détecter et Configurer la Résolution du Projecteur

Ce script :
1. Liste les écrans connectés.
2. permet d'identifier le projecteur.
3. Met à jour automatiquement config/projector.yaml.

Usage :
    python3 scripts/detect_projector_resolution.py
"""

import pygame
import sys
import yaml
from pathlib import Path

def load_projector_config():
    config_path = Path(__file__).parent.parent / 'config' / 'projector.yaml'
    if config_path.exists():
        with open(config_path) as f:
            return yaml.safe_load(f), config_path
    return None, config_path

def save_projector_config(config, path):
    with open(path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False)
    print(f"[OK] Configuration sauvegardée dans {path}")

def main():
    print("=" * 60)
    print("  CONFIGURATION AUTOMATIQUE DU PROJECTEUR")
    print("=" * 60)
    
    pygame.init()
    
    # 1. Détection des écrans
    try:
        displays = pygame.display.get_desktop_sizes()
    except AttributeError:
        # Fallback Pygame ancien
        info = pygame.display.Info()
        displays = [(info.current_w, info.current_h)]
        print("Note: Pygame ancien, détection limitée.")

    print(f"\n[DISPLAY] Écrans détectés : {len(displays)}\n")
    
    for i, (w, h) in enumerate(displays):
        print(f"  [{i}] {w} x {h} px  {'<-- Probablement le PC' if i==0 else '<-- Probablement le Projecteur'}")

    print("\nQuelle est l'ID de votre projecteur ?")
    
    try:
        choice = input(f"Entrez le numéro (0-{len(displays)-1}) ou 'q' pour quitter : ")
        if choice.lower() == 'q':
            return
        
        idx = int(choice)
        if idx < 0 or idx >= len(displays):
            print("[ERREUR] ID invalide.")
            return
            
        target_w, target_h = displays[idx]
        print(f"\n[OK] Vous avez choisi : {target_w} x {target_h} px")
        
        # 2. Mise à jour de la config
        config, path = load_projector_config()
        if config is None:
            print("[ERREUR] Erreur : config/projector.yaml introuvable.")
            return
            
        print(f"\nAncienne configuration : {config['projector']['width']} x {config['projector']['height']}")
        
        # Mise à jour resolution
        config['projector']['width'] = target_w
        config['projector']['height'] = target_h
        
        # Mise à jour offset (si 2 écrans et projecteur est le 2eme)
        # On suppose que l'offset X est la largeur du premier écran si on choisit le 2eme
        if len(displays) > 1 and idx == 1:
            offset_x = displays[0][0]
            print(f"Mise à jour de l'offset X à {offset_x} (largeur écran principal)")
            config['display']['monitor_offset_x'] = offset_x
        
        # Confirmation
        confirm = input("\nSauvegarder cette configuration ? (o/n) : ")
        if confirm.lower() == 'o':
            save_projector_config(config, path)
            print("\n[SUCCES] SUCCÈS ! La résolution est corrigée.")
            print("Relancez maintenant 'python3 scripts/run_calibration.py'")
        else:
            print("Annulé.")

    except ValueError:
        print("Entrée invalide.")
    except Exception as e:
        print(f"Erreur : {e}")
    finally:
        pygame.quit()

if __name__ == "__main__":
    main()

################################################################################
PATH: ./scripts/find_aruco_front.py
################################################################################
#!/usr/bin/env python3
"""
Validation Orientation Monde vs Orientation ArUco

Affiche simultanément :
- le repère MONDE (axes fixes)
- l'orientation AVANT de l'ArUco
- permet de vérifier si l'avant ArUco = avant robot réel
"""

import sys
import os
import yaml
import pygame
import numpy as np
import cv2
import time
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from perception.camera.realsense_stream import RealSenseStream
from perception.camera.aruco_detector import ArucoDetector
from core.world.unified_transform import load_calibration

# ===== Couleurs =====
C_BG = (0, 0, 0)
C_WORLD_X = (255, 0, 0)      # Rouge = +X monde
C_WORLD_Y = (0, 255, 0)      # Vert  = +Y monde
C_ARUCO = (0, 200, 255)      # Cyan = ArUco
C_TEXT = (255, 255, 255)

AXIS_LEN_M = 0.30            # 30 cm


def load_projector_config():
    config_dir = Path(__file__).parent.parent / 'config'
    path = config_dir / 'projector.yaml'
    if path.exists():
        with open(path) as f:
            return yaml.safe_load(f)
    return None


def get_aruco_pose_world(detection, transform_mgr):
    """
    Retourne (x, y, theta_world) depuis l'ArUco
    """
    u, v = detection['center']
    theta_pix = detection['orientation']

    dist_px = 30
    u_f = u + dist_px * np.cos(theta_pix)
    v_f = v - dist_px * np.sin(theta_pix)

    x, y = transform_mgr.camera_to_world(u, v)
    x_f, y_f = transform_mgr.camera_to_world(u_f, v_f)

    theta = np.arctan2(y_f - y, x_f - x)
    return x, y, theta


def main():
    print("[CHECK] Monde vs ArUco orientation")

    transform_mgr = load_calibration(str(Path(__file__).parent.parent / "config"))
    if not transform_mgr.is_calibrated():
        print("[ERREUR] Calibration absente")
        return

    proj_conf = load_projector_config()
    if proj_conf:
        win_w = proj_conf['projector']['width']
        win_h = proj_conf['projector']['height']
        off_x = proj_conf['display'].get('monitor_offset_x', 0)
        off_y = proj_conf['display'].get('monitor_offset_y', 0)
    else:
        win_w = transform_mgr.proj_width
        win_h = transform_mgr.proj_height
        off_x, off_y = 1920, 0

    os.environ['SDL_VIDEO_WINDOW_POS'] = f"{off_x},{off_y}"

    pygame.init()
    screen = pygame.display.set_mode((win_w, win_h), pygame.NOFRAME)
    font = pygame.font.SysFont("Consolas", 22, bold=True)

    camera = RealSenseStream(1280, 720, 30)
    camera.start()
    time.sleep(1.0)

    K, D = camera.get_intrinsics_matrix()
    aruco = ArucoDetector()

    clock = pygame.time.Clock()
    running = True

    try:
        while running:
            for event in pygame.event.get():
                if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE:
                    running = False

            frame, _ = camera.get_frames()
            if frame is None:
                continue

            if K is not None and D is not None:
                frame = cv2.undistort(frame, K, D)

            detections = aruco.detect(frame)

            screen.fill(C_BG)

            # ===== Repère MONDE fixe =====
            ox = transform_mgr.arena_width_m / 2
            oy = transform_mgr.arena_height_m / 2

            o_px = transform_mgr.world_to_projector(ox, oy)

            # +X monde
            x_px = transform_mgr.world_to_projector(ox + AXIS_LEN_M, oy)
            pygame.draw.line(screen, C_WORLD_X, o_px, x_px, 5)
            screen.blit(font.render("+X MONDE", True, C_WORLD_X), (x_px[0] + 5, x_px[1]))

            # +Y monde
            y_px = transform_mgr.world_to_projector(ox, oy + AXIS_LEN_M)
            pygame.draw.line(screen, C_WORLD_Y, o_px, y_px, 5)
            screen.blit(font.render("+Y MONDE", True, C_WORLD_Y), (y_px[0] + 5, y_px[1]))

            # ===== ArUco =====
            for mid, data in detections.items():
                x, y, theta = get_aruco_pose_world(data, transform_mgr)

                c_px = transform_mgr.world_to_projector(x, y)
                f_px = transform_mgr.world_to_projector(
                    x + AXIS_LEN_M * np.cos(theta),
                    y + AXIS_LEN_M * np.sin(theta)
                )

                pygame.draw.circle(screen, C_ARUCO, c_px, 8, 2)
                pygame.draw.line(screen, C_ARUCO, c_px, f_px, 4)

                txt = font.render(
                    f"ID:{mid}  θ_aruco={np.degrees(theta):+.1f}°",
                    True, C_TEXT
                )
                screen.blit(txt, (c_px[0] + 10, c_px[1] - 20))

            screen.blit(
                font.render("Comparez AVANT ArUco (cyan) avec +X monde (rouge)", True, (0, 255, 0)),
                (20, 20)
            )

            pygame.display.flip()
            clock.tick(60)

    finally:
        camera.stop()
        pygame.quit()


if __name__ == "__main__":
    main()


################################################################################
PATH: ./scripts/show_grid.py
################################################################################
#!/usr/bin/env python3
"""
Outil de Validation Grille & Homographie (CORRIGÉ)

Utilise STRICTEMENT la calibration unifiée (UnifiedTransform) pour l'affichage.
Garantit que l'affichage correspond exactement à ce que l'IA "voit".

Commandes :
    [D]   : Basculer l'affichage de l'Inflation (Costmap / Grille brute)
    [ESC] : Quitter
"""

import sys
import os
import yaml
import pygame
import numpy as np
import cv2
from pathlib import Path

# Ajout du chemin racine pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from perception.camera.realsense_stream import RealSenseStream
from perception.camera.aruco_detector import ArucoDetector
from core.world.world_model import WorldModel
from core.world.unified_transform import UnifiedTransform, load_calibration

# Couleurs
C_BG = (0, 0, 0)                 # Noir pur pour le projecteur
C_GRID = (50, 50, 50)            # Gris foncé
C_OBSTACLE = (200, 50, 50)       # Rouge (Mur physique)
C_INFLATION = (200, 100, 0)      # Orange (Zone de danger)
C_MARKER = (0, 255, 255)         # Cyan (Position détectée)
C_TEXT = (255, 255, 255)

def load_projector_config():
    """Charge la config projecteur pour la fenêtre."""
    config_dir = Path(__file__).parent.parent / 'config'
    path = config_dir / 'projector.yaml'
    if path.exists():
        with open(path) as f:
            return yaml.safe_load(f)
    return None

def main():
    print("[CHECK] Démarrage validation Homographie...")

    # 1. Chargement Calibration (CRITIQUE)
    config_dir = Path(__file__).parent.parent / 'config'
    transform_mgr = load_calibration(str(config_dir))
    
    if not transform_mgr.is_calibrated():
        print("\n[ERREUR CRITIQUE] Aucune calibration trouvée !")
        print("Veuillez lancer : python -m perception.calibration.standalone_wizard")
        return

    print(f"[CHECK] Calibration chargée : {transform_mgr.pixels_per_meter:.2f} px/m")

    # 2. Configuration Affichage (Depuis calibration ou projector.yaml)
    proj_conf = load_projector_config()
    
    if proj_conf:
        win_w = proj_conf['projector']['width']
        win_h = proj_conf['projector']['height']
        off_x = proj_conf['display'].get('monitor_offset_x', 0)
        off_y = proj_conf['display'].get('monitor_offset_y', 0)
    else:
        # Fallback sur les données de calibration
        win_w = transform_mgr.proj_width
        win_h = transform_mgr.proj_height
        off_x, off_y = 1920, 0 # Valeur par défaut courante

    # Force la position de la fenêtre (SDL Hack)
    os.environ['SDL_VIDEO_WINDOW_POS'] = f"{off_x},{off_y}"
    
    pygame.init()
    screen = pygame.display.set_mode((win_w, win_h), pygame.NOFRAME | pygame.DOUBLEBUF)
    pygame.display.set_caption("Calibration Check")
    font = pygame.font.SysFont("Consolas", 24, bold=True)

    # 3. Vision
    print("[CHECK] Démarrage caméra...")
    # On récupère les dimensions depuis la config si possible, sinon défaut
    camera = RealSenseStream(width=1280, height=720, fps=30) 
    camera.start()
    import time; time.sleep(1.0) # Warmup

    # Paramètres intrinsèques pour Undistort
    K, D = camera.get_intrinsics_matrix()
    aruco = ArucoDetector()

    # 4. World Model (Pour l'inflation)
    # On utilise les dimensions exactes calculées par la calibration
    world = WorldModel(
        arena_width_m=transform_mgr.arena_width_m,
        arena_height_m=transform_mgr.arena_height_m,
        grid_resolution_m=0.02, # 2cm
        robot_radius_m=0.15,    # Ajuster selon votre robot (30cm diamètre = 0.15 rayon)
        inflation_margin_m=0.05
    )
    world.generate_costmap()

    # Boucle Principale
    running = True
    show_inflation = False
    clock = pygame.time.Clock()

    try:
        while running:
            # Events
            for event in pygame.event.get():
                if event.type == pygame.QUIT: running = False
                elif event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_ESCAPE: running = False
                    elif event.key == pygame.K_d: show_inflation = not show_inflation

            # 1. Acquisition Image
            color_frame, _ = camera.get_frames()
            if color_frame is None: continue

            # 2. Correction Distorsion (CRITIQUE pour l'alignement sur les bords)
            if K is not None and D is not None:
                color_frame = cv2.undistort(color_frame, K, D)

            # 3. Détection
            detections = aruco.detect(color_frame)

            # 4. Rendu
            screen.fill(C_BG)

            # --- A. DESSIN DE LA GRILLE (Via UnifiedTransform) ---
            # On dessine une grille tous les 10cm pour vérifier l'échelle
            step_m = 0.10 
            
            # Lignes Verticales
            for x in np.arange(0, world.arena_width, step_m):
                # On utilise world_to_projector qui utilise la matrice exacte
                start_px = transform_mgr.world_to_projector(x, 0)
                end_px = transform_mgr.world_to_projector(x, world.arena_height)
                pygame.draw.line(screen, C_GRID, start_px, end_px, 1)

            # Lignes Horizontales
            for y in np.arange(0, world.arena_height, step_m):
                start_px = transform_mgr.world_to_projector(0, y)
                end_px = transform_mgr.world_to_projector(world.arena_width, y)
                pygame.draw.line(screen, C_GRID, start_px, end_px, 1)

            # --- B. DESSIN DES OBSTACLES (Inflation) ---
            # On n'affiche que si demandé (touche D) pour garder la grille claire
            if show_inflation:
                grid_data = world.grid.costmap
                # Optimisation : trouver les indices occupés
                occ_y, occ_x = np.where(grid_data > 0.5)
                
                # Taille d'une cellule en pixels (environ)
                cell_px = int(world.grid.resolution * transform_mgr.pixels_per_meter)
                
                for r, c in zip(occ_y, occ_x):
                    # Conversion Grille -> Monde -> Pixels Projecteur
                    xm, ym = world.grid.grid_to_world(r, c)
                    px, py = transform_mgr.world_to_projector(xm, ym)
                    
                    rect = pygame.Rect(px - cell_px//2, py - cell_px//2, cell_px, cell_px)
                    pygame.draw.rect(screen, C_INFLATION, rect)

            # --- C. DESSIN DES ARUCO EN TEMPS RÉEL (Le Test Ultime) ---
            for mid, data in detections.items():
                u, v = data['center'] # Pixels Caméra
                
                # Transformation : Caméra -> Projecteur (Directe via Homographie)
                proj_x, proj_y = transform_mgr.camera_to_projector(u, v)
                
                # Dessin Cible
                # Si la calibration est bonne, ce cercle cyan doit être PILE SUR le robot réel
                center = (int(proj_x), int(proj_y))
                pygame.draw.circle(screen, C_MARKER, center, 10, 2)
                pygame.draw.line(screen, C_MARKER, (center[0]-15, center[1]), (center[0]+15, center[1]), 2)
                pygame.draw.line(screen, C_MARKER, (center[0], center[1]-15), (center[0], center[1]+15), 2)
                
                # Label ID
                lbl = font.render(f"ID:{mid}", True, C_TEXT)
                screen.blit(lbl, (center[0]+15, center[1]-15))

            # UI Info
            mode = "COSTMAP" if show_inflation else "GRILLE"
            info = f"[ESC] Quitter | [D] Mode: {mode} | Scale: {transform_mgr.pixels_per_meter:.1f} px/m"
            screen.blit(font.render(info, True, (0, 255, 0)), (20, 20))

            pygame.display.flip()
            clock.tick(60)

    except KeyboardInterrupt:
        pass
    finally:
        camera.stop()
        pygame.quit()

if __name__ == '__main__':
    main()


################################################################################
PATH: ./visualization/colors.py
################################################################################
"""
Colors - Palette Couleurs

Définit toutes les couleurs utilisées dans le projet.
"""

# Couleurs de base
WHITE = (255, 255, 255)
BLACK = (0, 0, 0)
RED = (255, 50, 50)
GREEN = (50, 255, 50)
BLUE = (50, 150, 255)
YELLOW = (255, 255, 50)
ORANGE = (255, 165, 0)
PURPLE = (160, 32, 240)
CYAN = (0, 255, 255)
MAGENTA = (255, 0, 255)

# Nuances de gris
GRAY_LIGHT = (200, 200, 200)
GRAY = (150, 150, 150)
GRAY_DARK = (100, 100, 100)

# Couleurs robots
ROBOT_AI_COLOR = BLUE
ROBOT_HUMAN_COLOR = RED

# Couleurs interface
HUD_TEXT_COLOR = BLACK
HUD_BG_COLOR = (255, 255, 255, 180)  # Semi-transparent
TIMER_COLOR = BLACK
SCORE_AI_COLOR = BLUE
SCORE_HUMAN_COLOR = RED

# Couleurs visualisation
ARENA_BORDER_COLOR = BLACK
ARENA_BG_COLOR = WHITE
OBSTACLE_COLOR = GRAY_DARK
GRID_LINE_COLOR = (220, 220, 220)

# Couleurs tir
SHOT_AI_COLOR = (100, 200, 255)
SHOT_HUMAN_COLOR = (255, 100, 100)
HIT_FLASH_COLOR = (255, 255, 0, 200)

# Couleurs lock-on
LOCKON_COLOR = RED
LOCKON_PULSE_COLOR = (255, 100, 100)

# Couleurs debug
PATH_COLOR = (0, 255, 0, 100)
WAYPOINT_COLOR = GREEN
LOS_LINE_COLOR = YELLOW


################################################################################
PATH: ./visualization/debug_draw.py
################################################################################
"""
Debug Draw - Visualisation Debug

Affiche éléments de debug (paths, LOS, grille, etc.).

Logs: [DEBUG_DRAW] prefix
"""

import pygame
import numpy as np
from typing import List, Tuple, Optional
from .colors import *


class DebugDraw:
    """
    Gère affichage éléments debug.
    """
    
    def __init__(self, surface: pygame.Surface, projector_mapping):
        """
        Initialize debug drawer.
        
        Args:
            surface: Surface Pygame
            projector_mapping: ProjectorMapping instance
        """
        self.surface = surface
        self.mapping = projector_mapping
        self.font = pygame.font.SysFont('Courier', 16)
        
    def draw_path(self, waypoints: List[Tuple[float, float]], 
                 color: Tuple = PATH_COLOR):
        """
        Dessine chemin planifié.
        
        Args:
            waypoints: Liste (x, y) en mètres
            color: Couleur ligne
        """
        if len(waypoints) < 2:
            return
        
        # Convertir en pixels
        points_px = [self.mapping.world_to_projector(x, y) 
                    for x, y in waypoints]
        
        # Dessiner lignes
        pygame.draw.lines(self.surface, color, False, points_px, 3)
        
        # Dessiner waypoints
        for px in points_px:
            pygame.draw.circle(self.surface, WAYPOINT_COLOR, px, 5)
            
    def draw_line_of_sight(self,
                          start: Tuple[float, float],
                          end: Tuple[float, float],
                          blocked: bool = False):
        """
        Dessine ligne de vue.
        
        Args:
            start: Point départ (x, y) mètres
            end: Point arrivée (x, y) mètres
            blocked: True si bloquée
        """
        p1 = self.mapping.world_to_projector(*start)
        p2 = self.mapping.world_to_projector(*end)
        
        color = RED if blocked else LOS_LINE_COLOR
        pygame.draw.line(self.surface, color, p1, p2, 2)
        
    def draw_grid(self, grid_resolution: float = 0.5):
        """
        Dessine grille métrique.
        
        Args:
            grid_resolution: Espacement grille (mètres)
        """
        # Lignes verticales
        x = 0.0
        while x <= self.mapping.arena_width_m:
            p1 = self.mapping.world_to_projector(x, 0)
            p2 = self.mapping.world_to_projector(x, self.mapping.arena_height_m)
            pygame.draw.line(self.surface, GRID_LINE_COLOR, p1, p2, 1)
            x += grid_resolution
        
        # Lignes horizontales
        y = 0.0
        while y <= self.mapping.arena_height_m:
            p1 = self.mapping.world_to_projector(0, y)
            p2 = self.mapping.world_to_projector(self.mapping.arena_width_m, y)
            pygame.draw.line(self.surface, GRID_LINE_COLOR, p1, p2, 1)
            y += grid_resolution
            
    def draw_occupancy_grid(self, grid):
        """
        Dessine grille occupation.
        
        Args:
            grid: OccupancyGrid instance
        """
        # Simplification: dessiner cellules occupées
        for row in range(grid.n_rows):
            for col in range(grid.n_cols):
                if grid.grid[row, col] > 0.5:
                    # Convertir cell -> monde -> pixels
                    x_m, y_m = grid.grid_to_world(row, col)
                    px, py = self.mapping.world_to_projector(x_m, y_m)
                    
                    cell_size = self.mapping.scale_length(grid.resolution)
                    rect = pygame.Rect(px, py, cell_size, cell_size)
                    
                    alpha = int(grid.grid[row, col] * 150)
                    s = pygame.Surface((cell_size, cell_size))
                    s.set_alpha(alpha)
                    s.fill(OBSTACLE_COLOR)
                    self.surface.blit(s, rect)
                    
    def draw_robot_info(self, robot_id: int,
                       pose: Tuple[float, float, float],
                       velocity: Optional[Tuple[float, float, float]] = None):
        """
        Affiche infos robot au-dessus.
        
        Args:
            robot_id: ID robot
            pose: (x, y, theta)
            velocity: (vx, vy, omega) optionnel
        """
        x, y, theta = pose
        px, py = self.mapping.world_to_projector(x, y)
        
        # Texte infos
        info_lines = [f"R{robot_id}"]
        info_lines.append(f"({x:.2f}, {y:.2f})")
        
        if velocity:
            vx, vy, omega = velocity
            v_norm = np.sqrt(vx**2 + vy**2)
            info_lines.append(f"v={v_norm:.2f}m/s")
        
        # Dessiner texte
        y_offset = -40
        for line in info_lines:
            text = self.font.render(line, True, BLACK)
            text_rect = text.get_rect(center=(px, py + y_offset))
            
            # Fond
            bg = text_rect.inflate(10, 4)
            s = pygame.Surface((bg.width, bg.height))
            s.set_alpha(180)
            s.fill(WHITE)
            self.surface.blit(s, bg)
            
            self.surface.blit(text, text_rect)
            y_offset += 20


################################################################################
PATH: ./visualization/__init__.py
################################################################################


################################################################################
PATH: ./visualization/projector_overlay.py
################################################################################
"""
Projector Overlay - Gestion Affichage Projecteur

Gère affichage superposé sur projecteur (éléments calibration, debug).

Logs: [OVERLAY] prefix
"""

import pygame
import numpy as np
from typing import Tuple, Optional
from .colors import *


class ProjectorOverlay:
    """
    Gère éléments overlay projetés.
    """
    
    def __init__(self, surface: pygame.Surface):
        """
        Initialize overlay.
        
        Args:
            surface: Surface Pygame où dessiner
        """
        self.surface = surface
        self.font = pygame.font.SysFont('Arial', 32)
        self.font_small = pygame.font.SysFont('Arial', 20)
        
    def draw_calibration_markers(self,
                                positions: list,
                                marker_ids: list,
                                size: int = 100):
        """
        Dessine marqueurs ArUco virtuels pour calibration.
        
        Args:
            positions: Liste (x, y) positions pixels
            marker_ids: Liste IDs marqueurs (0-3)
            size: Taille marqueurs pixels
        """
        for pos, marker_id in zip(positions, marker_ids):
            # Dessiner carré blanc avec bordure noire
            rect = pygame.Rect(pos[0] - size//2, pos[1] - size//2, size, size)
            pygame.draw.rect(self.surface, WHITE, rect)
            pygame.draw.rect(self.surface, BLACK, rect, 3)
            
            # Dessiner ID au centre
            text = self.font.render(str(marker_id), True, BLACK)
            text_rect = text.get_rect(center=pos)
            self.surface.blit(text, text_rect)
    
    def draw_crosshair(self, pos: Tuple[int, int], 
                      size: int = 20,
                      color: Tuple = RED):
        """
        Dessine réticule.
        
        Args:
            pos: Position (x, y)
            size: Taille
            color: Couleur
        """
        x, y = pos
        pygame.draw.line(self.surface, color, 
                        (x - size, y), (x + size, y), 2)
        pygame.draw.line(self.surface, color,
                        (x, y - size), (x, y + size), 2)
        pygame.draw.circle(self.surface, color, pos, size, 2)
    
    def draw_message(self, message: str,
                    position: Tuple[int, int],
                    color: Tuple = BLACK,
                    font_size: str = 'normal'):
        """
        Affiche message texte.
        
        Args:
            message: Texte à afficher
            position: (x, y) position
            color: Couleur texte
            font_size: 'normal' ou 'small'
        """
        font = self.font if font_size == 'normal' else self.font_small
        text = font.render(message, True, color)
        self.surface.blit(text, position)
    
    def draw_instruction(self, instruction: str):
        """
        Affiche instruction centrée en haut.
        
        Args:
            instruction: Texte instruction
        """
        text = self.font.render(instruction, True, BLACK)
        text_rect = text.get_rect(center=(self.surface.get_width() // 2, 100))
        
        # Fond semi-transparent
        bg_rect = text_rect.inflate(40, 20)
        s = pygame.Surface((bg_rect.width, bg_rect.height))
        s.set_alpha(200)
        s.fill(WHITE)
        self.surface.blit(s, bg_rect)
        
        # Texte
        self.surface.blit(text, text_rect)


################################################################################
PATH: ./visualization/ui_hud.py
################################################################################
"""
UI HUD - Heads-Up Display

Gère affichage HUD (scores, timer, status).

Logs: [HUD] prefix
"""

import pygame
import time
from typing import Dict, Optional
from .colors import *


class UI_HUD:
    """
    Gère HUD du jeu.
    """
    
    def __init__(self, surface: pygame.Surface):
        """
        Initialize HUD.
        
        Args:
            surface: Surface Pygame
        """
        self.surface = surface
        self.width = surface.get_width()
        self.height = surface.get_height()
        
        # Fonts
        self.font_large = pygame.font.SysFont('Arial', 64, bold=True)
        self.font_medium = pygame.font.SysFont('Arial', 40)
        self.font_small = pygame.font.SysFont('Arial', 28)
        
    def draw_timer(self, time_remaining: float):
        """
        Dessine chronomètre.
        
        Args:
            time_remaining: Temps restant (secondes)
        """
        minutes = int(time_remaining // 60)
        seconds = int(time_remaining % 60)
        
        time_text = f"{minutes:02d}:{seconds:02d}"
        text = self.font_large.render(time_text, True, TIMER_COLOR)
        text_rect = text.get_rect(center=(self.width // 2, 60))
        
        # Fond
        bg_rect = text_rect.inflate(40, 20)
        pygame.draw.rect(self.surface, WHITE, bg_rect)
        pygame.draw.rect(self.surface, BLACK, bg_rect, 3)
        
        self.surface.blit(text, text_rect)
        
    def draw_scores(self, ai_hits: int, human_hits: int):
        """
        Dessine scores.
        
        Args:
            ai_hits: Hits IA
            human_hits: Hits humain
        """
        # Score IA (gauche)
        ai_text = f"IA: {ai_hits}"
        ai_surface = self.font_medium.render(ai_text, True, SCORE_AI_COLOR)
        
        bg_ai = pygame.Rect(40, 40, 150, 60)
        pygame.draw.rect(self.surface, WHITE, bg_ai)
        pygame.draw.rect(self.surface, SCORE_AI_COLOR, bg_ai, 3)
        self.surface.blit(ai_surface, (50, 50))
        
        # Score Humain (droite)
        human_text = f"HUMAIN: {human_hits}"
        human_surface = self.font_medium.render(human_text, True, SCORE_HUMAN_COLOR)
        
        bg_human = pygame.Rect(self.width - 250, 40, 210, 60)
        pygame.draw.rect(self.surface, WHITE, bg_human)
        pygame.draw.rect(self.surface, SCORE_HUMAN_COLOR, bg_human, 3)
        self.surface.blit(human_surface, (self.width - 240, 50))
        
    def draw_cooldown(self, robot_id: int, cooldown_remaining: float):
        """
        Dessine barre cooldown tir.
        
        Args:
            robot_id: 4 (IA) ou 5 (Humain)
            cooldown_remaining: Temps restant (secondes)
        """
        if cooldown_remaining <= 0:
            return
        
        # Position selon robot
        if robot_id == 4:  # IA
            x, y = 40, 120
            color = SCORE_AI_COLOR
        else:  # Humain
            x, y = self.width - 250, 120
            color = SCORE_HUMAN_COLOR
        
        # Barre fond
        bar_width = 200
        bar_height = 20
        pygame.draw.rect(self.surface, GRAY_LIGHT, (x, y, bar_width, bar_height))
        
        # Barre remplissage
        fill_width = int(bar_width * (cooldown_remaining / 5.0))  # Max 5s
        pygame.draw.rect(self.surface, color, (x, y, fill_width, bar_height))
        
        # Bordure
        pygame.draw.rect(self.surface, BLACK, (x, y, bar_width, bar_height), 2)
        
    def draw_game_over(self, winner: str):
        """
        Affiche écran fin de partie.
        
        Args:
            winner: "AI", "HUMAN", ou "DRAW"
        """
        # Fond semi-transparent
        overlay = pygame.Surface((self.width, self.height))
        overlay.set_alpha(180)
        overlay.fill(BLACK)
        self.surface.blit(overlay, (0, 0))
        
        # Texte vainqueur
        if winner == "DRAW":
            text = "ÉGALITÉ!"
            color = YELLOW
        elif winner == "AI":
            text = "VICTOIRE IA!"
            color = SCORE_AI_COLOR
        else:
            text = "VICTOIRE HUMAIN!"
            color = SCORE_HUMAN_COLOR
        
        winner_surface = self.font_large.render(text, True, color)
        winner_rect = winner_surface.get_rect(center=(self.width // 2, self.height // 2))
        self.surface.blit(winner_surface, winner_rect)
        
    def draw_status_message(self, message: str, 
                          position: Optional[tuple] = None):
        """
        Affiche message status.
        
        Args:
            message: Message à afficher
            position: Position (x,y) ou None pour centré bas
        """
        text = self.font_small.render(message, True, BLACK)
        
        if position is None:
            position = (self.width // 2 - text.get_width() // 2, 
                       self.height - 100)
        
        # Fond
        bg_rect = text.get_rect(topleft=position).inflate(20, 10)
        pygame.draw.rect(self.surface, WHITE, bg_rect)
        pygame.draw.rect(self.surface, BLACK, bg_rect, 2)
        
        self.surface.blit(text, position)


